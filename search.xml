<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hadoop</title>
    <url>/2022/03/03/Hadoop/</url>
    <content><![CDATA[<p><strong>HDFS MapReduce YARN</strong></p>
<span id="more"></span>

<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h01kfgx6g2j30ud0gp0u5.jpg"></p>
<h4 id="HDFS概述"><a href="#HDFS概述" class="headerlink" title="HDFS概述"></a>HDFS概述</h4><p><strong>HDFS优缺点</strong></p>
<p>优点：高容错性，适合大规模数据处理，廉价成本</p>
<p>缺点：不适合低延时数据访问，无法高效对大量小文件存储，不支持并发写入，文件随即修改。</p>
<p><strong>HDFS组成架构</strong></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h011gaq5xej30te0lu76b.jpg"></p>
<p>NameNode:管理HDFS名称空间，配置副本策略，管理数据块，处理客户端请求</p>
<p>DataNode:存储实际数据块，执行数据块的读写操作，</p>
<p>Client:文件切分，与NameNode交互获取文件位置信息，与DataNode交互，读写数据</p>
<p>SecondaryNameNode:辅助NameNode为其分担工作量(定期合并FSImage和Edit Log)，辅助恢复NameNode</p>
<p><strong>元数据格式</strong></p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h0128e6g95j30nr0amjss.jpg"></p>
<p><strong>Hadoop文件块大小</strong>默认为128M，文件块太小，会增加寻址时间；文件块太大，从磁盘传输数据的时间将大于定位数据块起始位置所需的时间，导致程序处理数据慢</p>
<p><strong>机架感知(副本节点选择)</strong></p>
<ol>
<li>第一个副本在client所处的节点上。如果客户端在集群外，随机选一个</li>
<li>第二个副本和第一个副本位于相同机架，随机节点。</li>
<li>第三个副本位于不同机架，随机节点</li>
</ol>
<p><strong>常用命令</strong></p>
<blockquote>
<p>-moveFromLocal,-copyFromLocal,-put,-appendToFile,-copyToLocal,-get,-ls,-cat,-cp,-mkdir,-mv,-tail,-rm,-rm -r,-du,-setrep</p>
</blockquote>
<h4 id="HDFS读写流程"><a href="#HDFS读写流程" class="headerlink" title="HDFS读写流程"></a>HDFS读写流程</h4><p><strong>HDFS写数据</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h011xz364sj30gz0b1dgm.jpg"></p>
<ol>
<li>Client向NameNode请求上传文件</li>
<li>NameNode返回是否可以上传</li>
<li>Client请求第一个Block上传到那个DataNode</li>
<li>NameNode返回3个DataNode节点</li>
<li>Client请求datanode1上传数据，datanode1收到请求会继续调用datanode2，datanode2调用datanode3，将通信管道建立完</li>
<li>datanode1，2，3逐级应答Client</li>
<li>Client向datanode1上传第一个Block(Packet)，先从磁盘读数据放到本地进行缓存，datanode1收到一个Packet就会传给datanode2，datanode2传给datanode3，</li>
<li>当一个Block传输完，Client再次请求上传第二个Block</li>
</ol>
<p><strong>HDFS读数据</strong></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h011x3wtgbj30ku0c5q3y.jpg"></p>
<ol>
<li>Client向NameNode请求下载文件，NameNode查询元数据，定位文件块所在DataNode地址</li>
<li>从(先就近后随机)DataNode请求读取数据</li>
<li>DataNode传输数据给Client(Packet)</li>
<li>Client(Packet)接受，先本地缓存后写入目标文件</li>
</ol>
<h4 id="NN和2NN工作机制"><a href="#NN和2NN工作机制" class="headerlink" title="NN和2NN工作机制"></a>NN和2NN工作机制</h4><p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h0143iqerxj30qw0d8tby.jpg"></p>
<p>NameNode启动：</p>
<ol>
<li>首次启动格式化，创建Fsimage，Edits，之后启动load edits和fsimage到内存</li>
<li>Client对元数据进行增删改请求</li>
<li>NameNode记录操作日志</li>
<li>NameNode在内存中对数据增删改</li>
</ol>
<p>SecondaryNode工作：</p>
<ol>
<li>2NN询问NN是否要CheckPoint</li>
<li>2NN请求执行CheckPoint</li>
<li>NN滚动正在写的Edits，将滚动前的edits和fsimage拷贝到2NN</li>
<li>2NN加载edits和fsimage到内存，并进行合并，生成新的fsimage.checkpoint</li>
<li>拷贝fsimage.checkpoint到NN，NN将fsimage.checkpoint更名为fsimage</li>
</ol>
<p><strong>FSImage和Edit Log</strong></p>
<p>fsimage保存了最新的元数据检查点，包含了整个HDFS文件系统的所有目录和文件的信息。</p>
<p>editlog在NameNode已经启动情况下对HDFS进行的各种更新操作进行记录，HDFS客户端执行所有的写操作都会被记录到editlog中</p>
<p>namenode在被格式化后会在/data/tmp/dfs/name/current目录下产生相应文件：</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h0148rc90jj30ba02g3yg.jpg"></p>
<h4 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h4><p>​    一个数据块在DataNode上以文件形式存储在磁盘，分别是数据本身和元数据(数据块长度，数据块校验和，时间戳)</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h013jtur26j30cf022aa0.jpg"></p>
<ol>
<li>DataNode启动后向NameNode注册，通过后周期性(1h)向NameNode上报所有块信息</li>
<li>心跳3s/次，心跳返回结果带有NameNode给DataNode的命令，若超过10min没有收到某个DataNode的心跳，则认为节点已挂</li>
<li>集群运行中可以服役新节点或退役旧节点</li>
</ol>
<p><strong>Client获取块数据</strong></p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h013kyka07j31f70i0gnj.jpg"></p>
<pre><code> 1. 客户端向namenode发送查询请求
 2. namenode返回数据块所在的节点datanode
 3. 客户端在返回的节点中寻找相应的块数据
 4. 如果某个datanode挂点了，返回相应的副本中寻找
</code></pre>
<p><strong>HDFS保证数据完整性</strong></p>
<p>客户端向HDFS写数据：</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h013s4e4vqj31f70i0gne.jpg"></p>
<ol>
<li>假设客户端发送2KB的数据 </li>
<li>客户端会以字节的方式往datanode发送，所以客户端会计算发送的数据有多少个，而这个单位就是chunk（512字节）。</li>
<li>客户端可以计算出checksum值，checksum = 2KB/512B=4 </li>
<li>然后datanode接收客户端发送来的数据，每接收512B的数据，就让checksum的值+1 </li>
<li>最后比较客户端和datanade的checksum值</li>
</ol>
<p>DataNode读取Block块：</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h013wr7aaoj31f70i0di4.jpg"></p>
<ol>
<li>block创建时会初始checksum值 </li>
<li>DataNode每隔一段时间就会计算block新的checksum值，看block块是否已经丢失 </li>
<li>如果checksum和之前一样，则没丢失，和之前比出现了不一样，那就说明数据丢失（或者异常） </li>
<li>当发生异常的时候，DateNode会报告给NameNode，NameNode会发送一条命令，清除这个异常块，然后找到这个块对应的副本，将完整的副本复制给其他的DataNode节点</li>
</ol>
<h4 id="HA高可用"><a href="#HA高可用" class="headerlink" title="HA高可用"></a>HA高可用</h4><p>HA即高可用(7*24不间断服务)</p>
<p>实现高可用最关键的策略是消除单点故障，通过配置Active/Standby两个NameNodes实现集群中对NameNode的热备</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h012120smaj30hq0bfmy7.jpg"></p>
<p>Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode,只有主 NameNode 才能对外提供读写服务</p>
<p>ZKFailoverController（主备切换控制器，FC）：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换</p>
<p>Zookeeper 集群：为主备切换控制器提供主备选举支持；</p>
<p>共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。</p>
<p>DataNode 节点：主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。</p>
<h4 id="MapReduce概述"><a href="#MapReduce概述" class="headerlink" title="MapReduce概述"></a>MapReduce概述</h4><p>官方文档的解释是：Hadoop Map/Reduce是一个使用简易的软件框架，基于它写出来的应用程序能够运行在由上千个商用机器组成的大型集群上，并以一种可靠容错的方式并行处理上T级别的数据集。</p>
<p>一个Map/Reduce <em>作业（job）</em> 通常会把输入的数据集切分为若干独立的数据块，由 <em>map任务（task）</em>以完全并行的方式处理它们。框架会对map的输出先进行排序， 然后把结果输入给<em>reduce任务</em>。通常作业的输入和输出都会被存储在文件系统中。 整个框架负责任务的调度和监控，以及重新执行已经失败的任务。</p>
<p>Map/Reduce框架由一个单独的master JobTracker 和每个集群节点一个slave TaskTracker共同组成。master负责调度构成一个作业的所有任务，这些任务分布在不同的slave上，master监控它们的执行，重新执行已经失败的任务。而slave仅负责执行由master指派的任务。</p>
<p><strong>MapReduce核心思想</strong></p>
<ol>
<li>分布式的计算通常要分为至少两个阶段</li>
<li>第一阶段的MapTask并发实例，完全并行运行</li>
<li>第二阶段的ReduceTask并发实例互不相干，但数据依赖于上个阶段所有MapTask并发实例的输出</li>
<li>MapReduce编程模型只能包含一个Map和Reduce阶段</li>
</ol>
<h4 id="MapReduce架构"><a href="#MapReduce架构" class="headerlink" title="MapReduce架构"></a>MapReduce架构</h4><p>MapReduce体系结构主要由四个部分组成，分别是：Client、JobTracker、TaskTracker以及Task</p>
<p>JobTracker是用于调度工作的，TaskTracker是用于执行工作的。<strong>一个Hadoop集群中只有一台JobTracker</strong><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h018835rytj30kc0cadgl.jpg"></p>
<p>客户端向JobTracker提交一个作业，JobTracker把作业拆分为多份分配给TaskTracker执行，TaskTracker每隔一段时间会向JobTracker发送Heartbeat，如果一段时间内JobTracker没有收到来至TaskTracker的Heartbeat，将认为TaskTracker挂掉了，会把该TaskTracker的作业分配给其他TaskTracker</p>
<h4 id="MapReduce执行过程"><a href="#MapReduce执行过程" class="headerlink" title="MapReduce执行过程"></a>MapReduce执行过程</h4><p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h018evc6mwj30jw0f1aav.jpg"></p>
<p>1、客户端启动一个job<br>2、向JobTracker请求一个JobID<br>3、将运行作业所需要的资源文件复制到HDFS上，包括MapReduce程序打包的JAR文件、配置文件和客户端计算所得的输入划分信息。这些文件都存放在JobTracker专门为该作业创建的文件夹中，文件夹名为该作业JobID。JAR文件默认会有10个副本，输入划分信息告诉JobTracker应该为这个作业启动多少个map任务等信息。<br>4、JobTracker接收到作业后将其放在作业队列中，等待JobTracker对其进行调度。当JobTracker根据自己的调度算法调度该作业时，会根据输入划分信息为每个划分创建一个map任务，并将map任务分配给TaskTracker执行。这里需要注意的是，map任务不是随便分配给某个TaskTracker的，Data-Local（数据本地化）将map任务分配给含有该map处理的数据库的TaskTracker上，同时将程序JAR包复制到该TaskTracker上运行，但是分配reducer任务时不考虑数据本地化。<br>5、TaskTracker每隔一段时间给JobTracker发送一个Heartbeat告诉JobTracker它仍然在运行，同时心跳还携带很多比如map任务完成的进度等信息。当JobTracker收到作业的最后一个任务完成信息时，便把作业设置成“成功”，JobClient再传达信息给用户。</p>
<p><strong>输入输出</strong></p>
<p>Map/Reduce框架运转在&lt;key, value&gt; 键值对上， 框架把作业的输入看为是一组&lt;key, value&gt; 键值对，同样也产出一组 &lt;key, value&gt; 键值对做为作业的输出，这两组键值对的类型可能不同。</p>
<blockquote>
<p>(input) &lt;k1, v1&gt; -&gt; <strong>map</strong> -&gt; &lt;k2, v2&gt; -&gt; <strong>combine</strong> -&gt; &lt;k2, v2&gt; -&gt; <strong>reduce</strong> -&gt; &lt;k3, v3&gt; (output)</p>
</blockquote>
<p><strong>MapReduce优缺点</strong></p>
<p>优点：易于编程，良好扩展性，高容错性，适合PB级以上海量数据离线处理</p>
<p>缺点：不擅长实时计算，不擅长流式计算，不擅长DAG计算</p>
<h4 id="Map和Reduce"><a href="#Map和Reduce" class="headerlink" title="Map和Reduce"></a>Map和Reduce</h4><p><strong>(input) -&gt;map-&gt; -&gt;combine-&gt; -&gt;reduce-&gt; (output)</strong></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h018j4wxuyj30kc0b60u1.jpg"></p>
<p><strong>map</strong>：map任务可细分四个阶段：record reader、mapper、combiner和partitioner。map任务的输出被称为中间键和中间值，会被发送到reducer做后续处理。</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h018kcz9j0j30k509q0tc.jpg"></p>
<ol>
<li>读取HDFS中的文件。每一行解析成一个&lt;k,v&gt;。每一个键值对调用一次map函数。&lt;0,helloyou&gt; &lt;10,hello me&gt;</li>
<li>覆盖map()，接收（1）中产生的&lt;k,v&gt;，进行处理，转换为新的&lt;k,v&gt;输出。&lt;hello,1&gt;&lt;you,1&gt;&lt;hello,1&gt;&lt;me,1&gt;</li>
<li>对（2）输出的&lt;k,v&gt;进行<strong>分区</strong>，默认分为一个区。</li>
<li>对不同分区中的数据进行<strong>按照Key排序、分组</strong>。分组指的是相同key的value放到一个集合中。排序后：&lt;hello,1&gt;&lt;hello,1&gt;&lt;me,1&gt; &lt;you,1&gt;，分组后：&lt;hello,{1,1}&gt;&lt;me,{1}&gt;&lt;you,{1}&gt;</li>
<li>对分组后的数据进行<strong>合并归约</strong></li>
</ol>
<p><strong>reduce</strong>：reduce任务也分为四个阶段：混排（shuffle）、排序（sort）、reducer和输出格式（output format）</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h018m68e0sj30k509q0tc.jpg"></p>
<ol>
<li>多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点上。（shuffle）</li>
<li>对多个map的输出进行<strong>合并、排序</strong>。覆盖reduce函数，接收的是分组后的数据，实现自己的业务逻辑，&lt;hello,2&gt;&lt;me,1&gt;&lt;you,1&gt;处理后，产生新的&lt;k,v&gt;输出。</li>
<li>对reduce输出的&lt;k,v&gt;写到HDFS中。</li>
</ol>
<h4 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a><strong>Shuffle机制</strong></h4><p>mapper 阶段处理的数据如何传递给 reducer 阶段，是 MapReduce 框架中 最关键的一个流程，就叫 Shuffle</p>
<p>Shuffle: 数据混洗（核心机制：数据分区，排序，局部聚合，缓存，拉取，再合并 排序）,具体说就是就是将 MapTask 输出的处理结果数据，按照 Partitioner 组件制定的规则分发 给 ReduceTask，并在分发的过程中，对数据按 key 进行了分区和排序</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h018q2xrp5j30p60djq4e.jpg"></p>
<ol>
<li>Collect阶段：MapTask的数据输出到环形缓冲区</li>
<li>Spill阶段：当内存中数据量到一定阈值，会将数据写入本地磁盘(将数据写入磁盘前需要对数据进行一次排序)，如果配置combiner会将相同分区号和key的数据进行排序</li>
<li>MapTask阶段的Merge：把所有溢出的临时文件进行一次merge，确保一个MapTask最终只产生一个中间数据文件</li>
<li>Copy阶段：ReduceTask启动Fetcher线程到已完成MapTask节点上copy一份数据</li>
<li>ReduceTask阶段的Merge：在ReduceTask复制数据同时，会在后台开启两个线程对内存到本地的数据文件进行merge</li>
<li>Sort阶段：对数据进行merge同时，会进行排序，由于MapTask阶段已经对数据进行了局部排序，ReduceTask只需保证Copy数据的最终整体有效性</li>
</ol>
<h4 id="YARN概述"><a href="#YARN概述" class="headerlink" title="YARN概述"></a>YARN概述</h4><p>是一个资源调度平台，负责为运算程序提供服务器运算资源</p>
<h4 id="YARN架构"><a href="#YARN架构" class="headerlink" title="YARN架构"></a>YARN架构</h4><p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h01gppj9o5j30pr0gb405.jpg"></p>
<p><strong>Container</strong>：Container是YARN对资源的抽象，封装了节点上多维度资源，像内存，cpu，磁盘，网络；容器由 NodeManager 启动和管理，并被它所监控；容器被 ResourceManager 进行调度。</p>
<p><strong>NodeManager</strong>：管理单个节点上的资源(负责启动和管理节点上的容器)；处理来自ResourceManager和ApplicationMaster的命令</p>
<p><strong>ResourceManager</strong>：处理客户端请求；监控NodeManager；启动和监控ApplicationMaster；资源的分配与调度；RM有定时调用器(Scheduler)和应用管理器(ApplicationManager)两个主要组件</p>
<p>定时调度器(Scheduler)：当 Client 提交一个任务的时候，它会根据所需要的资源以及当前集群的资源状况进行分配，只负责向应用程序分配资源，并不做监控以及应用程序的状态跟踪</p>
<p>应用管理器(ApplicationManager)：负责管理 Client 用户提交的应用</p>
<p><strong>ApplicationMaster</strong>：与 RM 调度器协商以获取资源（用 Container 表示）； 将得到的任务进一步分配给内部的任务；与 NM 通信以启动 / 停止任务；监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务。</p>
<h4 id="YARN工作机制"><a href="#YARN工作机制" class="headerlink" title="YARN工作机制"></a>YARN工作机制</h4><p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h01hap4w61j31d60u0k2i.jpg"></p>
<h4 id="作业提交流程"><a href="#作业提交流程" class="headerlink" title="作业提交流程"></a>作业提交流程</h4><p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h01h4jackzj30rs0rdn03.jpg"></p>
<ol>
<li>Client向Yarn提交Application，这里我们假设是一个MapReduce作业。</li>
<li>ResourceManager向NodeManager通信，为该Application分配第一个容器。并在这个容器中运行这个应用程序对应的ApplicationMaster。</li>
<li>ApplicationMaster启动以后，对作业（也就是Application）进行拆分，拆分task出来，这些task可以运行在一个或多个容器中。然后向ResourceManager申请要运行程序的容器，并定时向ResourceManager发送心跳。</li>
<li>申请到容器后，ApplicationMaster会去和容器对应的NodeManager通信，而后将作业分发到对应的NodeManager中的容器去运行，这里会将拆分后的MapReduce进行分发，对应容器中运行的可能是Map任务，也可能是Reduce任务。</li>
<li>容器中运行的任务会向ApplicationMaster发送心跳，汇报自身情况。当程序运行完成后，ApplicationMaster再向ResourceManager注销并释放容器资源。</li>
</ol>
<h4 id="YARN调度器"><a href="#YARN调度器" class="headerlink" title="YARN调度器"></a>YARN调度器</h4><p>FIFO 、Capacity Scheduler（容量调度器）和Fair Sceduler（公平调度器）</p>
<p>FIFO调度器：支持单队列 、先进先出  生产环境不会用。</p>
<p>容量调度器：支持多队列，保证先进入的任务优先执行。</p>
<p>公平调度器：支持多队列，保证每个任务公平享有队列资源。</p>
]]></content>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka</title>
    <url>/2022/03/05/Kafka/</url>
    <content><![CDATA[]]></content>
      <tags>
        <tag>中间件</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark</title>
    <url>/2022/03/04/Spark/</url>
    <content><![CDATA[<blockquote>
<p>Spark大数据的计算分析引擎  官网 <a href="https://spark.apache.org/">https://spark.apache.org/</a></p>
</blockquote>
<ul>
<li>本文摘录于《Spark大数据处理技术》</li>
</ul>
<span id="more"></span>

<h4 id="Spark概述"><a href="#Spark概述" class="headerlink" title="Spark概述"></a><strong>Spark概述</strong></h4><blockquote>
<p>Apache Spark is a unified analytics engine for large-scale data processing</p>
</blockquote>
<p>按照现在流行的大数据处理场景划分，可以将大数据处理分为三种情况：</p>
<ul>
<li>复杂的批量数据处理，通常时间跨度为数十分钟到数小时</li>
<li>基于历史数据的交互式查询，通常时间跨度为数十秒到数分钟</li>
<li>基于实时数据流的数据处理，通常时间跨度为数百毫秒到数秒</li>
</ul>
<p>在Spark Core基础上衍生出能同时处理上面三种情形的统一大数据处理平台</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h02j0862b7j30ky04ijrl.jpg"></p>
<p>Spark要做的是将批处理，交互式处理，流式处理融合到一个软件栈中</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h02j2w0ltxj30mu092mxt.jpg"></p>
<h4 id="Spark-RDD"><a href="#Spark-RDD" class="headerlink" title="Spark RDD"></a>Spark RDD</h4><p><strong>HelloWorld</strong></p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h02j5gw5b9j30pg047t90.jpg"></p>
<ul>
<li>对于Spark程序来讲，所有的操作的前提是要有一个Spark上下文，创建上下文过程中，程序会向集群申请资源并构建运行环境。</li>
<li>通过sc变量并利用textFile接口从HDFS文件系统读取文件，返回file变量</li>
<li>对file进行过滤操作，生成新的变量filterRDD</li>
<li>对filterRDD进行cache操作(方便重用filterRDD)</li>
<li>对filterRDD进行计数，返回结果</li>
</ul>
<p>这个程序中涉及到众多概念：</p>
<ul>
<li>弹性式分布式数据集 RDD</li>
<li>创建操作(creation operation)：RDD初始创建是SparkContext负责，将内存集合或外部文件系统作为输入源</li>
<li>转换操作(transformation operation)：将一个RDD通过一定操作转变为另一个RDD</li>
<li>控制操作(control operation)：对RDD进行持久化，让RDD保存磁盘或内存中，方便重复使用</li>
<li>行动操作(action operation)：Spark是惰性计算的，RDD所有行动操作，都会触发Spark作业运行</li>
</ul>
<p>RDD和操作之间关系：经过输入操作，转换操作，控制操作，输出操作来完成一个作业</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h02jiomgszj30lh0ckdgp.jpg"></p>
<p><strong>Spark RDD</strong></p>
<p>RDD是弹性分布式数据集，即一个RDD代表一个被分区的只读数据集，RDD有两种生成途径：来自内存集合和外部存储系统；通过RDD的转换操作</p>
<p>RDD继承关系(lineage)构建可以通过记录作用在RDD上的转换操作，可以有效进行容错处理</p>
<p>一般情况下抽象的RDD需要包含这五个接口</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h02jtb7ww4j30ki05jt96.jpg"></p>
<p><strong>RDD分区(partitions)</strong></p>
<p>对于RDD来说，分区数涉及到RDD进行并行计算的粒度，每一个RDD分区的计算操作都在一个单独的任务中被执行。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">val rdd = sc.parallelize(1 to 100,2)</span><br><span class="line">//1-100的数组转为RDD，第二个参数执行分区数</span><br><span class="line">rdd.partitions.size</span><br><span class="line">//查看RDD被划分的分区数</span><br></pre></td></tr></table></figure>

<p><strong>RDD优先位置(preferredLocations)</strong></p>
<p>RDD优先位置属性与Spark中的调度相关，返回的是RDD的每个partition存储的位置，“移动数据不如移动计算”所以Spark任务调度时候尽可能将任务分配到数据块存储位置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">val rdd = sc.textFile(&quot;hdfs://10.0.2.19:8000/bigfile&quot;)</span><br><span class="line">//读取bigfile生成rdd</span><br><span class="line">val hadoopRDD =rdd.dependencies(0).rdd</span><br><span class="line">//通过rdd依赖关系找到原始hadoopRDD</span><br><span class="line">hadoopRDD.partitions.size</span><br><span class="line">hadoopRDD.preferredLocations(hadoopRDD.partition(0))</span><br><span class="line">//返回partition(0)所在的机器位置</span><br></pre></td></tr></table></figure>

<p><strong>RDD依赖关系(dependencies)</strong></p>
<p>由于RDD是粗粒度操作数据集，每个转换操作都会生成一个新的RDD，所以RDD之间会形成类似流水线(pipline)的前后依赖关系，Spark中有两种类型依赖。</p>
<p>窄依赖(Narrow Dependencies)：每一个父RDD的分区最多只能被子RDD的一个分区使用</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h02kef8mpbj30hr0cmq3c.jpg"></p>
<p>宽依赖(Wide Dependencies)：多个子RDD的分区会依赖于同一个父RDD的分区</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h02kewzqs2j30ik07cweq.jpg"></p>
<p>Spark中明确区分宽窄依赖原因？</p>
<ol>
<li>窄依赖可以在集群的一个节点上如流水线一样执行，可以计算所有父RDD分区，相反宽依赖需要取得父RDD的所有分区上的数据进行计算，将执行类似MapReduce一样的Shuffle操作</li>
<li>对于窄依赖，节点计算失败后恢复会更有效，只需重新计算对应父RDD的分区，而且可以在其他节点上并行的计算，相反在宽依赖依赖关系中，一个节点失败会导致其父RDD的多个分区重新计算</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">val rdd =sc.makeRDD(1 to 10)</span><br><span class="line">val mapRDD= rdd.map(x=&gt;(x,x))</span><br><span class="line">mapRDD.dependencies</span><br><span class="line">val shuffleRDD = mapRDD.partitionBy(new org.apache.spark.HashPartitioner(3))</span><br><span class="line">shuffleRDD.dependencies</span><br></pre></td></tr></table></figure>

<p><strong>RDD分区计算(compute)</strong></p>
<p>RDD的计算都是以partition为单位的，而且RDD中的compute函数都是在对迭代器进行复合，不需要保存每次计算结果</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">val rdd = sc.parallelize(1 to 10,2)</span><br><span class="line">val map_rdd = rdd.map(a=&gt;a+1)</span><br><span class="line">val filter_rdd = map_rdd.filter(a=&gt;(a&gt;3))</span><br><span class="line">val context = new org.apache.spark.TaskContext(0,0,0)</span><br><span class="line">val iter0 = filter_rdd.compute(filter_rdd.partition(0),context)</span><br><span class="line">iter0.toList</span><br><span class="line">val iter1 = filter_rdd.compute(filter_rdd.partition(0),context)</span><br><span class="line">iter1.toList</span><br><span class="line">//在rdd上进行map和filter，由于compute函数只返回相应分区数据的迭代器，只有最后实例化才能显示出两个分区最终计算结果</span><br></pre></td></tr></table></figure>

<p><strong>RDD分区函数(partitioner)</strong></p>
<p>Spark目前有两种类型分区函数：HashPartitioner(哈希分区)和RangePartitioner(区域分区)，而且partitioner这个属性只存在于(K,V)类型的RDD中，对于非(K,V)类型的partitioner的值就是None，partitioner函数既决定了RDD本身的分区数量，也可作为其父RDD Shuffle输出(MapOutput)中每个分区进行数据切割的依据</p>
<p>使用HashPartitioner说明一下partitioner的功能</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">val rdd =sc.makeRDD(1 to 10,2).map(x=&gt;(x,x))</span><br><span class="line">rdd.partitioner</span><br><span class="line">val group_rdd = rdd.groupByKey(new org.apache.spark.HashPartitioner(3))</span><br><span class="line">//new 了HashPartitioner对象</span><br><span class="line">group_rdd.partitioner</span><br><span class="line">group_rdd.collectPartitions()</span><br></pre></td></tr></table></figure>

<p>HashPartitioner的原理图：</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h02nqgxyjwj3084071mx8.jpg"></p>
<p><strong>集合创建操作</strong></p>
<p>RDD：弹性分布式数据集，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型；</p>
<p>DAG：有向无环图，反应RDD之间依赖关系</p>
<p>Executor：运行在Work Node上的进程，负责运行任务，并为应用程序存储数据</p>
<p>Application：用户编写的Spark程序</p>
<p>Task：运行在Executor上的工作单元</p>
<p>Job：一个Job包含多个RDD以及作用在RDD上的各种操作</p>
<p>Stage：作业的基本调度单位，一个作业会分为多组任务，每组任务叫做Stage</p>
<p>Spark运行场景：</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h01kwjxgm6j30uw06nwfg.jpg"></p>
<p><strong>Spark运行架构</strong></p>
<p>采用主从架构，包含一个Master(Driver)和多个Worker</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h01l88927fj30lt09mwey.jpg"></p>
<p><strong>Spark部署</strong></p>
]]></content>
      <tags>
        <tag>数据 Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase</title>
    <url>/2022/03/05/HBase/</url>
    <content><![CDATA[<h2 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h2><p>本文内容摘录于《HBase原理与实践》</p>
<span id="more"></span>

<h4 id="概述一览"><a href="#概述一览" class="headerlink" title="概述一览"></a>概述一览</h4><p><strong>基本概念：</strong></p>
<p>table：表，一个表包含多行数据。</p>
<p>row：行，一行数据包含一个唯一标识rowkey、多个column以及对应的值。在HBase中，一张表中所有row都按照rowkey的字典序由小到大排序。</p>
<p>column：列，HBase中的column由column family（列簇）以及qualif ier（列名）两部分组成，两者中间使用”:”相连。column family在表创建的时候需要指定，用户不能随意增减。一个column family下可以设置任意多个qualif ier，因此可以理解为HBase中的列可以动态增加，理论上甚至可以扩展到上百万列。</p>
<p>timestamp：时间戳，每个cell在写入HBase的时候都会默认分配一个时间戳作为该cell的版本，当然，用户也可以在写入的时候自带时间戳。HBase支持多版本特性，即同一rowkey、column下可以有多个value存在，这些value使用timestamp作为版本号，版本越大，表示数据越新。</p>
<p>cell：单元格，由五元组（row, column, timestamp, type, value）组成的结构，其中type表示Put/Delete这样的操作类型，timestamp代表这个cell的版本。这个结构在数据库中实际是以KV结构存储的，其中（row, column, timestamp, type）是K，value字段对应KV结构的V。</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/078bfb0511e176a0.png"></p>
<p>​                                                                            <center>（HBase逻辑视图）</center></p>
<p><strong>体系架构：</strong></p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/0576dba2ceb0e8a3.png"></p>
<p>​                                                                                <center>（体系架构）</center>                                            </p>
<ol>
<li><p>Client：丰富的编程接口，Client访问数据行前要先通过元数据表定位目标数据所在RegionServer，发请求给RegionServer，这些元数据也会被缓存到本地。</p>
</li>
<li><p>Zookeeper：实现Master高可用(异常宕机进行选举)，管理系统核心元数据(保存元数据表所在的RegionServer地址)，参与RegionServer宕机恢复(心跳机制)，实现分布式表锁</p>
</li>
<li><p>Master：处理各种管理请求(建表、修改表、权限操作、切分表、合并数据分片以及Compaction)，管理集群中的RegionServer(RegionServer中Region的负载均衡、RegionServer的宕机恢复以及Region的迁移)，清理过期日志以及文件，Master会每隔一段时间检查HDFS中HLog是否过期、HFile是否已经被删除，并在过期之后将其删除</p>
</li>
<li><p>RegionServer：响应用户的IO请求，由WAL(HLog),BlockCache,多个Region构成</p>
<p>WAL(HLog):实现数据高可靠性，数据写入缓存前先顺序写入HLog，实现HBase集群间主从复制通过回放主集群推送过来的HLog日志实现主从复制。</p>
<p>BlockCache:HBase系统中的读缓存，客户端从磁盘读取数据之后通常会将数据缓存到系统内存中，后续访问同一行数据可以直接从内存中获取而不需要访问磁盘。</p>
<p>Region:数据表的一个分片，当数据表大小超过阈值就会水平切分，分裂为两个Region，一个Region由一个或多个Store构成，Store个数取决于列簇个数，每个列簇的数据存放在一起形成一个存储单元Store，每个Store由一个MemStore和一或多个HFile组成，写入数据先写入MemStore，到一定阈值(128M)将Flush成HFile，随着HFile数越来越多，将会执行Compact合并成大文件</p>
</li>
<li><p>HDFS：HBase底层依赖HDFS存储数据，用户数据文件、HLog日志文件等最终都会写入HDFS落盘</p>
</li>
</ol>
<p><strong>HBase特性：</strong></p>
<p>优点：容量大，良好可扩展性，稀疏性，高性能，多版本，支持过期，Hadoop原生支持</p>
<p>缺点：不支持复杂聚合运算，没有二级索引功能，不支持全局跨行事务</p>
<h4 id="依赖服务"><a href="#依赖服务" class="headerlink" title="依赖服务"></a>依赖服务</h4><p><strong>ZooKeeper</strong>：一个分布式HBase集群的部署运行强烈依赖于ZooKeeper</p>
<p><strong>HDFS</strong>：HBase的文件都存放在HDFS（Hadoop Distribuited File System）文件系统上</p>
<h4 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h4><p>通过ThriftServer的Java HBase客户端来请求HBase集群。</p>
<p>定位Meta表：</p>
<p>客户端在做任何数据操作时，都要先确定数据在哪个Region上，然后再根据Region的RegionServer信息，去对应的RegionServer上读取数据，HBase系统内部设计了一张特殊的表——hbase:meta表，专门用来存放整个集群所有的Region信息。hbase:meta中的hbase指的是namespace，HBase容许针对不同的业务设计不同的namespace，系统表采用统一的namespace，即hbase；meta指的是hbase这个namespace下的表名。</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h00gqa64zqj30r507vdgm.jpg"></p>
<p>​                                                                    <center>Hbase:meta结构</center></p>
<p><font color="red"> HBase作为一个分布式数据库系统，一个大的集群可能承担数千万的查询写入请求，而hbase:meta表只有一个Region，如果所有的流量都先请求hbase:meta表找到Region，再请求Region所在的RegionServer，那么hbase:meta表的将承载巨大的压力，这个Region将马上成为热点Region，且根本无法承担数千万的流量。那么，如何解决这个问题呢</font></p>
<p>把hbase:meta表的Region信息缓存在HBase客户端</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h00gu71x0zj30q309ldgz.jpg"></p>
<p>​                                                                                <center>客户端定义Region</center></p>
<p>少量写和批量写：</p>
<p>HBase提供了3种常见的数据写入API：</p>
<p>table.put(put)：在服务端先写WAL，然后写MemStore，一旦MemStore写满就f lush到磁盘上。特点是，默认每次写入都需要执行一次RPC和磁盘持久化。因此，写入吞吐量受限于磁盘带宽、网络带宽以及f lush的速度。但它能保证每次写入操作都持久化到磁盘，不会有任何数据丢失。最重要的是，它能保证put操作的原子性。</p>
<p>table.put(List<Put> puts)：在客户端缓存put，等凑足了一批put，就将这些数据打包成一次RPC发送到服务端，一次性写WAL，并写MemStore。相比第一种方式省去了多次往返RPC以及多次刷盘的开销，吞吐量大大提升。不过，这个RPC操作耗时一般都会长一点，因为一次写入了多行数据。这些put中，可能有一部分失败，失败的那些put操作会经历若干次重试</Put></p>
<p>bulk load：直接将待写入数据生成HFile，将这些HFile直接加载到对应的Region下的CF内。在生成HFile时，在HBase服务端没有任何RPC调用，只在load HFile时会调用RPC，一种完全离线的快速写入方式。bulk load应该是最快的批量写手段，同时不会对线上的集群产生巨大压力。在load完HFile之后，CF内部会进行Compaction，但是Compaction是异步的且可以限速，所以产生的IO压力是可控的。bulk load对线上集群非常友好。</p>
<h4 id="RegionServer结构"><a href="#RegionServer结构" class="headerlink" title="RegionServer结构"></a>RegionServer结构</h4><p>一个RegionServer由一或多个HLog，BlockCache及多个Region组成，RegionServer负责Region的数据读写，一个Region由多个store组成，Store存放对应列簇数据，Store包含一个MemStore和多个HFile。</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/8343fa265a93d486.png"></p>
<p>​                                                                                <center>（RegionServer结构）</center></p>
<p><strong>HLog</strong>：用来保证数据写入的可靠性</p>
<p>HBase中系统故障恢复及主从复制都基于HLog实现，数据以追加的形式先写入HLog，再写入MemStore，如果RegionServer宕机，此时写入MemStore中但尚未Flush磁盘数据丢失，需要回访HLog补救丢失数据，HBase主从复制需要主集群将HLog日志发送给从集群，完成集群间复制</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/fb81be8e4743af98.png"></p>
<p>​                                                                               <center> （HLog文件结构）</center></p>
<p>HLog文件生成后不会永久存储在系统中，使命完成后，文件会失效最终删除</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/eebdaf515bf189bf.png"></p>
<p>​                                                                                <center>（HLog声明周期）</center></p>
<p><strong>MemStore：</strong>MemStore作为一个缓存级的存储组件，缓存最近写入的数据</p>
<p>此外HFile中KeyValue数据需按Key排序，根据有序Key建立索引树，提升数据读取效率，数据在落盘生成HFile前应完成排序，MemStore执行KeyValue排序</p>
<p><strong>HFile：</strong>MemStore数据落盘会形成文件写入HDFS，即HFile</p>
<p>HFile文件由各种不同类型的Block构成，虽然Block类型不同，但却有相同数据结构</p>
<p><strong>BlockCache</strong>：将数据块缓存在内存提高数据读取性能</p>
<p>(通过这种方式来提升读取性能)客户端通过读取某个Block，先检查Block是否存在于BlockCache，存在直接加载不存在从HFile中加载，加载后缓存到BlockCache。</p>
<h4 id="读写流程"><a href="#读写流程" class="headerlink" title="读写流程"></a>读写流程</h4><p><strong>写入流程</strong></p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/dc380798368f4b30.png"></p>
<p>​                                                                                <center>（HBase写入流程）</center></p>
<p>整体架构上可分为三阶段：</p>
<p>客户端处理：客户端将写入请求预处理，根据元数据定位到写入数据的RegionServer，将请求发送给对应RegionServer</p>
<p>Region写入：RegionServer收到请求将数据解析，先写入WAL，在写入对应Region列簇对应的MemStore</p>
<p>MemStore Flush：当Region中的MemStore达到一定阈值时，将会Flush操作将内存中数据写入文件(HFile)</p>
<p><strong>BulkLoad</strong></p>
<p>首先使用MapReduce将待写入数据转换为HFile文件，再直接将这些HFile文件加载到集群中，BulkLoad没有将写请求发送给RegionServer处理，有效避免了给RegionServer带来较大的写入压力</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/94168ab67f4cc9c0.png"></p>
<p>​                                                                                <center>(BulkLoad工作流程)</center></p>
<p>BulkLoad主要有两个阶段：</p>
<p>HFile生成阶段：会跑一个MR任务，mapper由自己实现，reducer由HBase负责，这阶段将为每个Region生成一个对应的HFile文件</p>
<p>HFile导入阶段：HFile准备好后，使用工具completebulkload将HFile加载到HBase集群。</p>
<p><strong>读取流程</strong></p>
<p>相比写流程，读数据却复杂的多，查询可能涉及到多个Region，多块缓存甚至多个数据存储文件；由于更新和删除操作的实现导致读取却加大了难度，读取过程需要根据版本进行过滤，对已经标记删除的数据也要进行过滤。</p>
<p>大致分为四个流程</p>
<p>Client-Server读取交互逻辑：Client首先会从ZooKeeper中获取元数据hbase:meta表所在的RegionServer，然后根据待读写rowkey发送请求到元数据所在RegionServer，获取数据所在的目标RegionServer和Region（并将这部分元数据信息缓存到本地），最后将请求进行封装发送到目标RegionServer进行处理。</p>
<p>Server端Scan框架体系：一次scan可能会同时扫描一张表的多个Region，客户端会根据hbase:meta元数据将扫描的起始区间[startKey, stopKey)进行切分，切分成多个互相独立的查询子区间，每个子区间对应一个Region，RegionServer接收到客户端的get/scan请求之后做了两件事情：首先构建scanner iterator体系；然后执行next函数获取KeyValue，并对其进行条件过滤。</p>
<p>过滤淘汰不符合查询条件的HFile：经过Scanner体系的构建，KeyValue此时已经可以由小到大依次经过KeyValueScanner获得，但这些KeyValue是否满足用户设定的TimeRange条件、版本号条件以及Filter条件还需要进一步的检查—&gt;根据KeyRange过滤；根据TimeRange过滤；根据布隆过滤器进行过滤</p>
<p>从HFile中读取待查找Key：根据HFile索引树定位目标Block；BlockCache中检索目标Block； HDFS文件中检索目标Block；从Block中读取待查找KeyValue</p>
<h4 id="Compaction实现"><a href="#Compaction实现" class="headerlink" title="Compaction实现"></a>Compaction实现</h4><p>我们知道HFile小文件过多会导致读取效率过低，所以Compaction应运而生，Compaction核心功能是将小文件合并成大文件，提升读取效率</p>
<p><strong>Compaction工作原理</strong></p>
<p>Compaction是从Region的一个Store中选部分HFile来合并，合并原理：先从待合并文件中读出keyvalue，再由小到大排序成一个新文件，新文件代替之前文件对外提供服务</p>
<p>HBase中有两种Compaction：Minor Compaction和Major Compaction</p>
<p>Minor模式选取部分小的相邻的HFile，合成一个大的HFile，Major模式是将一个Store中所有的HFile合并成一个HFile，还会清理三种无意义数据(被删除数据，TTL过期数据，版本号超过设定版本号数据)，Major Compaction消耗资源大，所以通常手动触发</p>
<p>Compaction核心作用：合并小文件，减少文件数，稳定随机读延迟；提高数据的本地化率；清除无效数据，减小数据存储量，当然它的副作用是会消耗一定系统资源，影响上层业务的读取响应</p>
<p>compaction基本流程</p>
<p>HBase将该Compaction交给一个独立线程处理，该线程首先从对应Store中选择合适HFile文件进行合并。</p>
<p><img src="https://i.bmp.ovh/imgs/2022/03/b5b5c60918ac87f9.png"></p>
<p>​                                                                                <center>Compaction基本流程</center></p>
<p>Compaction触发时机：MemStore Flush；后台线程周期性检查及手动触发</p>
<p>HFile文件合并执行：(先选待合并HFile集合,再选合适处理线程,接着执行合并流程)</p>
<ol>
<li>分别读出待合并HFile文件的KeyValue，归并排序处理后写到./tmp目录下临时文件中</li>
<li>将临时文件移动到对应Store的数据目录</li>
<li>将Compaction的输入文件路径和输出文件路径封装为KV写入HLog日志,打Compaction标记，最后强制sync</li>
<li>将对应Store数据目录下的Compaction输入文件全部删除</li>
</ol>
<h4 id="负载均衡实现"><a href="#负载均衡实现" class="headerlink" title="负载均衡实现"></a>负载均衡实现</h4><p><strong>Region迁移</strong></p>
<p>HBase系统中，分片迁移就是Region迁移，Region在迁移的过程中不需迁移实际数据，只需将读写服务迁移</p>
<p>迁移过程涉及到Region迁移过程涉及多种状态的改变；迁移过程中涉及Master、ZooKeeper（ZK）以及RegionServer等多个组件的相互协调。</p>
<p><img src="https://i.bmp.ovh/imgs/2022/03/d9e83ebcda3a28bd.png"></p>
<p>​                                                                    <center>Region状态</center></p>
<p>执行过程中，Region分为unassign阶段和assign阶段</p>
<p>unassign表示Region从源RegionServer上下线</p>
<p><img src="https://i.bmp.ovh/imgs/2022/03/f6bc751c1dc2bfff.png"></p>
<p>​                                                                            <center>Region unassign阶段</center></p>
<p>assign表示Region在目标RegionServer上上线</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00fd07wtfj30q00deab0.jpg"></p>
<p>​                                                                    <center>Region assign阶段</center></p>
<p>Region迁移操作伴随着Region状态的不断变迁：思考一下</p>
<p>为什么需要这些状态？</p>
<p>无论是unassign操作还是assign操作，都是由多个子操作组成，涉及多个组件的协调合作，只有通过记录Region状态才能知道当前unassign或者assign的进度，在异常发生后才能根据具体进度继续执行</p>
<p>如何管理这些状态？</p>
<p>Region的这些状态会存储在三个区域：meta表，Master内存，ZooKeeper的region-in-transition节点，并且作用不同，只有这三个状态保持一致，对应的Region才处于正常的工作状态</p>
<p><strong>Region合并</strong></p>
<p>在线合并功能将空闲Region与相邻的Region合并，减少集群中空闲Region的个数</p>
<p>合并流程涉及到以下操作：</p>
<ol>
<li>客户端发送merge请求给Master。</li>
<li>Master将待合并的所有Region都move到同一个RegionServer上。</li>
<li>Master发送merge请求给该RegionServer。</li>
<li>RegionServer启动一个本地事务执行merge操作。</li>
<li>merge操作将待合并的两个Region下线，并将两个Region的文件进行合并。</li>
<li>将这两个Region从hbase:meta中删除，并将新生成的Region添加到hbase:meta中。</li>
<li>将新生成的Region上线。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">merge_region &#x27;encoded_regionname&#x27;,&#x27;encoded_regionname&#x27;</span><br></pre></td></tr></table></figure>

<p><strong>Region分裂</strong></p>
<p>Region分裂是HBase最核心的功能之一，是实现分布式可扩展性的基础。HBase中，Region分裂有多种触发策略可以配置，一旦触发，HBase会寻找分裂点，然后执行真正的分裂操作</p>
<p>Region分裂触发策略：</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00fbwjmt0j30pp06mjrt.jpg"></p>
<p>​                                                                            <center>分裂触发策略</center></p>
<p>满足Region分裂策略之后就会触发Region分裂。分裂被触发后的第一件事是寻找分裂点，HBase对于分裂点的定义为：整个Region中最大Store中的最大文件中最中心的一个Block的首个rowkey。如果定位到的rowkey是整个文件的首个rowkey或者最后一个rowkey，则认为没有分裂点</p>
<p>HBase将整个分裂过程包装成了一个事务，目的是保证分裂事务的原子性。整个分裂事务过程分为三个阶段：prepare、execute和rollback</p>
<p>prepare阶段：在内存中初始化两个子Region，具体生成两个HRegionInfo对象，包含tableName、regionName、startkey、endkey等。同时会生成一个transaction journal，这个对象用来记录分裂的进展</p>
<p>execute阶段：</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00g7f5hbgj30p20i6mzi.jpg"></p>
<p>​                                                                                    <center>分裂核心操作</center></p>
<p>rollback阶段：如果execute阶段出现异常，则执行rollback操作，为了实现回滚，整个分裂过程分为很多子阶段，回滚程序会根据当前进展到哪个子阶段清理对应的垃圾数据。</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h00g93t12hj30q80ag763.jpg"></p>
<p>​                                                                                <center>Region分裂子阶段</center></p>
<h4 id="宕机恢复原理"><a href="#宕机恢复原理" class="headerlink" title="宕机恢复原理"></a>宕机恢复原理</h4><p>常见故障分析(RegionServer)：Full GC异常；HDFS异常；HBase bug；机器宕机。</p>
<p>故障恢复基本原理：</p>
<p>Master：采用基本的热备方式来实现Master高可用，</p>
<p>RegionServer：RegionServer发生宕机，HBase马上会检测到，并在检测到宕机后将宕机RegionServer上所有Region重新分配到集群其他正常RegionServer，再根据HLog恢复丢失数据。</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h00g0bqfc9j30hv0bot9g.jpg"></p>
<p>​                                                            <center>RegionServer故障恢复</center></p>
<p>Master检测RegionServer宕机：</p>
<p>HBase使用ZooKeeper协助Master检测RegionServer宕机，使用ZooKeeper临时节点机制，RegionServer注册成临时节点之后，Master会watch在/rs节点上（该节点下的所有子节点一旦发生离线就会通知Master），这样一旦RegionServer发生宕机，RegionServer注册到/rs节点下的临时节点就会离线，这个消息会马上通知给Master，Master检测到RegionServer宕机。</p>
<p> 切分未持久化数据的HLog：</p>
<p>日志回放需要以Region为单元进行，一个Region一个Region地回放，回放之前首先需要将HLog按照Region进行分组，每个Region的日志数据合并放在一起，方便后面按照Region进行回放。这个分组合并过程称为HLog切分。</p>
<ol>
<li>LogSplitting策略：HBase最初阶段日志切分的整个过程都由Master控制执行</li>
</ol>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h00ghvcu7cj30t00chab6.jpg"></p>
<p>​                                                                                    <center>LogSplitting</center></p>
<ol start="2">
<li>Distributed Log Splitting：Log Splitting的分布式实现，借助Master和所有RegionServer的计算能力进行日志切分，其中Master是协调者，RegionServer是实际的工作者</li>
</ol>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00gjapb9rj30s9080q3e.jpg"></p>
<p>​                                                                            <center>Distributed Log Splitting</center></p>
<p>Distributed Log Replay：</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h00gk7vr1ij30q202fglu.jpg"></p>
<p>​                                                                            <center>Distributed Log Replay</center></p>
<h4 id="备份与恢复"><a href="#备份与恢复" class="headerlink" title="备份与恢复"></a>备份与恢复</h4><p>使用Snapshot在线备份。Snapshot备份以快照技术为基础原理，备份过程不需要拷贝任何数据，因此对当前集群几乎没有任何影响，备份速度非常快而且可以保证数据一致性，Snapshot是HBase非常核心的一个功能，使用在线Snapshot备份可以满足用户很多需求，比如增量备份和数据迁移。</p>
<p><strong>在线Snapshot备份与恢复的用法</strong></p>
<p>snapshot：可以为表打一个快照，但并不涉及数据移动</p>
<blockquote>
<p>snapshot ‘sourceTable’,’snapshotName’</p>
</blockquote>
<p>restore_snapshot：用于恢复指定快照,恢复过程会替代原有数据,将表还原到快照点,快照点之后所有更新将会丢失</p>
<blockquote>
<p>restore_snapshot ‘snatshotName’</p>
</blockquote>
<p>clone_snapshot，可以根据快照恢复出一个新表，恢复过程不涉及数据移动，可以在秒级完成</p>
<blockquote>
<p>clone_snapshot ‘snatshotName’,’tableName’</p>
</blockquote>
<p>ExportSnapshot，可以将A集群的快照数据迁移到B集群。ExportSnapshot是HDFS层面的操作，需使用MapReduce进行数据的并行迁移，因此需要在开启MapReduce的机器上进行迁移</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00h79o51tj30ou03e3z7.jpg"></p>
<p><strong>Snatshot原理</strong></p>
<p>Snatshot流程：将MemStore中的缓存数据f lush到文件中；为所有HFile文件分别新建引用指针，这些指针元数据就是Snapshot。</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00h8k684kj30ps0bejsf.jpg"></p>
<p>​                                                                        <center>snatshot原理</center></p>
<p><strong>两阶段提交</strong></p>
<p>HBase使用两阶段提交（Two-Phase Commit，2PC）协议来保证Snapshot的分布式原子性</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h00haw7033j30ro0bcgmm.jpg"></p>
<p>​                                                                <center>两阶段提交原理</center></p>
<p><strong>Snapshot核心实现</strong></p>
<p> Region实现Snapshot</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h00hcn988gj30p70agq3u.jpg"></p>
<p>​                                                                <center>snatshot基本流程</center></p>
<blockquote>
<p>Region生成的Snapshot文件是临时文件，在/hbase/.hbase-snapshot/.tmp目录下，因为Snapshot过程通常特别快，所以很难看到单个Region生成的Snapshot文件。</p>
</blockquote>
<p>Master汇总所有Region Snapshot的结果：Master会在所有Region完成Snapshot之后执行一个汇总操作（consolidate），将所有regionsnapshot manifest汇总成一个单独manifest，汇总后的Snapshot文件可以在HDFS目录下看到的，路径为：/hbase/.hbase-snapshot/snapshotname/data.manifest。</p>
<p><strong>洞见clone_snapshot</strong></p>
<ol>
<li>预检查。确认当前目标表没有执行snapshot以及restore等操作，否则直接返回错误</li>
<li>在tmp文件夹下新建目标表目录并在表目录下新建.tabledesc文件，在该文件中写入表schema信息。</li>
<li>新建region目录。根据snapshot manifest中的信息新建Region相关目录以及HFile文件。</li>
<li>将表目录从tmp文件夹下移到HBase Root Location。</li>
<li>修改hbase:meta表，将克隆表的Region信息添加到hbase:meta表中，注意克隆表的Region名称和原数据表的Region名称并不相同（Region名称与table名称相关，table名不同，Region名则肯定不同）</li>
<li>将这些Region通过round-robin方式均匀分配到整个集群中，并在ZooKeeper上将克隆表的状态设置为enabled，正式对外提供服务。</li>
</ol>
<h4 id="面试题解"><a href="#面试题解" class="headerlink" title="面试题解"></a>面试题解</h4><p><strong>HBase如何写数据</strong></p>
<p>Client写入，存入MemStore一直到MemStore满，Flush成一个StoreFile增长到一定阈值，出发Compact合并，多个StoreFile合并成一个StoreFile同时进行版本合并和数据删除，当StoreFiles Compact后形成越来与大的StoreFile，单个StoreFile大小超过一定阈值(10G)触发split，把当前Region Split成两个Region，Region会下线，新Split除两个Region会被HMaster分配到相应HRegionServer，Region得以分压</p>
<p><strong>HDFS和HBase使用场景</strong></p>
<p>首先Hbase基于HDFS存储的</p>
<p>HDFS：一次写入多次读取，数据一致性，容错和恢复机制</p>
<p>HBase：瞬间写入量很大，数据需要长久保存，不适用于有join和多级索引及表关系复杂数据模型，大数据量且有快速随机访问的需求，业务场景简单</p>
<p><strong>HBase存储结构</strong></p>
<p>HBase表通过rowkey按照一定范围被分割成多个子表HRegion(一个HRegion超过256要分为2个)，由HRegionServer管理，HMaster管理HRegion</p>
<p>HRegion存取一个子表时会创建一个HRegion对象，对表的每个列簇创建一个store实例，每个store都有0或多个StoreFile对应，每个StoreFile对应一个HFile</p>
<p><strong>HBase RowKey设计原则</strong></p>
<p>长度原则：100 字节以内，8 的倍数最好，可能的情况下越短越好。</p>
<p>散列原则：高位散列，低位时间字段。避免热点问题。</p>
<p>唯一原则：分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一起</p>
<p><strong>HBase列簇设计</strong></p>
<p>原则：在合理范围内能尽量少的减少列簇就尽量减少列簇，因为列簇是共享 region 的，每个列簇数据相差太大导致查询效率低下</p>
<p>最优：将所有相关性很强的 key-value 都放在同一个列簇下，这样既能做到查 询效率最高，也能保持尽可能少的访问不同的磁盘文件。</p>
<p><strong>热点现象怎么解决</strong></p>
<p>某个小的时段内，对 HBase 的读写请求集中到极少数的 Region 上，导致这些 region 所在的 RegionServer 处理请求量骤增，负载量明显偏大，而其他的 RgionServer 明显空闲</p>
<p>热点发生在大量的 client 直接访问集群的一个或极少数个节点</p>
<ol>
<li>加盐：在 rowkey 的前面增加随机数，使得它和之前的 rowkey 的开头不同。加盐之后的 rowkey 就会根据随机生成的前缀分散到各个 region 上， 以避免热点</li>
<li>哈希：哈希可以使负载分散到整个集群，但是读却是可以预测的</li>
<li>反转：反转固定长度或者数字格式的 rowkey，可以使得 rowkey 中经常改变的部分（最没有意义的部分）放在前面。</li>
<li>时间戳反转</li>
<li>HBase建表预分区：创建 HBase 表时，就预先根据可能的 RowKey 划分出多 个 region 而不是默认的一个，从而可以将后续的读写操作负载均衡到不 同的 region 上，避免热点现象。</li>
</ol>
<p><strong>HBase compact（minor major）</strong></p>
<p>在 hbase 中每当有 memstore 数据 flush 到磁盘之后，就形成一个 storefile， 当 storeFile 的数量达到一定程度就需要将 storefile 文件来进行 compaction</p>
<ol>
<li><p>合并文件</p>
</li>
<li><p>清洗过期多余版本数据</p>
</li>
<li><p>提高读写数据的效率</p>
</li>
<li><p>minor用来部分文件的合并以及包括minVersion=0并且设置ttl的过期版本清理，不做删除数据，多版本数据清理</p>
</li>
<li><p>major对Region下的HStore下所有StoreFile执行合并，最终合并出一个文件</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive</title>
    <url>/2022/03/07/Hive/</url>
    <content><![CDATA[]]></content>
  </entry>
</search>

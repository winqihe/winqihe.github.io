<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HBase</title>
    <url>/2022/03/05/HBase/</url>
    <content><![CDATA[<h2 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h2><p>本文内容摘录于《HBase原理与实践》</p>
<span id="more"></span>

<h4 id="概述一览"><a href="#概述一览" class="headerlink" title="概述一览"></a>概述一览</h4><p><strong>基本概念：</strong></p>
<p>table：表，一个表包含多行数据。</p>
<p>row：行，一行数据包含一个唯一标识rowkey、多个column以及对应的值。在HBase中，一张表中所有row都按照rowkey的字典序由小到大排序。</p>
<p>column：列，HBase中的column由column family（列簇）以及qualif ier（列名）两部分组成，两者中间使用”:”相连。column family在表创建的时候需要指定，用户不能随意增减。一个column family下可以设置任意多个qualif ier，因此可以理解为HBase中的列可以动态增加，理论上甚至可以扩展到上百万列。</p>
<p>timestamp：时间戳，每个cell在写入HBase的时候都会默认分配一个时间戳作为该cell的版本，当然，用户也可以在写入的时候自带时间戳。HBase支持多版本特性，即同一rowkey、column下可以有多个value存在，这些value使用timestamp作为版本号，版本越大，表示数据越新。</p>
<p>cell：单元格，由五元组（row, column, timestamp, type, value）组成的结构，其中type表示Put/Delete这样的操作类型，timestamp代表这个cell的版本。这个结构在数据库中实际是以KV结构存储的，其中（row, column, timestamp, type）是K，value字段对应KV结构的V。</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/078bfb0511e176a0.png"></p>
<p>​                                                                            <center>（HBase逻辑视图）</center></p>
<p><strong>体系架构：</strong></p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/0576dba2ceb0e8a3.png"></p>
<p>​                                                                                <center>（体系架构）</center>                                            </p>
<ol>
<li><p>Client：丰富的编程接口，Client访问数据行前要先通过元数据表定位目标数据所在RegionServer，发请求给RegionServer，这些元数据也会被缓存到本地。</p>
</li>
<li><p>Zookeeper：实现Master高可用(异常宕机进行选举)，管理系统核心元数据(保存元数据表所在的RegionServer地址)，参与RegionServer宕机恢复(心跳机制)，实现分布式表锁</p>
</li>
<li><p>Master：处理各种管理请求(建表、修改表、权限操作、切分表、合并数据分片以及Compaction)，管理集群中的RegionServer(RegionServer中Region的负载均衡、RegionServer的宕机恢复以及Region的迁移)，清理过期日志以及文件，Master会每隔一段时间检查HDFS中HLog是否过期、HFile是否已经被删除，并在过期之后将其删除</p>
</li>
<li><p>RegionServer：响应用户的IO请求，由WAL(HLog),BlockCache,多个Region构成</p>
<p>WAL(HLog):实现数据高可靠性，数据写入缓存前先顺序写入HLog，实现HBase集群间主从复制通过回放主集群推送过来的HLog日志实现主从复制。</p>
<p>BlockCache:HBase系统中的读缓存，客户端从磁盘读取数据之后通常会将数据缓存到系统内存中，后续访问同一行数据可以直接从内存中获取而不需要访问磁盘。</p>
<p>Region:数据表的一个分片，当数据表大小超过阈值就会水平切分，分裂为两个Region，一个Region由一个或多个Store构成，Store个数取决于列簇个数，每个列簇的数据存放在一起形成一个存储单元Store，每个Store由一个MemStore和一或多个HFile组成，写入数据先写入MemStore，到一定阈值(128M)将Flush成HFile，随着HFile数越来越多，将会执行Compact合并成大文件</p>
</li>
<li><p>HDFS：HBase底层依赖HDFS存储数据，用户数据文件、HLog日志文件等最终都会写入HDFS落盘</p>
</li>
</ol>
<p><strong>HBase特性：</strong></p>
<p>优点：容量大，良好可扩展性，稀疏性，高性能，多版本，支持过期，Hadoop原生支持</p>
<p>缺点：不支持复杂聚合运算，没有二级索引功能，不支持全局跨行事务</p>
<h4 id="依赖服务"><a href="#依赖服务" class="headerlink" title="依赖服务"></a>依赖服务</h4><p><strong>ZooKeeper</strong>：一个分布式HBase集群的部署运行强烈依赖于ZooKeeper</p>
<p><strong>HDFS</strong>：HBase的文件都存放在HDFS（Hadoop Distribuited File System）文件系统上</p>
<h4 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h4><p>通过ThriftServer的Java HBase客户端来请求HBase集群。</p>
<p>定位Meta表：</p>
<p>客户端在做任何数据操作时，都要先确定数据在哪个Region上，然后再根据Region的RegionServer信息，去对应的RegionServer上读取数据，HBase系统内部设计了一张特殊的表——hbase:meta表，专门用来存放整个集群所有的Region信息。hbase:meta中的hbase指的是namespace，HBase容许针对不同的业务设计不同的namespace，系统表采用统一的namespace，即hbase；meta指的是hbase这个namespace下的表名。</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h00gqa64zqj30r507vdgm.jpg"></p>
<p>​                                                                    <center>Hbase:meta结构</center></p>
<p><font color="red"> HBase作为一个分布式数据库系统，一个大的集群可能承担数千万的查询写入请求，而hbase:meta表只有一个Region，如果所有的流量都先请求hbase:meta表找到Region，再请求Region所在的RegionServer，那么hbase:meta表的将承载巨大的压力，这个Region将马上成为热点Region，且根本无法承担数千万的流量。那么，如何解决这个问题呢</font></p>
<p>把hbase:meta表的Region信息缓存在HBase客户端</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h00gu71x0zj30q309ldgz.jpg"></p>
<p>​                                                                                <center>客户端定义Region</center></p>
<p>少量写和批量写：</p>
<p>HBase提供了3种常见的数据写入API：</p>
<p>table.put(put)：在服务端先写WAL，然后写MemStore，一旦MemStore写满就f lush到磁盘上。特点是，默认每次写入都需要执行一次RPC和磁盘持久化。因此，写入吞吐量受限于磁盘带宽、网络带宽以及f lush的速度。但它能保证每次写入操作都持久化到磁盘，不会有任何数据丢失。最重要的是，它能保证put操作的原子性。</p>
<p>table.put(List<Put> puts)：在客户端缓存put，等凑足了一批put，就将这些数据打包成一次RPC发送到服务端，一次性写WAL，并写MemStore。相比第一种方式省去了多次往返RPC以及多次刷盘的开销，吞吐量大大提升。不过，这个RPC操作耗时一般都会长一点，因为一次写入了多行数据。这些put中，可能有一部分失败，失败的那些put操作会经历若干次重试</Put></p>
<p>bulk load：直接将待写入数据生成HFile，将这些HFile直接加载到对应的Region下的CF内。在生成HFile时，在HBase服务端没有任何RPC调用，只在load HFile时会调用RPC，一种完全离线的快速写入方式。bulk load应该是最快的批量写手段，同时不会对线上的集群产生巨大压力。在load完HFile之后，CF内部会进行Compaction，但是Compaction是异步的且可以限速，所以产生的IO压力是可控的。bulk load对线上集群非常友好。</p>
<h4 id="RegionServer结构"><a href="#RegionServer结构" class="headerlink" title="RegionServer结构"></a>RegionServer结构</h4><p>一个RegionServer由一或多个HLog，BlockCache及多个Region组成，RegionServer负责Region的数据读写，一个Region由多个store组成，Store存放对应列簇数据，Store包含一个MemStore和多个HFile。</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/8343fa265a93d486.png"></p>
<p>​                                                                                <center>（RegionServer结构）</center></p>
<p><strong>HLog</strong>：用来保证数据写入的可靠性</p>
<p>HBase中系统故障恢复及主从复制都基于HLog实现，数据以追加的形式先写入HLog，再写入MemStore，如果RegionServer宕机，此时写入MemStore中但尚未Flush磁盘数据丢失，需要回访HLog补救丢失数据，HBase主从复制需要主集群将HLog日志发送给从集群，完成集群间复制</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/fb81be8e4743af98.png"></p>
<p>​                                                                               <center> （HLog文件结构）</center></p>
<p>HLog文件生成后不会永久存储在系统中，使命完成后，文件会失效最终删除</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/eebdaf515bf189bf.png"></p>
<p>​                                                                                <center>（HLog声明周期）</center></p>
<p><strong>MemStore：</strong>MemStore作为一个缓存级的存储组件，缓存最近写入的数据</p>
<p>此外HFile中KeyValue数据需按Key排序，根据有序Key建立索引树，提升数据读取效率，数据在落盘生成HFile前应完成排序，MemStore执行KeyValue排序</p>
<p><strong>HFile：</strong>MemStore数据落盘会形成文件写入HDFS，即HFile</p>
<p>HFile文件由各种不同类型的Block构成，虽然Block类型不同，但却有相同数据结构</p>
<p><strong>BlockCache</strong>：将数据块缓存在内存提高数据读取性能</p>
<p>(通过这种方式来提升读取性能)客户端通过读取某个Block，先检查Block是否存在于BlockCache，存在直接加载不存在从HFile中加载，加载后缓存到BlockCache。</p>
<h4 id="读写流程"><a href="#读写流程" class="headerlink" title="读写流程"></a>读写流程</h4><p><strong>写入流程</strong></p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/dc380798368f4b30.png"></p>
<p>​                                                                                <center>（HBase写入流程）</center></p>
<p>整体架构上可分为三阶段：</p>
<p>客户端处理：客户端将写入请求预处理，根据元数据定位到写入数据的RegionServer，将请求发送给对应RegionServer</p>
<p>Region写入：RegionServer收到请求将数据解析，先写入WAL，在写入对应Region列簇对应的MemStore</p>
<p>MemStore Flush：当Region中的MemStore达到一定阈值时，将会Flush操作将内存中数据写入文件(HFile)</p>
<p><strong>BulkLoad</strong></p>
<p>首先使用MapReduce将待写入数据转换为HFile文件，再直接将这些HFile文件加载到集群中，BulkLoad没有将写请求发送给RegionServer处理，有效避免了给RegionServer带来较大的写入压力</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/94168ab67f4cc9c0.png"></p>
<p>​                                                                                <center>(BulkLoad工作流程)</center></p>
<p>BulkLoad主要有两个阶段：</p>
<p>HFile生成阶段：会跑一个MR任务，mapper由自己实现，reducer由HBase负责，这阶段将为每个Region生成一个对应的HFile文件</p>
<p>HFile导入阶段：HFile准备好后，使用工具completebulkload将HFile加载到HBase集群。</p>
<p><strong>读取流程</strong></p>
<p>相比写流程，读数据却复杂的多，查询可能涉及到多个Region，多块缓存甚至多个数据存储文件；由于更新和删除操作的实现导致读取却加大了难度，读取过程需要根据版本进行过滤，对已经标记删除的数据也要进行过滤。</p>
<p>大致分为四个流程</p>
<p>Client-Server读取交互逻辑：Client首先会从ZooKeeper中获取元数据hbase:meta表所在的RegionServer，然后根据待读写rowkey发送请求到元数据所在RegionServer，获取数据所在的目标RegionServer和Region（并将这部分元数据信息缓存到本地），最后将请求进行封装发送到目标RegionServer进行处理。</p>
<p>Server端Scan框架体系：一次scan可能会同时扫描一张表的多个Region，客户端会根据hbase:meta元数据将扫描的起始区间[startKey, stopKey)进行切分，切分成多个互相独立的查询子区间，每个子区间对应一个Region，RegionServer接收到客户端的get/scan请求之后做了两件事情：首先构建scanner iterator体系；然后执行next函数获取KeyValue，并对其进行条件过滤。</p>
<p>过滤淘汰不符合查询条件的HFile：经过Scanner体系的构建，KeyValue此时已经可以由小到大依次经过KeyValueScanner获得，但这些KeyValue是否满足用户设定的TimeRange条件、版本号条件以及Filter条件还需要进一步的检查—&gt;根据KeyRange过滤；根据TimeRange过滤；根据布隆过滤器进行过滤</p>
<p>从HFile中读取待查找Key：根据HFile索引树定位目标Block；BlockCache中检索目标Block； HDFS文件中检索目标Block；从Block中读取待查找KeyValue</p>
<h4 id="Compaction实现"><a href="#Compaction实现" class="headerlink" title="Compaction实现"></a>Compaction实现</h4><p>我们知道HFile小文件过多会导致读取效率过低，所以Compaction应运而生，Compaction核心功能是将小文件合并成大文件，提升读取效率</p>
<p><strong>Compaction工作原理</strong></p>
<p>Compaction是从Region的一个Store中选部分HFile来合并，合并原理：先从待合并文件中读出keyvalue，再由小到大排序成一个新文件，新文件代替之前文件对外提供服务</p>
<p>HBase中有两种Compaction：Minor Compaction和Major Compaction</p>
<p>Minor模式选取部分小的相邻的HFile，合成一个大的HFile，Major模式是将一个Store中所有的HFile合并成一个HFile，还会清理三种无意义数据(被删除数据，TTL过期数据，版本号超过设定版本号数据)，Major Compaction消耗资源大，所以通常手动触发</p>
<p>Compaction核心作用：合并小文件，减少文件数，稳定随机读延迟；提高数据的本地化率；清除无效数据，减小数据存储量，当然它的副作用是会消耗一定系统资源，影响上层业务的读取响应</p>
<p>compaction基本流程</p>
<p>HBase将该Compaction交给一个独立线程处理，该线程首先从对应Store中选择合适HFile文件进行合并。</p>
<p><img src="https://i.bmp.ovh/imgs/2022/03/b5b5c60918ac87f9.png"></p>
<p>​                                                                                <center>Compaction基本流程</center></p>
<p>Compaction触发时机：MemStore Flush；后台线程周期性检查及手动触发</p>
<p>HFile文件合并执行：(先选待合并HFile集合,再选合适处理线程,接着执行合并流程)</p>
<ol>
<li>分别读出待合并HFile文件的KeyValue，归并排序处理后写到./tmp目录下临时文件中</li>
<li>将临时文件移动到对应Store的数据目录</li>
<li>将Compaction的输入文件路径和输出文件路径封装为KV写入HLog日志,打Compaction标记，最后强制sync</li>
<li>将对应Store数据目录下的Compaction输入文件全部删除</li>
</ol>
<h4 id="负载均衡实现"><a href="#负载均衡实现" class="headerlink" title="负载均衡实现"></a>负载均衡实现</h4><p><strong>Region迁移</strong></p>
<p>HBase系统中，分片迁移就是Region迁移，Region在迁移的过程中不需迁移实际数据，只需将读写服务迁移</p>
<p>迁移过程涉及到Region迁移过程涉及多种状态的改变；迁移过程中涉及Master、ZooKeeper（ZK）以及RegionServer等多个组件的相互协调。</p>
<p><img src="https://i.bmp.ovh/imgs/2022/03/d9e83ebcda3a28bd.png"></p>
<p>​                                                                    <center>Region状态</center></p>
<p>执行过程中，Region分为unassign阶段和assign阶段</p>
<p>unassign表示Region从源RegionServer上下线</p>
<p><img src="https://i.bmp.ovh/imgs/2022/03/f6bc751c1dc2bfff.png"></p>
<p>​                                                                            <center>Region unassign阶段</center></p>
<p>assign表示Region在目标RegionServer上上线</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00fd07wtfj30q00deab0.jpg"></p>
<p>​                                                                    <center>Region assign阶段</center></p>
<p>Region迁移操作伴随着Region状态的不断变迁：思考一下</p>
<p>为什么需要这些状态？</p>
<p>无论是unassign操作还是assign操作，都是由多个子操作组成，涉及多个组件的协调合作，只有通过记录Region状态才能知道当前unassign或者assign的进度，在异常发生后才能根据具体进度继续执行</p>
<p>如何管理这些状态？</p>
<p>Region的这些状态会存储在三个区域：meta表，Master内存，ZooKeeper的region-in-transition节点，并且作用不同，只有这三个状态保持一致，对应的Region才处于正常的工作状态</p>
<p><strong>Region合并</strong></p>
<p>在线合并功能将空闲Region与相邻的Region合并，减少集群中空闲Region的个数</p>
<p>合并流程涉及到以下操作：</p>
<ol>
<li>客户端发送merge请求给Master。</li>
<li>Master将待合并的所有Region都move到同一个RegionServer上。</li>
<li>Master发送merge请求给该RegionServer。</li>
<li>RegionServer启动一个本地事务执行merge操作。</li>
<li>merge操作将待合并的两个Region下线，并将两个Region的文件进行合并。</li>
<li>将这两个Region从hbase:meta中删除，并将新生成的Region添加到hbase:meta中。</li>
<li>将新生成的Region上线。</li>
</ol>
<figure class="highlight nginx"><table><tr><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">merge_region</span> <span class="hljs-string">&#x27;encoded_regionname&#x27;</span>,<span class="hljs-string">&#x27;encoded_regionname&#x27;</span><br></code></pre></td></tr></table></figure>

<p><strong>Region分裂</strong></p>
<p>Region分裂是HBase最核心的功能之一，是实现分布式可扩展性的基础。HBase中，Region分裂有多种触发策略可以配置，一旦触发，HBase会寻找分裂点，然后执行真正的分裂操作</p>
<p>Region分裂触发策略：</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00fbwjmt0j30pp06mjrt.jpg"></p>
<p>​                                                                            <center>分裂触发策略</center></p>
<p>满足Region分裂策略之后就会触发Region分裂。分裂被触发后的第一件事是寻找分裂点，HBase对于分裂点的定义为：整个Region中最大Store中的最大文件中最中心的一个Block的首个rowkey。如果定位到的rowkey是整个文件的首个rowkey或者最后一个rowkey，则认为没有分裂点</p>
<p>HBase将整个分裂过程包装成了一个事务，目的是保证分裂事务的原子性。整个分裂事务过程分为三个阶段：prepare、execute和rollback</p>
<p>prepare阶段：在内存中初始化两个子Region，具体生成两个HRegionInfo对象，包含tableName、regionName、startkey、endkey等。同时会生成一个transaction journal，这个对象用来记录分裂的进展</p>
<p>execute阶段：</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00g7f5hbgj30p20i6mzi.jpg"></p>
<p>​                                                                                    <center>分裂核心操作</center></p>
<p>rollback阶段：如果execute阶段出现异常，则执行rollback操作，为了实现回滚，整个分裂过程分为很多子阶段，回滚程序会根据当前进展到哪个子阶段清理对应的垃圾数据。</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h00g93t12hj30q80ag763.jpg"></p>
<p>​                                                                                <center>Region分裂子阶段</center></p>
<h4 id="宕机恢复原理"><a href="#宕机恢复原理" class="headerlink" title="宕机恢复原理"></a>宕机恢复原理</h4><p>常见故障分析(RegionServer)：Full GC异常；HDFS异常；HBase bug；机器宕机。</p>
<p>故障恢复基本原理：</p>
<p>Master：采用基本的热备方式来实现Master高可用，</p>
<p>RegionServer：RegionServer发生宕机，HBase马上会检测到，并在检测到宕机后将宕机RegionServer上所有Region重新分配到集群其他正常RegionServer，再根据HLog恢复丢失数据。</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h00g0bqfc9j30hv0bot9g.jpg"></p>
<p>​                                                            <center>RegionServer故障恢复</center></p>
<p>Master检测RegionServer宕机：</p>
<p>HBase使用ZooKeeper协助Master检测RegionServer宕机，使用ZooKeeper临时节点机制，RegionServer注册成临时节点之后，Master会watch在/rs节点上（该节点下的所有子节点一旦发生离线就会通知Master），这样一旦RegionServer发生宕机，RegionServer注册到/rs节点下的临时节点就会离线，这个消息会马上通知给Master，Master检测到RegionServer宕机。</p>
<p> 切分未持久化数据的HLog：</p>
<p>日志回放需要以Region为单元进行，一个Region一个Region地回放，回放之前首先需要将HLog按照Region进行分组，每个Region的日志数据合并放在一起，方便后面按照Region进行回放。这个分组合并过程称为HLog切分。</p>
<ol>
<li>LogSplitting策略：HBase最初阶段日志切分的整个过程都由Master控制执行</li>
</ol>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h00ghvcu7cj30t00chab6.jpg"></p>
<p>​                                                                                    <center>LogSplitting</center></p>
<ol start="2">
<li>Distributed Log Splitting：Log Splitting的分布式实现，借助Master和所有RegionServer的计算能力进行日志切分，其中Master是协调者，RegionServer是实际的工作者</li>
</ol>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00gjapb9rj30s9080q3e.jpg"></p>
<p>​                                                                            <center>Distributed Log Splitting</center></p>
<p>Distributed Log Replay：</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h00gk7vr1ij30q202fglu.jpg"></p>
<p>​                                                                            <center>Distributed Log Replay</center></p>
<h4 id="备份与恢复"><a href="#备份与恢复" class="headerlink" title="备份与恢复"></a>备份与恢复</h4><p>使用Snapshot在线备份。Snapshot备份以快照技术为基础原理，备份过程不需要拷贝任何数据，因此对当前集群几乎没有任何影响，备份速度非常快而且可以保证数据一致性，Snapshot是HBase非常核心的一个功能，使用在线Snapshot备份可以满足用户很多需求，比如增量备份和数据迁移。</p>
<p><strong>在线Snapshot备份与恢复的用法</strong></p>
<p>snapshot：可以为表打一个快照，但并不涉及数据移动</p>
<blockquote>
<p>snapshot ‘sourceTable’,’snapshotName’</p>
</blockquote>
<p>restore_snapshot：用于恢复指定快照,恢复过程会替代原有数据,将表还原到快照点,快照点之后所有更新将会丢失</p>
<blockquote>
<p>restore_snapshot ‘snatshotName’</p>
</blockquote>
<p>clone_snapshot，可以根据快照恢复出一个新表，恢复过程不涉及数据移动，可以在秒级完成</p>
<blockquote>
<p>clone_snapshot ‘snatshotName’,’tableName’</p>
</blockquote>
<p>ExportSnapshot，可以将A集群的快照数据迁移到B集群。ExportSnapshot是HDFS层面的操作，需使用MapReduce进行数据的并行迁移，因此需要在开启MapReduce的机器上进行迁移</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00h79o51tj30ou03e3z7.jpg"></p>
<p><strong>Snatshot原理</strong></p>
<p>Snatshot流程：将MemStore中的缓存数据f lush到文件中；为所有HFile文件分别新建引用指针，这些指针元数据就是Snapshot。</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00h8k684kj30ps0bejsf.jpg"></p>
<p>​                                                                        <center>snatshot原理</center></p>
<p><strong>两阶段提交</strong></p>
<p>HBase使用两阶段提交（Two-Phase Commit，2PC）协议来保证Snapshot的分布式原子性</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h00haw7033j30ro0bcgmm.jpg"></p>
<p>​                                                                <center>两阶段提交原理</center></p>
<p><strong>Snapshot核心实现</strong></p>
<p> Region实现Snapshot</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h00hcn988gj30p70agq3u.jpg"></p>
<p>​                                                                <center>snatshot基本流程</center></p>
<blockquote>
<p>Region生成的Snapshot文件是临时文件，在/hbase/.hbase-snapshot/.tmp目录下，因为Snapshot过程通常特别快，所以很难看到单个Region生成的Snapshot文件。</p>
</blockquote>
<p>Master汇总所有Region Snapshot的结果：Master会在所有Region完成Snapshot之后执行一个汇总操作（consolidate），将所有regionsnapshot manifest汇总成一个单独manifest，汇总后的Snapshot文件可以在HDFS目录下看到的，路径为：/hbase/.hbase-snapshot/snapshotname/data.manifest。</p>
<p><strong>洞见clone_snapshot</strong></p>
<ol>
<li>预检查。确认当前目标表没有执行snapshot以及restore等操作，否则直接返回错误</li>
<li>在tmp文件夹下新建目标表目录并在表目录下新建.tabledesc文件，在该文件中写入表schema信息。</li>
<li>新建region目录。根据snapshot manifest中的信息新建Region相关目录以及HFile文件。</li>
<li>将表目录从tmp文件夹下移到HBase Root Location。</li>
<li>修改hbase:meta表，将克隆表的Region信息添加到hbase:meta表中，注意克隆表的Region名称和原数据表的Region名称并不相同（Region名称与table名称相关，table名不同，Region名则肯定不同）</li>
<li>将这些Region通过round-robin方式均匀分配到整个集群中，并在ZooKeeper上将克隆表的状态设置为enabled，正式对外提供服务。</li>
</ol>
<h4 id="面试题解"><a href="#面试题解" class="headerlink" title="面试题解"></a>面试题解</h4><p><strong>HBase如何写数据</strong></p>
<p>Client写入，存入MemStore一直到MemStore满，Flush成一个StoreFile增长到一定阈值，出发Compact合并，多个StoreFile合并成一个StoreFile同时进行版本合并和数据删除，当StoreFiles Compact后形成越来与大的StoreFile，单个StoreFile大小超过一定阈值(10G)触发split，把当前Region Split成两个Region，Region会下线，新Split除两个Region会被HMaster分配到相应HRegionServer，Region得以分压</p>
<p><strong>HDFS和HBase使用场景</strong></p>
<p>首先Hbase基于HDFS存储的</p>
<p>HDFS：一次写入多次读取，数据一致性，容错和恢复机制</p>
<p>HBase：瞬间写入量很大，数据需要长久保存，不适用于有join和多级索引及表关系复杂数据模型，大数据量且有快速随机访问的需求，业务场景简单</p>
<p><strong>HBase存储结构</strong></p>
<p>HBase表通过rowkey按照一定范围被分割成多个子表HRegion(一个HRegion超过256要分为2个)，由HRegionServer管理，HMaster管理HRegion</p>
<p>HRegion存取一个子表时会创建一个HRegion对象，对表的每个列簇创建一个store实例，每个store都有0或多个StoreFile对应，每个StoreFile对应一个HFile</p>
<p><strong>HBase RowKey设计原则</strong></p>
<p>长度原则：100 字节以内，8 的倍数最好，可能的情况下越短越好。</p>
<p>散列原则：高位散列，低位时间字段。避免热点问题。</p>
<p>唯一原则：分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一起</p>
<p><strong>HBase列簇设计</strong></p>
<p>原则：在合理范围内能尽量少的减少列簇就尽量减少列簇，因为列簇是共享 region 的，每个列簇数据相差太大导致查询效率低下</p>
<p>最优：将所有相关性很强的 key-value 都放在同一个列簇下，这样既能做到查 询效率最高，也能保持尽可能少的访问不同的磁盘文件。</p>
<p><strong>热点现象怎么解决</strong></p>
<p>某个小的时段内，对 HBase 的读写请求集中到极少数的 Region 上，导致这些 region 所在的 RegionServer 处理请求量骤增，负载量明显偏大，而其他的 RgionServer 明显空闲</p>
<p>热点发生在大量的 client 直接访问集群的一个或极少数个节点</p>
<ol>
<li>加盐：在 rowkey 的前面增加随机数，使得它和之前的 rowkey 的开头不同。加盐之后的 rowkey 就会根据随机生成的前缀分散到各个 region 上， 以避免热点</li>
<li>哈希：哈希可以使负载分散到整个集群，但是读却是可以预测的</li>
<li>反转：反转固定长度或者数字格式的 rowkey，可以使得 rowkey 中经常改变的部分（最没有意义的部分）放在前面。</li>
<li>时间戳反转</li>
<li>HBase建表预分区：创建 HBase 表时，就预先根据可能的 RowKey 划分出多 个 region 而不是默认的一个，从而可以将后续的读写操作负载均衡到不 同的 region 上，避免热点现象。</li>
</ol>
<p><strong>HBase compact（minor major）</strong></p>
<p>在 hbase 中每当有 memstore 数据 flush 到磁盘之后，就形成一个 storefile， 当 storeFile 的数量达到一定程度就需要将 storefile 文件来进行 compaction</p>
<ol>
<li><p>合并文件</p>
</li>
<li><p>清洗过期多余版本数据</p>
</li>
<li><p>提高读写数据的效率</p>
</li>
<li><p>minor用来部分文件的合并以及包括minVersion=0并且设置ttl的过期版本清理，不做删除数据，多版本数据清理</p>
</li>
<li><p>major对Region下的HStore下所有StoreFile执行合并，最终合并出一个文件</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop</title>
    <url>/2022/03/03/Hadoop/</url>
    <content><![CDATA[<p><strong>HDFS MapReduce YARN</strong></p>
<span id="more"></span>

<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h01kfgx6g2j30ud0gp0u5.jpg"></p>
<h4 id="HDFS概述"><a href="#HDFS概述" class="headerlink" title="HDFS概述"></a>HDFS概述</h4><p><strong>HDFS优缺点</strong></p>
<p>优点：高容错性，适合大规模数据处理，廉价成本</p>
<p>缺点：不适合低延时数据访问，无法高效对大量小文件存储，不支持并发写入，文件随即修改。</p>
<p><strong>HDFS组成架构</strong></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h011gaq5xej30te0lu76b.jpg"></p>
<p>NameNode:管理HDFS名称空间，配置副本策略，管理数据块，处理客户端请求</p>
<p>DataNode:存储实际数据块，执行数据块的读写操作，</p>
<p>Client:文件切分，与NameNode交互获取文件位置信息，与DataNode交互，读写数据</p>
<p>SecondaryNameNode:辅助NameNode为其分担工作量(定期合并FSImage和Edit Log)，辅助恢复NameNode</p>
<p><strong>元数据格式</strong></p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h0128e6g95j30nr0amjss.jpg"></p>
<p><strong>Hadoop文件块大小</strong>默认为128M，文件块太小，会增加寻址时间；文件块太大，从磁盘传输数据的时间将大于定位数据块起始位置所需的时间，导致程序处理数据慢</p>
<p><strong>机架感知(副本节点选择)</strong></p>
<ol>
<li>第一个副本在client所处的节点上。如果客户端在集群外，随机选一个</li>
<li>第二个副本和第一个副本位于相同机架，随机节点。</li>
<li>第三个副本位于不同机架，随机节点</li>
</ol>
<p><strong>常用命令</strong></p>
<blockquote>
<p>-moveFromLocal,-copyFromLocal,-put,-appendToFile,-copyToLocal,-get,-ls,-cat,-cp,-mkdir,-mv,-tail,-rm,-rm -r,-du,-setrep</p>
</blockquote>
<h4 id="HDFS读写流程"><a href="#HDFS读写流程" class="headerlink" title="HDFS读写流程"></a>HDFS读写流程</h4><p><strong>HDFS写数据</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h011xz364sj30gz0b1dgm.jpg"></p>
<ol>
<li>Client向NameNode请求上传文件</li>
<li>NameNode返回是否可以上传</li>
<li>Client请求第一个Block上传到那个DataNode</li>
<li>NameNode返回3个DataNode节点</li>
<li>Client请求datanode1上传数据，datanode1收到请求会继续调用datanode2，datanode2调用datanode3，将通信管道建立完</li>
<li>datanode1，2，3逐级应答Client</li>
<li>Client向datanode1上传第一个Block(Packet)，先从磁盘读数据放到本地进行缓存，datanode1收到一个Packet就会传给datanode2，datanode2传给datanode3，</li>
<li>当一个Block传输完，Client再次请求上传第二个Block</li>
</ol>
<p><strong>HDFS读数据</strong></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h011x3wtgbj30ku0c5q3y.jpg"></p>
<ol>
<li>Client向NameNode请求下载文件，NameNode查询元数据，定位文件块所在DataNode地址</li>
<li>从(先就近后随机)DataNode请求读取数据</li>
<li>DataNode传输数据给Client(Packet)</li>
<li>Client(Packet)接受，先本地缓存后写入目标文件</li>
</ol>
<h4 id="NN和2NN工作机制"><a href="#NN和2NN工作机制" class="headerlink" title="NN和2NN工作机制"></a>NN和2NN工作机制</h4><p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h0143iqerxj30qw0d8tby.jpg"></p>
<p>NameNode启动：</p>
<ol>
<li>首次启动格式化，创建Fsimage，Edits，之后启动load edits和fsimage到内存</li>
<li>Client对元数据进行增删改请求</li>
<li>NameNode记录操作日志</li>
<li>NameNode在内存中对数据增删改</li>
</ol>
<p>SecondaryNode工作：</p>
<ol>
<li>2NN询问NN是否要CheckPoint</li>
<li>2NN请求执行CheckPoint</li>
<li>NN滚动正在写的Edits，将滚动前的edits和fsimage拷贝到2NN</li>
<li>2NN加载edits和fsimage到内存，并进行合并，生成新的fsimage.checkpoint</li>
<li>拷贝fsimage.checkpoint到NN，NN将fsimage.checkpoint更名为fsimage</li>
</ol>
<p><strong>FSImage和Edit Log</strong></p>
<p>fsimage保存了最新的元数据检查点，包含了整个HDFS文件系统的所有目录和文件的信息。</p>
<p>editlog在NameNode已经启动情况下对HDFS进行的各种更新操作进行记录，HDFS客户端执行所有的写操作都会被记录到editlog中</p>
<p>namenode在被格式化后会在/data/tmp/dfs/name/current目录下产生相应文件：</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h0148rc90jj30ba02g3yg.jpg"></p>
<h4 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h4><p>​    一个数据块在DataNode上以文件形式存储在磁盘，分别是数据本身和元数据(数据块长度，数据块校验和，时间戳)</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h013jtur26j30cf022aa0.jpg"></p>
<ol>
<li>DataNode启动后向NameNode注册，通过后周期性(1h)向NameNode上报所有块信息</li>
<li>心跳3s/次，心跳返回结果带有NameNode给DataNode的命令，若超过10min没有收到某个DataNode的心跳，则认为节点已挂</li>
<li>集群运行中可以服役新节点或退役旧节点</li>
</ol>
<p><strong>Client获取块数据</strong></p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h013kyka07j31f70i0gnj.jpg"></p>
<pre><code class="hljs"> 1. 客户端向namenode发送查询请求
 2. namenode返回数据块所在的节点datanode
 3. 客户端在返回的节点中寻找相应的块数据
 4. 如果某个datanode挂点了，返回相应的副本中寻找
</code></pre>
<p><strong>HDFS保证数据完整性</strong></p>
<p>客户端向HDFS写数据：</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h013s4e4vqj31f70i0gne.jpg"></p>
<ol>
<li>假设客户端发送2KB的数据 </li>
<li>客户端会以字节的方式往datanode发送，所以客户端会计算发送的数据有多少个，而这个单位就是chunk（512字节）。</li>
<li>客户端可以计算出checksum值，checksum = 2KB/512B=4 </li>
<li>然后datanode接收客户端发送来的数据，每接收512B的数据，就让checksum的值+1 </li>
<li>最后比较客户端和datanade的checksum值</li>
</ol>
<p>DataNode读取Block块：</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h013wr7aaoj31f70i0di4.jpg"></p>
<ol>
<li>block创建时会初始checksum值 </li>
<li>DataNode每隔一段时间就会计算block新的checksum值，看block块是否已经丢失 </li>
<li>如果checksum和之前一样，则没丢失，和之前比出现了不一样，那就说明数据丢失（或者异常） </li>
<li>当发生异常的时候，DateNode会报告给NameNode，NameNode会发送一条命令，清除这个异常块，然后找到这个块对应的副本，将完整的副本复制给其他的DataNode节点</li>
</ol>
<h4 id="HA高可用"><a href="#HA高可用" class="headerlink" title="HA高可用"></a>HA高可用</h4><p>HA即高可用(7*24不间断服务)</p>
<p>实现高可用最关键的策略是消除单点故障，通过配置Active/Standby两个NameNodes实现集群中对NameNode的热备</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h012120smaj30hq0bfmy7.jpg"></p>
<p>Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode,只有主 NameNode 才能对外提供读写服务</p>
<p>ZKFailoverController（主备切换控制器，FC）：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换</p>
<p>Zookeeper 集群：为主备切换控制器提供主备选举支持；</p>
<p>共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。</p>
<p>DataNode 节点：主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。</p>
<h4 id="MapReduce概述"><a href="#MapReduce概述" class="headerlink" title="MapReduce概述"></a>MapReduce概述</h4><p>官方文档的解释是：Hadoop Map/Reduce是一个使用简易的软件框架，基于它写出来的应用程序能够运行在由上千个商用机器组成的大型集群上，并以一种可靠容错的方式并行处理上T级别的数据集。</p>
<p>一个Map/Reduce <em>作业（job）</em> 通常会把输入的数据集切分为若干独立的数据块，由 <em>map任务（task）</em>以完全并行的方式处理它们。框架会对map的输出先进行排序， 然后把结果输入给<em>reduce任务</em>。通常作业的输入和输出都会被存储在文件系统中。 整个框架负责任务的调度和监控，以及重新执行已经失败的任务。</p>
<p>Map/Reduce框架由一个单独的master JobTracker 和每个集群节点一个slave TaskTracker共同组成。master负责调度构成一个作业的所有任务，这些任务分布在不同的slave上，master监控它们的执行，重新执行已经失败的任务。而slave仅负责执行由master指派的任务。</p>
<p><strong>MapReduce核心思想</strong></p>
<ol>
<li>分布式的计算通常要分为至少两个阶段</li>
<li>第一阶段的MapTask并发实例，完全并行运行</li>
<li>第二阶段的ReduceTask并发实例互不相干，但数据依赖于上个阶段所有MapTask并发实例的输出</li>
<li>MapReduce编程模型只能包含一个Map和Reduce阶段</li>
</ol>
<h4 id="MapReduce架构"><a href="#MapReduce架构" class="headerlink" title="MapReduce架构"></a>MapReduce架构</h4><p>MapReduce体系结构主要由四个部分组成，分别是：Client、JobTracker、TaskTracker以及Task</p>
<p>JobTracker是用于调度工作的，TaskTracker是用于执行工作的。<strong>一个Hadoop集群中只有一台JobTracker</strong><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h018835rytj30kc0cadgl.jpg"></p>
<p>客户端向JobTracker提交一个作业，JobTracker把作业拆分为多份分配给TaskTracker执行，TaskTracker每隔一段时间会向JobTracker发送Heartbeat，如果一段时间内JobTracker没有收到来至TaskTracker的Heartbeat，将认为TaskTracker挂掉了，会把该TaskTracker的作业分配给其他TaskTracker</p>
<h4 id="MapReduce执行过程"><a href="#MapReduce执行过程" class="headerlink" title="MapReduce执行过程"></a>MapReduce执行过程</h4><p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h018evc6mwj30jw0f1aav.jpg"></p>
<p>1、客户端启动一个job<br>2、向JobTracker请求一个JobID<br>3、将运行作业所需要的资源文件复制到HDFS上，包括MapReduce程序打包的JAR文件、配置文件和客户端计算所得的输入划分信息。这些文件都存放在JobTracker专门为该作业创建的文件夹中，文件夹名为该作业JobID。JAR文件默认会有10个副本，输入划分信息告诉JobTracker应该为这个作业启动多少个map任务等信息。<br>4、JobTracker接收到作业后将其放在作业队列中，等待JobTracker对其进行调度。当JobTracker根据自己的调度算法调度该作业时，会根据输入划分信息为每个划分创建一个map任务，并将map任务分配给TaskTracker执行。这里需要注意的是，map任务不是随便分配给某个TaskTracker的，Data-Local（数据本地化）将map任务分配给含有该map处理的数据库的TaskTracker上，同时将程序JAR包复制到该TaskTracker上运行，但是分配reducer任务时不考虑数据本地化。<br>5、TaskTracker每隔一段时间给JobTracker发送一个Heartbeat告诉JobTracker它仍然在运行，同时心跳还携带很多比如map任务完成的进度等信息。当JobTracker收到作业的最后一个任务完成信息时，便把作业设置成“成功”，JobClient再传达信息给用户。</p>
<p><strong>输入输出</strong></p>
<p>Map/Reduce框架运转在&lt;key, value&gt; 键值对上， 框架把作业的输入看为是一组&lt;key, value&gt; 键值对，同样也产出一组 &lt;key, value&gt; 键值对做为作业的输出，这两组键值对的类型可能不同。</p>
<blockquote>
<p>(input) &lt;k1, v1&gt; -&gt; <strong>map</strong> -&gt; &lt;k2, v2&gt; -&gt; <strong>combine</strong> -&gt; &lt;k2, v2&gt; -&gt; <strong>reduce</strong> -&gt; &lt;k3, v3&gt; (output)</p>
</blockquote>
<p><strong>MapReduce优缺点</strong></p>
<p>优点：易于编程，良好扩展性，高容错性，适合PB级以上海量数据离线处理</p>
<p>缺点：不擅长实时计算，不擅长流式计算，不擅长DAG计算</p>
<h4 id="Map和Reduce"><a href="#Map和Reduce" class="headerlink" title="Map和Reduce"></a>Map和Reduce</h4><p><strong>(input) -&gt;map-&gt; -&gt;combine-&gt; -&gt;reduce-&gt; (output)</strong></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h018j4wxuyj30kc0b60u1.jpg"></p>
<p><strong>map</strong>：map任务可细分四个阶段：record reader、mapper、combiner和partitioner。map任务的输出被称为中间键和中间值，会被发送到reducer做后续处理。</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h018kcz9j0j30k509q0tc.jpg"></p>
<ol>
<li>读取HDFS中的文件。每一行解析成一个&lt;k,v&gt;。每一个键值对调用一次map函数。&lt;0,helloyou&gt; &lt;10,hello me&gt;</li>
<li>覆盖map()，接收（1）中产生的&lt;k,v&gt;，进行处理，转换为新的&lt;k,v&gt;输出。&lt;hello,1&gt;&lt;you,1&gt;&lt;hello,1&gt;&lt;me,1&gt;</li>
<li>对（2）输出的&lt;k,v&gt;进行<strong>分区</strong>，默认分为一个区。</li>
<li>对不同分区中的数据进行<strong>按照Key排序、分组</strong>。分组指的是相同key的value放到一个集合中。排序后：&lt;hello,1&gt;&lt;hello,1&gt;&lt;me,1&gt; &lt;you,1&gt;，分组后：&lt;hello,{1,1}&gt;&lt;me,{1}&gt;&lt;you,{1}&gt;</li>
<li>对分组后的数据进行<strong>合并归约</strong></li>
</ol>
<p><strong>reduce</strong>：reduce任务也分为四个阶段：混排（shuffle）、排序（sort）、reducer和输出格式（output format）</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h018m68e0sj30k509q0tc.jpg"></p>
<ol>
<li>多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点上。（shuffle）</li>
<li>对多个map的输出进行<strong>合并、排序</strong>。覆盖reduce函数，接收的是分组后的数据，实现自己的业务逻辑，&lt;hello,2&gt;&lt;me,1&gt;&lt;you,1&gt;处理后，产生新的&lt;k,v&gt;输出。</li>
<li>对reduce输出的&lt;k,v&gt;写到HDFS中。</li>
</ol>
<h4 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a><strong>Shuffle机制</strong></h4><p>mapper 阶段处理的数据如何传递给 reducer 阶段，是 MapReduce 框架中 最关键的一个流程，就叫 Shuffle</p>
<p>Shuffle: 数据混洗（核心机制：数据分区，排序，局部聚合，缓存，拉取，再合并 排序）,具体说就是就是将 MapTask 输出的处理结果数据，按照 Partitioner 组件制定的规则分发 给 ReduceTask，并在分发的过程中，对数据按 key 进行了分区和排序</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h018q2xrp5j30p60djq4e.jpg"></p>
<ol>
<li>Collect阶段：MapTask的数据输出到环形缓冲区</li>
<li>Spill阶段：当内存中数据量到一定阈值，会将数据写入本地磁盘(将数据写入磁盘前需要对数据进行一次排序)，如果配置combiner会将相同分区号和key的数据进行排序</li>
<li>MapTask阶段的Merge：把所有溢出的临时文件进行一次merge，确保一个MapTask最终只产生一个中间数据文件</li>
<li>Copy阶段：ReduceTask启动Fetcher线程到已完成MapTask节点上copy一份数据</li>
<li>ReduceTask阶段的Merge：在ReduceTask复制数据同时，会在后台开启两个线程对内存到本地的数据文件进行merge</li>
<li>Sort阶段：对数据进行merge同时，会进行排序，由于MapTask阶段已经对数据进行了局部排序，ReduceTask只需保证Copy数据的最终整体有效性</li>
</ol>
<h4 id="YARN概述"><a href="#YARN概述" class="headerlink" title="YARN概述"></a>YARN概述</h4><p>是一个资源调度平台，负责为运算程序提供服务器运算资源</p>
<h4 id="YARN架构"><a href="#YARN架构" class="headerlink" title="YARN架构"></a>YARN架构</h4><p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h01gppj9o5j30pr0gb405.jpg"></p>
<p><strong>Container</strong>：Container是YARN对资源的抽象，封装了节点上多维度资源，像内存，cpu，磁盘，网络；容器由 NodeManager 启动和管理，并被它所监控；容器被 ResourceManager 进行调度。</p>
<p><strong>NodeManager</strong>：管理单个节点上的资源(负责启动和管理节点上的容器)；处理来自ResourceManager和ApplicationMaster的命令</p>
<p><strong>ResourceManager</strong>：处理客户端请求；监控NodeManager；启动和监控ApplicationMaster；资源的分配与调度；RM有定时调用器(Scheduler)和应用管理器(ApplicationManager)两个主要组件</p>
<p>定时调度器(Scheduler)：当 Client 提交一个任务的时候，它会根据所需要的资源以及当前集群的资源状况进行分配，只负责向应用程序分配资源，并不做监控以及应用程序的状态跟踪</p>
<p>应用管理器(ApplicationManager)：负责管理 Client 用户提交的应用</p>
<p><strong>ApplicationMaster</strong>：与 RM 调度器协商以获取资源（用 Container 表示）； 将得到的任务进一步分配给内部的任务；与 NM 通信以启动 / 停止任务；监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务。</p>
<h4 id="YARN工作机制"><a href="#YARN工作机制" class="headerlink" title="YARN工作机制"></a>YARN工作机制</h4><p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h01hap4w61j31d60u0k2i.jpg"></p>
<h4 id="作业提交流程"><a href="#作业提交流程" class="headerlink" title="作业提交流程"></a>作业提交流程</h4><p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h01h4jackzj30rs0rdn03.jpg"></p>
<ol>
<li>Client向Yarn提交Application，这里我们假设是一个MapReduce作业。</li>
<li>ResourceManager向NodeManager通信，为该Application分配第一个容器。并在这个容器中运行这个应用程序对应的ApplicationMaster。</li>
<li>ApplicationMaster启动以后，对作业（也就是Application）进行拆分，拆分task出来，这些task可以运行在一个或多个容器中。然后向ResourceManager申请要运行程序的容器，并定时向ResourceManager发送心跳。</li>
<li>申请到容器后，ApplicationMaster会去和容器对应的NodeManager通信，而后将作业分发到对应的NodeManager中的容器去运行，这里会将拆分后的MapReduce进行分发，对应容器中运行的可能是Map任务，也可能是Reduce任务。</li>
<li>容器中运行的任务会向ApplicationMaster发送心跳，汇报自身情况。当程序运行完成后，ApplicationMaster再向ResourceManager注销并释放容器资源。</li>
</ol>
<h4 id="YARN调度器"><a href="#YARN调度器" class="headerlink" title="YARN调度器"></a>YARN调度器</h4><p>FIFO 、Capacity Scheduler（容量调度器）和Fair Sceduler（公平调度器）</p>
<p>FIFO调度器：支持单队列 、先进先出  生产环境不会用。</p>
<p>容量调度器：支持多队列，保证先进入的任务优先执行。</p>
<p>公平调度器：支持多队列，保证每个任务公平享有队列资源。</p>
]]></content>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据项目</title>
    <url>/2022/03/12/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A1%B9%E7%9B%AE/</url>
    <content><![CDATA[<blockquote>
<p>大数据新闻热点项目</p>
</blockquote>
<span id="more"></span>

<h3 id="项目需求设计与分析"><a href="#项目需求设计与分析" class="headerlink" title="项目需求设计与分析"></a>项目需求设计与分析</h3><p><strong>目标</strong></p>
<p>1、完成大数据项目的架构设计，安装部署，架构继承与开发、用户可视化交互设计</p>
<p>2、完成实时在线数据分析</p>
<p>3、完成离线数据分析</p>
<p><strong>具体功能</strong></p>
<p>1、捕获用户浏览日志信息</p>
<p>2、实时分析前20名流量最高的新闻话题</p>
<p>3、实时统计当前线上已曝光的新闻话题</p>
<p>4、统计哪个时段用户浏览量最高</p>
<p>5、报表</p>
<p><strong>项目技术栈</strong></p>
<p>Hadoop2.x、Zookeeper、Flume、Hive、Hbase、Kafka、Spark2.x、SparkStreaming、MySQL、Hue、J2EE、websoket、Echarts</p>
<p><strong>项目架构</strong></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h06thvxvapj30op0ee40r.jpg"></p>
<p><strong>集群资源规划</strong></p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h06tiyis0nj30m20ck75k.jpg"></p>
<h3 id="环境准备与设置"><a href="#环境准备与设置" class="headerlink" title="环境准备与设置"></a>环境准备与设置</h3><p><strong>Linux重要配置</strong></p>
<p><strong>1）设置ip地址</strong> 项目视频里面直接使用界面修改ip比较方便，如果Linux没有安装操作界面，需要使用命令：vi /etc/sysconfig/network-scripts/ifcfg-eth0 来修改ip地址，然后重启网络服务service network restart即可。 参考链接：<a href="https://www.willxu.xyz/2018/08/23/hadoop/1%E3%80%81vmware%E4%B8%8A%E7%BD%91%E9%85%8D%E7%BD%AE/">请点击。</a></p>
<p><strong>2）创建用户</strong> 大数据项目开发中，一般不直接使用root用户，需要我们创建新的用户来操作，比如kfk。 a）创建用户命令：adduser kfk b）设置用户密码命令：passwd kfk</p>
<p><strong>3）文件中设置主机名</strong> Linux系统的主机名默认是localhost，显然不方便后面集群的操作，我们需要手动修改Linux系统的主机名。 a）查看主机名命令：hostname b）修改主机名称 vi /etc/sysconfig/network NETWORKING=yes HOSTNAME=bigdata-pro01.kfk.com</p>
<p><strong>4）主机名映射</strong> 如果想通过主机名访问Linux系统，还需要配置主机名跟ip地址之间的映射关系。 vi /etc/hosts 192.168.31.151 bigdata-pro01.kfk.com 配置完成之后，reboot重启Linux系统即可。 如果需要在windows也能通过hostname访问Linux系统，也需要在windows下的hosts文件中配置主机名称与ip之间的映射关系。在windows系统下找到C:\WINDOWS\system32\drivers\etc\路径，打开HOSTS文件添加如下内容： 192.168.31.151 bigdata-pro01.kfk.com</p>
<p><strong>5）root用户下设置无密码用户切换</strong> 在Linux系统中操作是，kfk用户经常需要操作root用户权限下的文件，但是访问权限受限或者需要输入密码。修改/etc/sudoers这个文件添加如下代码，即可实现无密码用户切换操作。 vi /etc/sudoers 。。。添加如下内容即可 kfk ALL=(root)NOPASSWD:ALL</p>
<p><strong>6）关闭防火墙</strong> 我们都知道防火墙对我们的服务器是进行一种保护，但是有时候防火墙也会给我们带来很大的麻烦。 比如它会妨碍hadoop集群间的相互通信，所以我们需要关闭防火墙。 那么我们永久关闭防火墙的方法如下: vi /etc/sysconfig/selinux SELINUX=disabled 保存、重启后，验证机器的防火墙是否已经关闭。 a）查看防火墙状态：service iptables status b）打开防火墙：service iptables start c）关闭防火墙：service iptables stop</p>
<p><strong>7）卸载Linux本身自带的jdk</strong> 一般情况下jdk需要我们手动安装兼容的版本，此时Linux自带的jdk需要手动删除掉，具体操作如下所示： a）查看Linux自带的jdk rpm -qa|grep java b）删除Linux自带的jdk rpm -e –nodeps [jdk进程名称1 jdk进程名称2 …]</p>
<p><strong>克隆虚拟机并进行相关的配置</strong></p>
<p>前面我们已经做好了Linux的系统常规设置，接下来需要克隆虚拟机并进行相关的配置。 <strong>1）kfk用户下创建我们将要使用的各个目录</strong></p>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk">软件目录<br>mkdir <span class="hljs-regexp">/opt/</span>softwares<br>模块目录<br>mkdir <span class="hljs-regexp">/opt/m</span>odules<br>工具目录<br>mkdir <span class="hljs-regexp">/opt/</span>tools<br>数据目录<br>mkdir <span class="hljs-regexp">/opt/</span>datas<br></code></pre></td></tr></table></figure>

<p><strong>2）jdk安装(1.7以上，1.9以下)</strong> 大数据平台运行环境依赖JVM，所以我们需要提前安装和配置好jdk。 前面我们已经安装了64位的centos系统，所以我们的jdk也需要安装64位的，与之相匹配 下面步骤给的是1.7的。我自己用的是jdk1.8.0_191 a）将jdk安装包通过工具上传到/opt/softwares目录下 b）解压jdk安装包</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-comment">#解压命令</span><br><span class="hljs-attribute">tar</span> -zxf jdk-<span class="hljs-number">7</span>u67-linux-x64.tar.gz /opt/modules/<br><span class="hljs-comment">#查看解压结果</span><br><span class="hljs-attribute">ls</span><br><span class="hljs-attribute">jdk1</span>.<span class="hljs-number">7</span>.<span class="hljs-number">0</span>_67<br></code></pre></td></tr></table></figure>

<p>c）配置Java 环境变量</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><code class="hljs routeros">vi /etc<span class="hljs-built_in">/profile</span><br><span class="hljs-built_in"></span><span class="hljs-built_in">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/opt/modules/jdk1.7.0_67<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-variable">$PATH</span>:$JAVA_HOME/bin<br></code></pre></td></tr></table></figure>

<p>d）查看Java是否安装成功</p>
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">java </span>-version<br><span class="hljs-keyword">java </span>version <span class="hljs-string">&quot;1.7.0_67&quot;</span><br><span class="hljs-keyword">Java(TM) </span>SE Runtime Environment (<span class="hljs-keyword">build </span><span class="hljs-number">1</span>.<span class="hljs-number">7</span>.<span class="hljs-number">0</span>_67-<span class="hljs-keyword">b15)</span><br><span class="hljs-keyword"></span><span class="hljs-keyword">Java </span>HotSpot(TM) <span class="hljs-number">64</span>-<span class="hljs-keyword">Bit </span>Server VM (<span class="hljs-keyword">build </span><span class="hljs-number">24</span>.<span class="hljs-number">79</span>-<span class="hljs-keyword">b02, </span>mixed mode)<br></code></pre></td></tr></table></figure>

<p><strong>3）克隆虚拟机</strong></p>
<p>在克隆虚拟机之前，需要关闭虚拟机，然后右键选中虚拟机——》选择管理——》选择克隆——》选择下一步——》选择下一步——》选择创建完整克隆，下一步——》选择克隆虚拟机位置（提前创建好），修改虚拟机名称为Hadoop-Linux-pro-2，然后选择完成即可。 然后使用同样的方式创建第三个虚拟机Hadoop-Linux-pro-3。</p>
<p><strong>4）修改克隆虚拟机配置</strong> 克隆完虚拟机Hadoop-Linux-pro-2和Hadoop-Linux-pro-3之后，可以按照Hadoop-Linux-pro-1的方式配置好ip地址、hostname，以及ip地址与hostname之间的关系</p>
<h3 id="Hadoop2-X分布式集群部署"><a href="#Hadoop2-X分布式集群部署" class="headerlink" title="Hadoop2.X分布式集群部署"></a>Hadoop2.X分布式集群部署</h3><p><strong>配置要点</strong></p>
<p><strong>1）hadoop2.x版本下载及安装</strong> 官网下载2.x版本就好</p>
<p><strong>2）hadoop配置要点</strong> 参考官网给的例子：<a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html</a> 网站左下角有全部配置信息 <strong>1）hadoop2.x分布式集群配置-HDFS</strong><br>安装hdfs需要修改4个配置文件：hadoop-env.sh、core-site.xml、hdfs-site.xml和slaves <strong>2）hadoop2.x分布式集群配置-YARN</strong> 安装yarn需要修改4个配置文件：yarn-env.sh、mapred-env.sh、yarn-site.xml和mapred-site.xml</p>
<p><strong>3）分发配置到节点</strong> 最好先SCP设置成无密码访问，需要生成秘钥，自己百度吧 hadoop相关配置在第一个节点配置好之后，可以通过脚本命令分发给另外两个节点即可，具体操作如下所示。 将安装包分发给第二个节点 scp -r hadoop-2.5.0 <a href="mailto:kaf@bigdata-pro02.kfk.com">kaf@bigdata-pro02.kfk.com</a>:/opt/modules/ 将安装包分发给第三个节点 scp -r hadoop-2.5.0 <a href="mailto:kaf@bigdata-pro02.kfk.com">kaf@bigdata-pro02.kfk.com</a>:/opt/modules/</p>
<p><strong>4）HDFS启动集群运行测试</strong> hdfs相关配置好之后，可以启动hdfs集群。 1.格式化NameNode 通过命令：bin/hdfs namenode -format 格式化NameNode。 2.启动各个节点机器服务 1）启动NameNode命令：sbin/hadoop-daemon.sh start namenode 2) 启动DataNode命令：sbin/hadoop-daemon.sh start datanode 3）启动ResourceManager命令：sbin/yarn-daemon.sh start resourcemanager 4）启动NodeManager命令：sbin/yarn-daemon.sh start resourcemanager 5）启动log日志命令：sbin/mr-jobhistory-daemon.sh start historyserver</p>
<p><strong>5）YARN集群运行MapReduce程序测试</strong> 前面hdfs和yarn都启动起来之后，可以通过运行WordCount程序检测一下集群是否能run起来。 集群自带的WordCount程序执行命令：bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0.jar wordcount input output</p>
<p><strong>6）ssh无秘钥登录</strong> （可以提前设置好） 在集群搭建的过程中，需要不同节点分发文件，那么节点间分发文件每次都需要输入密码，比较麻烦。另外在hadoop 集群启动过程中，也需要使用批量脚本统一启动各个节点服务，此时也需要节点之间实现无秘钥登录。具体操作步骤如下所示： 1.主节点上创建 .ssh 目录，然后生成公钥文件id_rsa.pub和私钥文件id_rsa mkdir .ssh ssh-keygen -t rsa 2.拷贝公钥到各个机器 ssh-copy-id bigdata-pro1.kfk.com ssh-copy-id bigdata-pro2.kfk.com ssh-copy-id bigdata-pro3.kfk.com 3.测试ssh连接 ssh bigdata-pro1.kfk.com ssh bigdata-pro2.kfk.com ssh bigdata-pro3.kfk.com 4.测试hdfs ssh无秘钥登录做好之后，可以在主节点通过一键启动命令，启动hdfs各个节点的服务，具体操作如下所示： sbin/start-dfs.sh 如果yarn和hdfs主节点共用，配置一个节点即可。否则，yarn也需要单独配置ssh无秘钥登录。</p>
<p><strong>7）配置集群内机器时间同步（使用Linux ntp进行）</strong> 选择一台机器作为时间服务器，比如bigdata-pro1.kfk.com节点。 1.查看ntp服务是否已经存在 sudo rpm -qa|grep ntp 2.ntp服务相关操作 1）查看ntp状态 sudo service ntpd status 2）启动ntp sudo service ntpd start 3）关闭ntp sudo service ntpd stop 3.设置ntp随机器启动 sudo chkconfig ntpd on 4.修改ntp配置文件 vi /etc/ntp.conf 释放注释并将ip地址修改为 restrict 192.168.31.151 mask 255.255.255.0 nomodify notrap 注释掉以下命令行 server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst 释放以下命令行 server 127.127.1.0 #local clock fudge 127.127.1.0 stratum 10 重启ntp服务 sudo service ntpd restart 5.修改服务器时间</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-comment">#设置当前日期</span><br><span class="hljs-attribute">sudo</span> date -s <span class="hljs-number">2017</span>-<span class="hljs-number">06</span>-<span class="hljs-number">16</span><br><span class="hljs-comment">#设置当前时间</span><br><span class="hljs-attribute">sudo</span> date -s <span class="hljs-number">22</span>:<span class="hljs-number">06</span>:<span class="hljs-number">00</span><br></code></pre></td></tr></table></figure>

<p>6.其他节点手动同步主服务器时间</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk"><span class="hljs-comment">#查看ntp位置</span><br>which ntpdate<br><span class="hljs-regexp">/usr/</span>sbin/ntpdate<br><span class="hljs-number">1</span>）手动同步bigdata-pro2.kfk.com节点时间<br>sudo <span class="hljs-regexp">/usr/</span>sbin/ntpdate bigdata-pro2.kfk.com<br><span class="hljs-number">2</span>）手动同步bigdata-pro3.kfk.com节点时间<br>sudo <span class="hljs-regexp">/usr/</span>sbin/ntpdate bigdata-pro3.kfk.com<br><span class="hljs-number">7</span>.其他节点定时同步主服务器时间<br>bigdata-pro2.kfk.com和bigdata-pro3.kfk.com节点分别切换到root用户， 通过crontab -e 命令，每<span class="hljs-number">10</span>分钟同步一次主服务器节点的时间。<br>crontab -e<br><span class="hljs-comment">#定时，每隔10分钟同步bigdata-pro1.kfk.com服务器时间</span><br><span class="hljs-number">0</span>-<span class="hljs-number">59</span><span class="hljs-regexp">/10 * * * *  /u</span>sr<span class="hljs-regexp">/sbin/</span>ntpdate bigdata-pro1.kfk.com<br></code></pre></td></tr></table></figure>

<h3 id="Zookeeper分布式集群部署"><a href="#Zookeeper分布式集群部署" class="headerlink" title="Zookeeper分布式集群部署"></a>Zookeeper分布式集群部署</h3><p><strong>Zookeeper部署步骤</strong></p>
<p><strong>1）下载Zookeeper</strong> 这里选择cdh版本的zookeeper-3.4.5-cdh5.10.0.tar.gz，将下载好的安装包上传至bigdata-pro01.kfk.com节点的/opt/softwares目录下。</p>
<p><strong>2）解压Zookeeper</strong> tar -zxf zookeeper-3.4.5-cdh5.10.0.tar.gz -C /opt/modules/ </p>
<p><strong>3）修改配置</strong> </p>
<ol>
<li>复制配置文件 cp conf/zoo_sample.cfg zoo.cfg</li>
<li>修改配置文件zoo.cfg</li>
</ol>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">vi</span> zoo.cfg<br><span class="hljs-comment">#这个时间是作为Zookeeper服务器之间或客户端与服务器之间维持心跳的时间间隔</span><br><span class="hljs-attribute">tickTime</span>=<span class="hljs-number">2000</span><br><span class="hljs-comment">#配置 Zookeeper 接受客户端初始化连接时最长能忍受多少个心跳时间间隔数。</span><br><span class="hljs-attribute">initLimit</span>=<span class="hljs-number">10</span><br><span class="hljs-comment">#Leader 与 Follower 之间发送消息，请求和应答时间长度</span><br><span class="hljs-attribute">syncLimit</span>=<span class="hljs-number">5</span><br><span class="hljs-comment">#数据目录需要提前创建</span><br><span class="hljs-attribute">dataDir</span>=/opt/modules/zookeeper-<span class="hljs-number">3</span>.<span class="hljs-number">4</span>.<span class="hljs-number">5</span>-cdh5.<span class="hljs-number">10</span>.<span class="hljs-number">0</span>/zkData<br><span class="hljs-comment">#访问端口号</span><br><span class="hljs-attribute">clientPort</span>=<span class="hljs-number">2181</span><br><span class="hljs-comment">#server.每个节点服务编号=服务器ip地址：集群通信端口：选举端口</span><br><span class="hljs-attribute">server</span>.<span class="hljs-number">1</span>=bigdata-pro01.kfk.com:<span class="hljs-number">2888</span>:<span class="hljs-number">3888</span><br><span class="hljs-attribute">server</span>.<span class="hljs-number">2</span>=bigdata-pro02.kfk.com:<span class="hljs-number">2888</span>:<span class="hljs-number">3888</span><br><span class="hljs-attribute">server</span>.<span class="hljs-number">3</span>=bigdata-pro03.kfk.com:<span class="hljs-number">2888</span>:<span class="hljs-number">3888</span><br></code></pre></td></tr></table></figure>

<p><strong>4）分发各个节点</strong> 将Zookeeper安装配置分发到其他两个节点，具体操作如下所示： scp -r zookeeper-3.4.5-cdh5.10.0/ bigdata-pro02.kfk.com:/opt/modules/ scp -r zookeeper-3.4.5-cdh5.10.0/ bigdata-pro03.kfk.com:/opt/modules/ </p>
<p><strong>5）创建相关目录和文件</strong> </p>
<ol>
<li>在3个节点上分别创建数据目录 mkdir /opt/modules/zookeeper-3.4.5-cdh5.10.0/zkData </li>
<li>在各个节点的数据存储目录下创建myid文件，并且编辑每个机器的myid内容为</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#切换到数据目录</span><br><span class="hljs-built_in">cd</span> /opt/modules/zookeeper-3.4.5-cdh5.10.0/zkData<br><span class="hljs-comment">#bigdata-pro01.kfk.com节点</span><br><span class="hljs-built_in">touch</span> myid<br>vi myid<br>1<br><span class="hljs-comment">#bigdata-pro02.kfk.com节点</span><br><span class="hljs-built_in">touch</span> myid<br>vi myid<br>2<br><span class="hljs-comment">#bigdata-pro03.kfk.com节点</span><br><span class="hljs-built_in">touch</span> myid<br>vi myid<br>3<br></code></pre></td></tr></table></figure>

<p><strong>6）启动Zookeeper服务</strong> </p>
<ol>
<li>各个节点使用如下命令启动Zookeeper服务 bin/zkServer.sh start </li>
<li>查看各个节点服务状态 bin/zkServer.sh status 不是follower </li>
<li>关闭各个节点服务 bin/zkServer.sh stop </li>
<li>查看Zookeeper目录树结构 bin/zkCli.sh</li>
</ol>
<h3 id="Hadoop高可用配置-HA"><a href="#Hadoop高可用配置-HA" class="headerlink" title="Hadoop高可用配置(HA)"></a>Hadoop高可用配置(HA)</h3><p><strong>HA原理</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h06tt0ow1oj30dp0bo3zb.jpg"></p>
<p>当发生故障时，Active的 NN 挂掉后，Standby NN 会在它成为Active NN 前，读取所有的JN里面的修改日志，这样就能高可靠的保证与挂掉的NN的目录镜像树一致，然后无缝的接替它的职责，维护来自客户端请求，从而达到一个高可用的目的。</p>
<p><strong>HDFS-HA配置</strong></p>
<p>1）修改hdfs-site.xml配置文件 </p>
<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.nameservices<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>ns<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.namenodes.ns<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>nn1,nn2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.rpc-address.ns.nn1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com:8020<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.rpc-address.ns.nn2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro02.kfk.com:8020<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.http-address.ns.nn1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com:50070<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>       <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.http-address.ns.nn2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>       <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro02.kfk.com:50070<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>qjournal://bigdata-pro01.kfk.com:8485;bigdata-pro02.kfk.com:8485;bigdata-pro03.kfk.com:8485/ns<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>       <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>       <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/opt/modules/hadoop-2.6.0/data/jn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled.ns<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>sshfence<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/home/kfk/.ssh/id_rsa<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>3<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.permissions.enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>false<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure>

<p>2）修改core-site.xml配置文件</p>
<figure class="highlight dust"><table><tr><td class="code"><pre><code class="hljs dust"><span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://ns<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>kfk<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>	</span><br><span class="language-xml">	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/opt/modules/hadoop-2.6.0/data/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.name.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file://$</span><span class="hljs-template-variable">&#123;hadoop.tmp.dir&#125;</span><span class="language-xml">/dfs/name<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>ha.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com:2181,bigdata-pro02.kfk.com:2181,bigdata-pro03.kfk.com:2181<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span></span><br></code></pre></td></tr></table></figure>

<p>3）将修改的配置分发到其他节点</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk">scp hdfs-site.xml bigdata-pro02.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc<span class="hljs-regexp">/hadoop/</span><br>scp hdfs-site.xml bigdata-pro03.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc<span class="hljs-regexp">/hadoop/</span><br>scp core-site.xml bigdata-pro02.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc<span class="hljs-regexp">/hadoop/</span><br>scp core-site.xml bigdata-pro03.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc<span class="hljs-regexp">/hadoop/</span><br></code></pre></td></tr></table></figure>

<p><strong>HDFS-HA自动故障转移测试</strong></p>
<p>1）在所有节点启动zookeeper cd /opt/modules/zookeeper-3.4.5-cdh5.10.0/ sbin/zkServer.sh start bin/hdfs zkfc -formatZK （第一次使用zkfc需要格式化） </p>
<p>2）启动hdfs bin/hdfs namenode -format （第一次使用hdfs需要格式化，在namenode） sbin/start-dfs.sh （会在各个节点上启动namenode/datanode/journalnode） </p>
<p>3）在HA的namenode节点上启动zkfc线程（两个namenode都要启动） sbin/hadoop-daemon.sh start zkfc 查看两个namenode状态一个是active(先启动zkfc的)，一个是standy，查看网页。 <a href="http://bigdata-pro01.kfk.com:50070/">http://bigdata-pro01.kfk.com:50070</a> <a href="http://bigdata-pro02.kfk.com:50070/">http://bigdata-pro02.kfk.com:50070</a> </p>
<p>4）上传文件到hdfs bin/hdfs dfs -mkdir /usr bin/hdfs dfs -put /opt/modules/hadoop-2.6.0/etc/hadoop/hdfs-site.xml /usr 在网页中可以看到 </p>
<p>5）杀死active的namenode </p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h06twirurij30au032t91.jpg"></p>
<p>6）再次查看namenode状态 应该完成了主备切换。原来的standy变成了active.</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h06txd90zdj30py096dgy.jpg"></p>
<h3 id="HDFS-HA所遇到的问题（看输出日志和查看日志）"><a href="#HDFS-HA所遇到的问题（看输出日志和查看日志）" class="headerlink" title="HDFS-HA所遇到的问题（看输出日志和查看日志）"></a>HDFS-HA所遇到的问题（看输出日志和查看日志）</h3><p><strong>1）输出提示：无法解析bigdata-pro03.kfk.com:2181</strong> 原因：因为我的core-site.xml配置文件写错了,参数一栏不能有换行，要不然读的不对的。</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-section">&lt;property&gt;</span><br>	<span class="hljs-section">&lt;name&gt;</span><span class="hljs-attribute">ha</span>.zookeeper.quorum&lt;/name&gt;<br>	<span class="hljs-section">&lt;value&gt;</span><span class="hljs-attribute">bigdata</span>-pro01.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro02.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro03.kfk.com:<span class="hljs-number">2181</span>&lt;/value&gt;<br><span class="hljs-section">&lt;/property&gt;</span><br></code></pre></td></tr></table></figure>

<p><strong>2） sbin/start-dfs.sh 启动不成功</strong> 因为这个启动需要配置ssh，所以 （1）在节点1上 ssh-keygen ssh-copy-id bigdata-pro1.kfk.com (包括自己的也要ssh) ssh-copy-id bigdata-pro2.kfk.com ssh-copy-id bigdata-pro3.kfk.com （2）测试ssh连接 ssh bigdata-pro1.kfk.com ssh bigdata-pro2.kfk.com ssh bigdata-pro3.kfk.com </p>
<p><strong>3） namenode准备切换失败</strong> bigdata-pro1.kfk.com可以竞选成active，但是杀掉bigdata-pro1.kfk.com，而bigdata-pro2.kfk.com不会竞选成active，仍然是standby。 查看bigdata-pro2.kfk.com日志： tail -10f hadoop-kfk-zkfc-bigdata-pro02.kfk.com.log </p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h06txpr4o5j30q10ftqds.jpg"></p>
<p><strong>红线部分说明，在bigdata-pro2.kfk.com准备选举时，需要对pro1进行fence，但是失败了，原因是ssh失败，说明在节点2上没法ssh到节点1上，所以需要在节点2上进行ssh-keygen,然后拷贝到节点1，这样就解决了</strong></p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h06tyjius2j30qd07043b.jpg"></p>
<p><strong>YARN-HA原理</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h06tzghav5j30hp0azt99.jpg"></p>
<p>ResourceManager HA 由一对Active，Standby结点构成，通过RMStateStore存储内部数据和主要应用的数据及标记。 目前支持的可替代的RMStateStore实现有：基于内存的MemoryRMStateStore，基于文件系统的FileSystemRMStateStore，及基于zookeeper的ZKRMStateStore。 ResourceManager HA的架构模式同NameNode HA的架构模式基本一致，数据共享由RMStateStore，而ZKFC成为 ResourceManager进程的一个服务，非独立存在。</p>
<p><strong>YARN-HA配置</strong></p>
<p><strong>1）修改yarn-site.xml配置文件</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>10000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>rs<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>rm1,rm2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro02.kfk.com<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com:2181,bigdata-pro02.kfk.com:2181,bigdata-pro03.kfk.com:2181<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>	<br><br>	<br><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure>

<p><strong>2）分发至其他节点</strong> </p>
<p>​    <code>scp yarn-site.xml bigdata-pro02.kfk.com:/opt/modules/hadoop-2.6.0/etc/hadoop/</code></p>
<p><code> scp yarn-site.xml bigdata-pro03.kfk.com:/opt/modules/hadoop-2.6.0/etc/hadoop/</code></p>
<p><strong>YARN-HA故障转移测试</strong></p>
<ol>
<li>在rm1节点上启动yarn服务 sbin/start-yarn.sh </li>
<li>在rm2节点上启动ResourceManager服务 sbin/yarn-daemon.sh start resourcemanager </li>
<li>查看yarn的web界面 <a href="http://bigdata-pro01.kfk.com:8088/">http://bigdata-pro01.kfk.com:8088</a> <a href="http://bigdata-pro02.kfk.com:8088/">http://bigdata-pro02.kfk.com:8088</a> </li>
<li>上传wordcount所需的文件到hdfs并执行MapReduce例子 bin/hdfs dfs -put data/wc /usr/kfk/data<br>bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /usr/kfk/data/wc /usr/kfk/data/wc.out </li>
<li>执行到一半的时候，kill掉rm1上的resourcemanager 任务会转移到rm2继续处理 这是bigdata-pro01.kfk.com输出的日志（额外打开一个bigdata-pro01.kfk.com进行kill）</li>
</ol>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h06u1030pij30mb01xgmb.jpg"></p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h06u1grjbnj31cb0b2jut.jpg"></p>
<h3 id="HBase分布式部署"><a href="#HBase分布式部署" class="headerlink" title="HBase分布式部署"></a>HBase分布式部署</h3><p><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fynxzha2acj30hr053weh.jpg"><br>1、解压安装到/opt/modules/<br>2、修改配置文件<br><strong>a.hbase-env.sh</strong><br>配置jdk<br>export JAVA_HOME=/opt/modules/jdk1.8.0_191<br>使用外部的Zookeeper<br>export HBASE_MANAGES_ZK=false<br><strong>b.hbase-site.xml</strong><br>这里采用hadoop高可用下的配置</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.rootdir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://ns/hbase<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.cluster.distributed<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com,bigdata-pro02.kfk.com,bigdata-pro03.kfk.com<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure>
<p><strong>c.regionservers</strong><br>bigdata-pro01.kfk.com<br>bigdata-pro02.kfk.com<br>bigdata-pro03.kfk.com</p>
<p><strong>3、将hadoop中hdfs-site.xml和core-site.xml拷贝到hbase的conf下</strong><br>要不然会启动失败，具体日志如下：不认识ns，因为ns在hadoop中配置的<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fyny4rdsrfj30pu0cjdh8.jpg"><br>4、将hbase配置分发到各个节点<br>scp -r hbase-1.0.0-cdh5.4.0 bigdata-pro02.kfk.com:/opt/modules/<br>scp -r hbase-1.0.0-cdh5.4.0 bigdata-pro03.kfk.com:/opt/modules/</p>
<p><strong>HBase启动与测试</strong></p>
<ol>
<li>先启动zookeeper<pre><code class="hljs">zkServer.sh start
</code></pre>
</li>
<li>启动高可用下的hdfs<pre><code class="hljs">sbin/start-dfs.sh （会在各个节点上启动namenode/datanode/journalnode）
</code></pre>
在HA的namenode节点上启动zkfc线程（两个namenode都要启动）<br>sbin/hadoop-daemon.sh start zkfc</li>
<li>启动hbase<br>bin/start-hbase.sh</li>
<li>查看HBase Web界面<br>bigdata-pro01.kfk.com:60010/</li>
<li>HBase的master高可用测试</li>
</ol>
<figure class="highlight crmsh"><table><tr><td class="code"><pre><code class="hljs crmsh">在bigdata-pro02.kfk.com上启动<span class="hljs-literal">master</span>,<br>./hbase-daemon.sh <span class="hljs-literal">start</span> <span class="hljs-keyword">master</span><br><span class="hljs-title">然后杀死bigdata-pro01</span>.kfk.com的Hmaster<br>zookeeper会自动切换<span class="hljs-literal">master</span><br></code></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fynycepyczj30xp0cl3zb.jpg"></p>
<p><strong>HBase的shell测试</strong></p>
<p>1、启动shell<br>bin/hbase shell<br>2、创建表<br>create ‘weblogs’,’info’<br>3、列出表<br>list<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fyososol82j306q02o0sk.jpg"></p>
<h3 id="Kafka分布式部署"><a href="#Kafka分布式部署" class="headerlink" title="Kafka分布式部署"></a>Kafka分布式部署</h3><p>1）解压<br>tar -zxf kafka_2.10-0.9.0.0.tgz  -C /opt/modules/<br>2）配置server.properties文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs propertis">#节点唯一标识<br>broker.id=1<br><br>listeners=PLAINTEXT://bigdata-pro01.kfk.com:9092<br>#默认端口号<br>port=9092<br>#主机名绑定<br>host.name=bigdata-pro01.kfk.com<br>#Kafka数据目录<br>log.dirs=/opt/modules/kafka_2.10-0.9.0.0/kafka-logs<br>#配置Zookeeper<br>zookeeper.connect=bigdata-pro01.kfk.com:2181,bigdata-pro02.kfk.com:2181,bigdata-pro03.kfk.com:2181<br></code></pre></td></tr></table></figure>
<p>3）配置zookeeper.properties文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs propertis">#Zookeeper的数据存储路径与Zookeeper集群配置保持一致<br>dataDir=/opt/modules/zookeeper-3.4.5-cdh5.10.0/zkData<br></code></pre></td></tr></table></figure>

<p>4）配置consumer.properties文件</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">#配置Zookeeper地址<br>zookeeper.connect=bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro03<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span><br></code></pre></td></tr></table></figure>
<p>5）配置producer.properties文件</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">#配置Kafka集群地址  ,分布在三台机器上<br>metadata<span class="hljs-selector-class">.broker</span>.list=bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">9092</span>,bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">9092</span>,bigdata-pro03<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">9092</span><br></code></pre></td></tr></table></figure>
<p>6）拷贝<br>scp -r kafka_2.10-0.9.0.0 bigdata-pro02.kfk.com:/opt/modules/<br>scp -r kafka_2.10-0.9.0.0 bigdata-pro03.kfk.com:/opt/modules/<br>7）修改另外两个节点的server.properties</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-comment">#bigdata-pro02.kfk.com节点</span><br><span class="hljs-attr">broker.id</span>=<span class="hljs-number">2</span><br><span class="hljs-attr">listeners</span>=PLAINTEXT://bigdata-pro02.kfk.com:<span class="hljs-number">9092</span><br><span class="hljs-attr">host.name</span>=bigdata-pro02.kfk.com<br><span class="hljs-comment">#bigdata-pro03.kfk.com节点</span><br><span class="hljs-attr">broker.id</span>=<span class="hljs-number">3</span><br><span class="hljs-attr">listeners</span>=PLAINTEXT://bigdata-pro03.kfk.com:<span class="hljs-number">9092</span><br><span class="hljs-attr">host.name</span>=bigdata-pro03.kfk.com<br></code></pre></td></tr></table></figure>

<p><strong>kafka测试</strong></p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus"><span class="hljs-number">1</span>、所有节点启动zk<br>bin/zkServer<span class="hljs-selector-class">.sh</span> start<br><span class="hljs-number">2</span>、各个节点启动Kafka集群<br>bin/kafka-server-start<span class="hljs-selector-class">.sh</span> config/server<span class="hljs-selector-class">.properties</span> &amp;<br><span class="hljs-number">3</span>、创建topic<br>bin/kafka-topics<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--zookeeper</span> localhost:<span class="hljs-number">2181</span> <span class="hljs-attr">--create</span> <span class="hljs-attr">--topic</span> test <span class="hljs-attr">--replication-factor</span> <span class="hljs-number">1</span> <span class="hljs-attr">--partitions</span> <span class="hljs-number">1</span><br><span class="hljs-number">4</span>、查看topic<br>bin/kafka-topics<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--zookeeper</span> localhost:<span class="hljs-number">2181</span> –list<br><br>bin/kafka-topics<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--describe</span> <span class="hljs-attr">--zookeeper</span> localhost:<span class="hljs-number">2181</span> <span class="hljs-attr">--topic</span> test<br>结果：<br>        Topic:test      PartitionCount:<span class="hljs-number">1</span>        ReplicationFactor:<span class="hljs-number">1</span>     Configs:<br>        Topic: test     Partition: <span class="hljs-number">0</span>    Leader: <span class="hljs-number">2</span>       Replicas: <span class="hljs-number">2</span>     Isr: <span class="hljs-number">2</span><br><span class="hljs-number">5</span>、生产者生产数据（节点<span class="hljs-number">1</span>）<br>bin/kafka-console-producer<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--broker-list</span> bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">9092</span> <span class="hljs-attr">--topic</span> test<br><span class="hljs-number">6</span>、消费者消费数据（节点<span class="hljs-number">2</span>）<br>bin/kafka-console-consumer<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--zookeeper</span> bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span> <span class="hljs-attr">--topic</span> test <span class="hljs-attr">--from-beginning</span><br></code></pre></td></tr></table></figure>
<p>说下分区和消费关系：<br>一个主题可以有多个分区，具体分区方法有多种；关于消费，有消费组的概念。一种是指定消费组（每个消费者的组名一致），那么每个分区对应一个消费者；二是指定消费组（每个消费者的组名不一致），那么所有分区每个消息都会送至各个小组的消费者；三是不指定消费组，那么每条消息会发给消费组中一个消费者。</p>
<h3 id="Flume搭建部署"><a href="#Flume搭建部署" class="headerlink" title="Flume搭建部署"></a>Flume搭建部署</h3><p><strong>（先部署和设置了节点2和3采集部分，节点1的汇总分发后面继续）</strong><br>每一步都可以去查官方资料：官方地址：<a href="http://flume.apache.org/">http://flume.apache.org/</a></p>
<p>1、解压Flume<br>tar -zxf apache-flume-1.7.0-bin.tar.gz  -C /opt/modules/</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><code class="hljs routeros">vi flume-env.sh<br>配置下环境变量问题<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/opt/modules/jdk1.8.0_191<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">HADOOP_HOME</span>=/opt/modules/hadoop-2.6.0<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">HBASE_HOME</span>=/opt/modules/hbase-1.0.0-cdh5.4.0<br></code></pre></td></tr></table></figure>
<p>2、将flume分发到其他两个节点<br>scp -r flume-1.7.0-bin bigdata-pro02.kfk.com:/opt/modules/<br>scp -r flume-1.7.0-bin bigdata-pro03.kfk.com:/opt/modules/<br>3、flume agent-2采集节点服务配置（在bigdata-pro02.kfk.com）<br>三个部分：sources、channels、sinks<br>/opt/datas/weblogs.log是我们要采集的日志</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">vi flume-conf<span class="hljs-selector-class">.properties</span><br><br>agent2<span class="hljs-selector-class">.sources</span> = r1<br>agent2<span class="hljs-selector-class">.channels</span> = c1<br>agent2<span class="hljs-selector-class">.sinks</span> = k1<br><br>agent2<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.type</span> = exec<br>agent2<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.command</span> = tail -F /opt/datas/weblog-flume<span class="hljs-selector-class">.log</span><br>agent2<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.channels</span> = c1<br><br>agent2<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.type</span> = memory<br>agent2<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.capacity</span> = <span class="hljs-number">10000</span><br>agent2<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.transactionCapacity</span> = <span class="hljs-number">10000</span><br>agent2<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.keep-alive</span> = <span class="hljs-number">5</span><br><br>agent2<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.type</span> = avro<br>agent2<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.channel</span> = c1<br>agent2<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.hostname</span> = bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span><br>agent2<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.port</span> = <span class="hljs-number">5555</span><br></code></pre></td></tr></table></figure>
<p>4、flume agent-3采集节点服务配置（在bigdata-pro03.kfk.com）</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">vi flume-conf<span class="hljs-selector-class">.properties</span><br><br>agent3<span class="hljs-selector-class">.sources</span> = r1<br>agent3<span class="hljs-selector-class">.channels</span> = c1<br>agent3<span class="hljs-selector-class">.sinks</span> = k1<br><br>agent3<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.type</span> = exec<br>agent3<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.command</span> = tail -F /opt/datas/weblog-flume<span class="hljs-selector-class">.log</span><br>agent3<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.channels</span> = c1<br><br>agent3<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.type</span> = memory<br>agent3<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.capacity</span> = <span class="hljs-number">10000</span><br>agent3<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.transactionCapacity</span> = <span class="hljs-number">10000</span><br>agent3<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.keep-alive</span> = <span class="hljs-number">5</span><br><br>agent3<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.type</span> = avro<br>agent3<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.channel</span> = c1<br>agent3<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.hostname</span> = bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span><br>agent3<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.port</span> = <span class="hljs-number">5555</span><br></code></pre></td></tr></table></figure>

<h3 id="Flume源码修改与HBase-Kafka集成"><a href="#Flume源码修改与HBase-Kafka集成" class="headerlink" title="Flume源码修改与HBase+Kafka集成"></a>Flume源码修改与HBase+Kafka集成</h3><p><strong>如何修改flume源码？</strong></p>
<p>因为我们需要在节点1上将flume同时发送至Hbase以及kafka，但是hbase结构需要自定义，所以由flume发送至hbase代码需要进行修改。<br>步骤：<br>1.下载Flume源码并导入Idea开发工具<br>1）将apache-flume-1.7.0-src.tar.gz源码下载到本地解压<br>2）通过idea导入flume源码<br>打开idea开发工具，选择File——》Open，找到源码包，选中flume-ng-hbase-sink，点击ok加载相应模块的源码。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzd1atesvcj30dw0fmaae.jpg"><br>2、自己写个类完成类的修改。KfkAsyncHbaseEventSerializer这个是我自定义的。修改其中的下面这个方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> List&lt;PutRequest&gt; <span class="hljs-title function_">getActions</span><span class="hljs-params">()</span> &#123;<br>        List&lt;PutRequest&gt; actions = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br>        <span class="hljs-keyword">if</span> (payloadColumn != <span class="hljs-literal">null</span>) &#123;<br>            <span class="hljs-type">byte</span>[] rowKey;<br>            <span class="hljs-keyword">try</span> &#123;<br>                <span class="hljs-comment">/*---------------------------代码修改开始---------------------------------*/</span><br>                <span class="hljs-comment">//解析列字段</span><br>                String[] columns = <span class="hljs-keyword">new</span> <span class="hljs-title class_">String</span>(<span class="hljs-built_in">this</span>.payloadColumn).split(<span class="hljs-string">&quot;,&quot;</span>);<br>                <span class="hljs-comment">//解析flume采集过来的每行的值</span><br>                String[] values = <span class="hljs-keyword">new</span> <span class="hljs-title class_">String</span>(<span class="hljs-built_in">this</span>.payload).split(<span class="hljs-string">&quot;,&quot;</span>);<br>                <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i &lt; columns.length;i++) &#123;<br>                    <span class="hljs-type">byte</span>[] colColumn = columns[i].getBytes();<br>                    <span class="hljs-type">byte</span>[] colValue = values[i].getBytes(Charsets.UTF_8);<br><br>                    <span class="hljs-comment">//数据校验：字段和值是否对应</span><br>                    <span class="hljs-keyword">if</span> (colColumn.length != colValue.length) <span class="hljs-keyword">break</span>;<br><br>                    <span class="hljs-comment">//时间</span><br>                    <span class="hljs-type">String</span> <span class="hljs-variable">datetime</span> <span class="hljs-operator">=</span> values[<span class="hljs-number">0</span>].toString();<br>                    <span class="hljs-comment">//用户id</span><br>                    <span class="hljs-type">String</span> <span class="hljs-variable">userid</span> <span class="hljs-operator">=</span> values[<span class="hljs-number">1</span>].toString();<br>                    <span class="hljs-comment">//根据业务自定义Rowkey</span><br>                    rowKey = SimpleRowKeyGenerator.getKfkRowKey(userid, datetime);<br>                    <span class="hljs-comment">//插入数据</span><br>                    <span class="hljs-type">PutRequest</span> <span class="hljs-variable">putRequest</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">PutRequest</span>(table, rowKey, cf,<br>                            colColumn, colValue);<br>                    actions.add(putRequest);<br>                    <span class="hljs-comment">/*---------------------------代码修改结束---------------------------------*/</span><br>                &#125;<br>            &#125; <span class="hljs-keyword">catch</span> (Exception e) &#123;<br>                <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">FlumeException</span>(<span class="hljs-string">&quot;Could not get row key!&quot;</span>, e);<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> actions;<br>    &#125;<br></code></pre></td></tr></table></figure>
<p>修改这个类中自定义KEY生成方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleRowKeyGenerator</span> &#123;<br><br>  <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-type">byte</span>[] getKfkRowKey(String userid,String datetime)<span class="hljs-keyword">throws</span> UnsupportedEncodingException &#123;<br>    <span class="hljs-keyword">return</span> (userid + datetime + String.valueOf(System.currentTimeMillis())).getBytes(<span class="hljs-string">&quot;UTF8&quot;</span>);<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<p>3、应该进行测试，但是这边测试完成，目前不知如何搭建，就直接生成jar包放到虚拟机直接用了。<br>4、生成jar包，idea很好用<br>可参考：<a href="https://jingyan.baidu.com/article/c275f6ba0bbb65e33d7567cb.html">https://jingyan.baidu.com/article/c275f6ba0bbb65e33d7567cb.html</a><br>1）在idea工具中，选择File——》ProjectStructrue<br>2）左侧选中Artifacts，然后点击右侧的+号，最后选择JAR——》From modules with dependencies<br>3）一定要设置main class这一项选择自己要打包的类，然后直接点击ok<br>4）删除其他依赖包，只把flume-ng-hbase-sink打成jar包就可以了。<br>5）然后依次点击apply，ok<br>6）点击build进行编译，会自动打成jar包<br>7）到项目的apache-flume-1.7.0-src\flume-ng-sinks\flume-ng-hbase-sink\classes\artifacts\flume_ng_hbase_sink_jar目录下找到刚刚打的jar包<br>8）将打包名字替换为flume自带的包名flume-ng-hbase-sink-1.7.0.jar ，然后上传至虚拟机上flume/lib目录下，覆盖原有的jar包即可。</p>
<p><strong>修改flume配置</strong></p>
<p>这里在节点1上修改flume的配置，完成与hbase和kafka的集成。（flume自定义的jar已经上传覆盖）<br>修改flume-conf.properties</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">agent1.sources</span> = r1<br><span class="hljs-attr">agent1.channels</span> = kafkaC hbaseC <br><span class="hljs-attr">agent1.sinks</span> =  kafkaSink hbaseSink<br><br><span class="hljs-attr">agent1.sources.r1.type</span> = avro<br><span class="hljs-attr">agent1.sources.r1.channels</span> = hbaseC kafkaC<br><span class="hljs-attr">agent1.sources.r1.bind</span> = bigdata-pro01.kfk.com<br><span class="hljs-attr">agent1.sources.r1.port</span> = <span class="hljs-number">5555</span><br><span class="hljs-attr">agent1.sources.r1.threads</span> = <span class="hljs-number">5</span><br><span class="hljs-comment"># flume-hbase</span><br><span class="hljs-attr">agent1.channels.hbaseC.type</span> = memory<br><span class="hljs-attr">agent1.channels.hbaseC.capacity</span> = <span class="hljs-number">100000</span><br><span class="hljs-attr">agent1.channels.hbaseC.transactionCapacity</span> = <span class="hljs-number">100000</span><br><span class="hljs-attr">agent1.channels.hbaseC.keep-alive</span> = <span class="hljs-number">20</span><br><br><span class="hljs-attr">agent1.sinks.hbaseSink.type</span> = asynchbase<br><span class="hljs-attr">agent1.sinks.hbaseSink.table</span> = weblogs<br><span class="hljs-attr">agent1.sinks.hbaseSink.columnFamily</span> = info<br><span class="hljs-attr">agent1.sinks.hbaseSink.channel</span> = hbaseC<br><span class="hljs-attr">agent1.sinks.hbaseSink.serializer</span> = org.apache.flume.sink.hbase.KfkAsyncHbaseEventSerializer<br><span class="hljs-attr">agent1.sinks.hbaseSink.serializer.payloadColumn</span> = datatime,userid,searchname,retorder,cliorder,cliurl<br><span class="hljs-comment">#flume-kafka</span><br><span class="hljs-attr">agent1.channels.kafkaC.type</span> = memory<br><span class="hljs-attr">agent1.channels.kafkaC.capacity</span> = <span class="hljs-number">100000</span><br><span class="hljs-attr">agent1.channels.kafkaC.transactionCapacity</span> = <span class="hljs-number">100000</span><br><span class="hljs-attr">agent1.channels.kafkaC.keep-alive</span> = <span class="hljs-number">20</span><br><br><span class="hljs-attr">agent1.sinks.kafkaSink.channel</span> = kafkaC<br><span class="hljs-attr">agent1.sinks.kafkaSink.type</span> = org.apache.flume.sink.kafka.KafkaSink<br><span class="hljs-attr">agent1.sinks.kafkaSink.brokerList</span> = bigdata-pro01.kfk.com:<span class="hljs-number">9092</span>,bigdata-pro02.kfk.com:<span class="hljs-number">9092</span>,bigdata-pro03.kfk.com:<span class="hljs-number">9092</span><br><span class="hljs-attr">agent1.sinks.kafkaSink.topic</span> = weblogs<br><span class="hljs-attr">agent1.sinks.kafkaSink.zookeeperConnect</span> = bigdata-pro01.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro02.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro03.kfk.com:<span class="hljs-number">2181</span><br><span class="hljs-attr">agent1.sinks.kafkaSink.requiredAcks</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">agent1.sinks.kafkaSink.batchSize</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">agent1.sinks.kafkaSink.serializer.class</span> = kafka.serializer.StringEncoder<br></code></pre></td></tr></table></figure>

<p><strong>小结</strong></p>
<p>项目进行到这里，已经完成了节点2和节点3上flume采集配置、节点1上flume采集并发送至kafka和hbase配置。<br>如下图，这部分都已经完成，下一章进行联调。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzd2e99ywhj30go0gp43u.jpg"></p>
<h3 id="Flume-HBase-Kafka集成全流程测试"><a href="#Flume-HBase-Kafka集成全流程测试" class="headerlink" title="Flume+HBase+Kafka集成全流程测试"></a>Flume+HBase+Kafka集成全流程测试</h3><p><strong>全流程测试简介</strong></p>
<p>将完成对前面所有的设计进行测试，核心是进行flume日志的采集、汇总以及发送至kafka消费、hbase保存。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzd3g6rboxj30go0gp43u.jpg"></p>
<p><strong>原始日志数据简单处理</strong></p>
<p>1、下载搜狗实验室数据<br><a href="http://www.sogou.com/labs/resource/q.php">http://www.sogou.com/labs/resource/q.php</a><br>2、格式说明<br>数据格式为:访问时间\t用户ID\t[查询词]\t该URL在返回结果中的排名\t用户点击的顺序号\t用户点击的URL<br>其中，用户ID是根据用户使用浏览器访问搜索引擎时的Cookie信息自动赋值，即同一次使用浏览器输入的不同查询对应同一个用户ID<br>3、日志简单处理<br>1）将文件中的tab更换成逗号<br>cat weblog.log|tr “\t” “,” &gt; weblog2.log<br>2）将文件中的空格更换成逗号<br>cat weblog2.log|tr “ “ “,” &gt; weblog3.log<br>处理完：<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzd3eylp5zj30l008fab1.jpg"></p>
<p><strong>编写模拟日志生成过程</strong></p>
<p>1、代码实现<br>    实现功能是将原始日志，每次读取一行不断写入到另一个文件中（weblog-flume.log），所以这个文件就相等于服务器中日志不断增加的过程。编写完程序，将该项目打成weblogs.jar包，然后上传至bigdata-pro02.kfk.com节点和bigdata-pro03.kfk.com节点的/opt/jars目录下（目录需要提前创建）<br>2、编写运行模拟日志程序的shell脚本</p>
<figure class="highlight vim"><table><tr><td class="code"><pre><code class="hljs vim"><span class="hljs-number">1</span>）<br>在bigdata-pro02.kfk.<span class="hljs-keyword">com</span>节点的/<span class="hljs-keyword">opt</span>/datas目录下，创建weblog-<span class="hljs-keyword">shell</span>.<span class="hljs-keyword">sh</span>脚本。<br><span class="hljs-keyword">vi</span> weblog-<span class="hljs-keyword">shell</span>.<span class="hljs-keyword">sh</span><br>#/bin/bash<br><span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;start log......&quot;</span><br>#第一个参数是原日志文件，第二个参数是日志生成输出文件<br>java -jar /<span class="hljs-keyword">opt</span>/jars/weblogs.jar /<span class="hljs-keyword">opt</span>/datas/weblog.<span class="hljs-built_in">log</span> /<span class="hljs-keyword">opt</span>/datas/weblog-flume.<span class="hljs-built_in">log</span><br><br>修改weblog-<span class="hljs-keyword">shell</span>.<span class="hljs-keyword">sh</span>可执行权限<br>chmod <span class="hljs-number">777</span> weblog-<span class="hljs-keyword">shell</span>.<span class="hljs-keyword">sh</span><br><span class="hljs-number">2</span>）<br>将bigdata-pro02.kfk.<span class="hljs-keyword">com</span>节点上的/<span class="hljs-keyword">opt</span>/datas/目录拷贝到bigdata-pro03节点.kfk.<span class="hljs-keyword">com</span><br>scp -r /<span class="hljs-keyword">opt</span>/datas/ bigdata-pro03.kfk.<span class="hljs-keyword">com</span>:/<span class="hljs-keyword">opt</span>/datas/<br></code></pre></td></tr></table></figure>
<p>3、运行测试<br>/opt/datas/weblog-shell.sh<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdb284hefj30he0chn95.jpg"></p>
<p><strong>编写一些shell脚本便于执行</strong></p>
<p>1、编写启动flume服务程序的shell脚本</p>
<figure class="highlight vim"><table><tr><td class="code"><pre><code class="hljs vim"><span class="hljs-number">1</span>.在bigdata-pro02.kfk.<span class="hljs-keyword">com</span>节点的flume安装目录下编写flume启动脚本。<br><span class="hljs-keyword">vi</span> flume-kfk-start.<span class="hljs-keyword">sh</span><br>#/bin/bash<br><span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;flume-2 start ......&quot;</span><br>bin/flume-ng agent --<span class="hljs-keyword">conf</span> <span class="hljs-keyword">conf</span> -<span class="hljs-keyword">f</span> <span class="hljs-keyword">conf</span>/flume-<span class="hljs-keyword">conf</span>.properties -n agent2 -Dflume.root.logger=INFO,console<br><span class="hljs-number">2</span>.在bigdata-pro03.kfk.<span class="hljs-keyword">com</span>节点的flume安装目录下编写flume启动脚本。<br><span class="hljs-keyword">vi</span> flume-kfk-start.<span class="hljs-keyword">sh</span><br>#/bin/bash<br><span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;flume-3 start ......&quot;</span><br>bin/flume-ng agent --<span class="hljs-keyword">conf</span> <span class="hljs-keyword">conf</span> -<span class="hljs-keyword">f</span> <span class="hljs-keyword">conf</span>/flume-<span class="hljs-keyword">conf</span>.properties -n agent3 -Dflume.root.logger=INFO,console<br><span class="hljs-number">3</span>.在bigdata-pro01.kfk.<span class="hljs-keyword">com</span>节点的flume安装目录下编写flume启动脚本。<br><span class="hljs-keyword">vi</span> flume-kfk-start.<span class="hljs-keyword">sh</span><br>#/bin/bash<br><span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;flume-1 start ......&quot;</span><br>bin/flume-ng agent --<span class="hljs-keyword">conf</span> <span class="hljs-keyword">conf</span> -<span class="hljs-keyword">f</span> <span class="hljs-keyword">conf</span>/flume-<span class="hljs-keyword">conf</span>.properties -n agent1 -Dflume.root.logger=INFO,console<br><br></code></pre></td></tr></table></figure>
<p>2、编写Kafka Consumer执行脚本</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus"><span class="hljs-number">1</span>.在bigdata-pro01<span class="hljs-selector-class">.kfk</span>.com节点的Kafka安装目录下编写Kafka Consumer执行脚本<br>vi kfk-test-consumer<span class="hljs-selector-class">.sh</span><br>#/bin/bash<br>echo <span class="hljs-string">&quot;kfk-kafka-consumer.sh start ......&quot;</span><br>bin/kafka-console-consumer<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--zookeeper</span> bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro03<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span> <span class="hljs-attr">--from-beginning</span> <span class="hljs-attr">--topic</span> weblogs<br><span class="hljs-number">2</span>.将kfk-test-consumer.sh脚本分发另外两个节点<br>scp kfk-test-consumer<span class="hljs-selector-class">.sh</span> bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:/opt/modules/kakfa_2.<span class="hljs-number">11</span>-<span class="hljs-number">0.8</span>.<span class="hljs-number">2.1</span>/<br>scp kfk-test-consumer<span class="hljs-selector-class">.sh</span> bigdata-pro03<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:/opt/modules/kakfa_2.<span class="hljs-number">11</span>-<span class="hljs-number">0.8</span>.<span class="hljs-number">2.1</span>/<br><br></code></pre></td></tr></table></figure>
<p><strong>联调测试-数据采集分发</strong></p>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk"><span class="hljs-number">1</span>、在各个节点上启动zk<br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/zookeeper-3.4.5-cdh5.10.0/</span>sbin/zkServer.sh start  <br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/zookeeper-3.4.5-cdh5.10.0/</span>bin/zkCli.sh  登陆客户端进行测试是否启动成功<br><br><span class="hljs-number">2</span>、启动hdfs  --- http:<span class="hljs-regexp">//</span>bigdata-pro01.kfk.com:<span class="hljs-number">50070</span>/<br>在节点<span class="hljs-number">1</span>：<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>sbin/start-dfs.sh <br><span class="hljs-comment">#节点1 和 节点2  启动namenode高可用</span><br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>sbin/hadoop-daemon.sh start zkfc<br><br><span class="hljs-number">3</span>、启动hbase  ----http:<span class="hljs-regexp">//</span>bigdata-pro01.kfk.com:<span class="hljs-number">60010</span>/<br><span class="hljs-comment">#节点 1  启动hbase</span><br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hbase-1.0.0-cdh5.4.0/</span>bin/start-hbase.sh<br><span class="hljs-comment">#在节点2 启动备用master</span><br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hbase-1.0.0-cdh5.4.0/</span>bin/hbase-daemon.sh start  master<br><span class="hljs-comment">#启动hbase的shell用于操作</span><br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hbase-1.0.0-cdh5.4.0/</span>bin/hbase shell<br><span class="hljs-comment">#创建hbase业务表</span><br>bin/hbase shell<br>create <span class="hljs-string">&#x27;weblogs&#x27;</span>,<span class="hljs-string">&#x27;info&#x27;</span><br><br><span class="hljs-number">4</span>、启动kafka<br><span class="hljs-comment">#在各个个节点启动kafka</span><br>cd <span class="hljs-regexp">/opt/m</span>odules/kafka_2.<span class="hljs-number">10</span>-<span class="hljs-number">0.9</span>.<span class="hljs-number">0.0</span><br>bin<span class="hljs-regexp">/kafka-server-start.sh config/</span>server.properties &amp;<br><span class="hljs-comment">#创建业务</span><br>bin/kafka-topics.sh --zookeeper bigdata-pro01.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro02.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro03.kfk.com:<span class="hljs-number">2181</span> --create --topic weblogs --replication-factor <span class="hljs-number">2</span> --partitions <span class="hljs-number">1</span><br><span class="hljs-comment">#消费(之前编写的脚本可以用)</span><br>bin/kafka-console-consumer.sh --zookeeper bigdata-pro01.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro02.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro03.kfk.com:<span class="hljs-number">2181</span> --from-beginning --topic weblogs<br></code></pre></td></tr></table></figure>
<p>一定确保上述都启动成功能，利用jps查看各个节点进程情况。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbmh1n31j309v042glj.jpg"><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbmovok3j309n03sa9y.jpg"><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbmw14tjj309o02cweb.jpg"></p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus"><span class="hljs-number">5</span>、各个节点启动flume<br>#三节点启动flume<br>/opt/modules/flume-<span class="hljs-number">1.7</span>.<span class="hljs-number">0</span>-bin/flume-kfk-start<span class="hljs-selector-class">.sh</span><br><br><span class="hljs-number">6</span>、在节点<span class="hljs-number">2</span>和<span class="hljs-number">3</span>启动日志模拟生产<br>/opt/datas/weblog-shell<span class="hljs-selector-class">.sh</span><br><br><span class="hljs-number">7</span>、启动kafka消费程序<br>#消费（或者使用写好的脚本kfk-test-consumer.sh）<br>bin/kafka-console-consumer<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--zookeeper</span> bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro03<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span> <span class="hljs-attr">--from-beginning</span> <span class="hljs-attr">--topic</span> weblogs<br><br><span class="hljs-number">8</span>、查看hbase数据写入情况<br>./hbase-shell<br>count <span class="hljs-string">&#x27;weblogs&#x27;</span><br></code></pre></td></tr></table></figure>
<p>结果：<br>kafka不断消费<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbszmkybj30rh0940ue.jpg"><br>hbase数据不断增加<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbtek6eqj30rv0ar0ud.jpg"></p>
<h3 id="mysql、Hive安装与集成"><a href="#mysql、Hive安装与集成" class="headerlink" title="mysql、Hive安装与集成"></a>mysql、Hive安装与集成</h3><p><strong>为什么要用mysql?</strong></p>
<p>一方面，本项目用来存储Hive的元数据；另一方面，可以把离线分析结果放入mysql中；</p>
<p><strong>安装mysql</strong></p>
<p>通过yum在线mysql，具体操作命令如下所示(关于yum源可以修改为阿里的，比较快和稳定)</p>
<figure class="highlight subunit"><table><tr><td class="code"><pre><code class="hljs subunit">1、在线安装mysql<br>通过yum在线mysql，具体操作命令如下所示。<br>yum clean all<br>yum install mysql-server<br>2、mysql 服务启动并测试<br>sudo chown -R kfk:kfk /usr/bin/mysql    修改权限给kfk<br>1）查看mysql服务状态<br>sudo service mysqld status  <br>2）启动mysql服务<br>sudo service mysqld start<br>3）设置mysql密码<br>/usr/bin/mysqladmin -u root password &#x27;123456&#x27;<br>4）连接mysql<br>mysql –uroot -p123456<br>a）查看数据库<br>show databases;<br>mysql<br><span class="hljs-keyword">test</span><br><span class="hljs-keyword"></span>b）查看数据库<br>use test;<br>c）查看表列表<br>show tables;<br></code></pre></td></tr></table></figure>
<p>出现问题，大多数是权限问题，利用sudo执行或者重启mysql.</p>
<p><strong>安装Hive</strong></p>
<p>Hive在本项目中功能是，将hbase中的数据进行离线分析，输出处理结果，可以到mysql或者hbase，然后进行可视化。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzfpw9k0v7j30kv09rtfp.jpg"><br>这里版本采用的是：apache-hive-2.1.0-bin.tar.gz<br>（之前用apache-hive-0.13.1-bin.tar.gz出现和hbase集成失败，原因很奇怪，下一章详细讲）。<br>1、解压</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk">步骤都老生常谈了。。。<br>tar -zxf apache-hive-<span class="hljs-number">2.1</span>.<span class="hljs-number">0</span>-bin.tar.gz -C <span class="hljs-regexp">/opt/m</span>odules/<br>mv  apache-hive-<span class="hljs-number">2.1</span>.<span class="hljs-number">0</span>-bin hive-<span class="hljs-number">2.1</span>.<span class="hljs-number">0</span>     <span class="hljs-regexp">//</span>重命名<br></code></pre></td></tr></table></figure>
<p>2、修改配置文件</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">1</span>）hive-log4j.properties<br><span class="hljs-comment">#日志目录需要提前创建</span><br><span class="hljs-attribute">hive</span>.log.dir=/opt/modules/hive-<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">0</span>/logs<br><span class="hljs-attribute">2</span>）修改hive-env.sh配置文件<br><span class="hljs-attribute">HADOOP_HOME</span>=/opt/modules/hadoop-<span class="hljs-number">2</span>.<span class="hljs-number">6</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">HBASE_HOME</span>=/opt/modules/hbase-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span><br><span class="hljs-comment"># Hive Configuration Directory can be controlled by:</span><br><span class="hljs-attribute">export</span> HIVE_CONF_DIR=/opt/modules/hive-<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">0</span>/conf<br></code></pre></td></tr></table></figure>
<p>3、启动进行测试<br>首先启动HDFS，然后创建Hive的目录<br>bin/hdfs dfs -mkdir -p /tmp<br>bin/hdfs dfs -chmod g+w /tmp<br>bin/hdfs dfs -mkdir -p /user/hive/warehouse<br>bin/hdfs dfs -chmod g+w /user/hive/warehouse<br>4、测试</p>
<figure class="highlight gauss"><table><tr><td class="code"><pre><code class="hljs gauss">./hive<br><span class="hljs-meta">#查看数据库</span><br><span class="hljs-keyword">show</span> databases;<br><span class="hljs-meta">#使用默认数据库</span><br><span class="hljs-keyword">use</span> default;<br><span class="hljs-meta">#查看表</span><br><span class="hljs-keyword">show</span> tables;<br><br></code></pre></td></tr></table></figure>
<p><strong>Hive与mysql集成</strong></p>
<p>利用mysql放Hive的元数据。<br>1、在/opt/modules/hive-2.1.0/conf目录下创建hive-site.xml文件，配置mysql元数据库。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="hljs-meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><br><br><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>jdbc:mysql://bigdata-pro01.kfk.com/metastore?createDatabaseIfNotExist=true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br> <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>root<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>123456<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>   <br>	<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com,bigdata-pro02.kfk.com,bigdata-pro03.kfk.com<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure>
<p>2、设置用户连接信息</p>
<p>1）查看用户信息<br>mysql -uroot -p123456<br>show databases;<br>use mysql;<br>show tables;<br>select User,Host,Password from user;<br>2）更新用户信息<br>update user set Host=’%’ where User = ‘root’ and Host=’localhost’<br>3）删除用户信息<br>delete from user where user=’root’ and host=’127.0.0.1’<br>select User,Host,Password from user;<br>delete from user where host=’localhost’;<br>删除到只剩图中这一行数据<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzfqckmjxej30ej031q2s.jpg"><br>4）刷新信息<br>flush privileges;<br>3.拷贝mysql驱动包到hive的lib目录下<br>cp  mysql-connector-java-5.1.35.jar /opt/modules/hive-2.1.0/lib/<br>4.保证第三台集群到其他节点无秘钥登录</p>
<p><strong>Hive与mysql测试</strong></p>
<p>1.启动HDFS和YARN服务<br>2.启动hive<br>./hive<br>3.通过hive服务创建表<br>CREATE TABLE stu(id INT,name STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’ ;<br>4.创建数据文件<br>vi /opt/datas/stu.txt<br>00001    zhangsan<br>00002    lisi<br>00003    wangwu<br>00004    zhaoliu<br>5.加载数据到hive表中<br>load data local inpath ‘/opt/datas/stu.txt’ into table stu;<br>直接在hive查看表中内容就ok。<br>在mysql数据库中hive的metastore元数据。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzfqeibkrtj306103ta9v.jpg"></p>
<h3 id="Hive与Hbase集成"><a href="#Hive与Hbase集成" class="headerlink" title="Hive与Hbase集成"></a>Hive与Hbase集成</h3><p><strong>Hive与HBase集成配置</strong></p>
<p>1）在hive-site.xml文件中配置Zookeeper，hive通过这个参数去连接HBase集群。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>   <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com,bigdata-pro02.kfk.com,bigdata-pro03.kfk.com<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br></code></pre></td></tr></table></figure>
<p>2）需要把hbase中的部分jar包拷贝到hive中<br>这里采用软连接的方式：<br>执行如下命令：</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">export</span> HBASE_HOME=/opt/modules/hbase-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">export</span> HIVE_HOME=/opt/modules/hive-<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-server-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-server-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar<br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-client-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-client-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar<br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-protocol-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-protocol-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar <br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-it-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-it-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar <br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/htrace-core-<span class="hljs-number">3</span>.<span class="hljs-number">0</span>.<span class="hljs-number">4</span>.jar $HIVE_HOME/lib/htrace-core-<span class="hljs-number">3</span>.<span class="hljs-number">0</span>.<span class="hljs-number">4</span>.jar<br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-hadoop2-compat-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-hadoop2-compat-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar <br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-hadoop-compat-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-hadoop-compat-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar<br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/high-scale-lib-<span class="hljs-number">1</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span>.jar $HIVE_HOME/lib/high-scale-lib-<span class="hljs-number">1</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span>.jar <br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-common-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-common-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar <br></code></pre></td></tr></table></figure>
<p>3）测试<br>在hbase中建立一个表，里面存有数据（实际底层就是在hdfs上），然后Hive创建一个表与HBase中的表建立联系。</p>
<ol>
<li>先在hbase建立一个表<br>（不熟悉的，看指令<a href="https://www.cnblogs.com/cxzdy/p/5583239.html%EF%BC%89">https://www.cnblogs.com/cxzdy/p/5583239.html）</a><br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzgupdmei1j30h5037mx4.jpg"></li>
<li>启动hive,建立联系（之前要先启动mysql，因为元数据在里面)</li>
</ol>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">external</span> <span class="hljs-keyword">table</span> t1(<br>key <span class="hljs-type">int</span>,<br><span class="hljs-type">name</span> string,<br>age string<br>)  <br>STORED <span class="hljs-keyword">BY</span>  <span class="hljs-string">&#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span> <br><span class="hljs-keyword">WITH</span> SERDEPROPERTIES(&quot;hbase.columns.mapping&quot; = &quot;:key,info:name,info:age&quot;) <br>TBLPROPERTIES(&quot;hbase.table.name&quot; = &quot;t1&quot;);<br></code></pre></td></tr></table></figure>
<ol start="3">
<li>hive结果<br>执行 select * from t1;<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzgutrr5x7j30b0035glg.jpg"></li>
<li>为项目中的weblogs建立联系<br>之前我们把数据通过flume导入到hbase中了，所以同样我们在hive中建立联系，可以用hive对hbase中的数据进行简单的sql分析，离线分析。</li>
</ol>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><code class="hljs n1ql"><span class="hljs-keyword">create</span> external table weblogs(<br>id <span class="hljs-keyword">string</span>,<br>datatime <span class="hljs-keyword">string</span>,<br>userid <span class="hljs-keyword">string</span>,<br>searchname <span class="hljs-keyword">string</span>,<br>retorder <span class="hljs-keyword">string</span>,<br>cliorder <span class="hljs-keyword">string</span>,<br>cliurl <span class="hljs-keyword">string</span><br>)  <br>STORED <span class="hljs-keyword">BY</span>  <span class="hljs-string">&#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span> <br><span class="hljs-keyword">WITH</span> SERDEPROPERTIES(<span class="hljs-string">&quot;hbase.columns.mapping&quot;</span> = <span class="hljs-string">&quot;:key,info:datatime,info:userid,info:searchname,info:retorder,info:cliorder,info:cliurl&quot;</span>) <br>TBLPROPERTIES(<span class="hljs-string">&quot;hbase.table.name&quot;</span> = <span class="hljs-string">&quot;weblogs&quot;</span>);<br></code></pre></td></tr></table></figure>

<p><strong>Hive与HBase集成中的致命bug</strong></p>
<p>问题如图：<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzguxe0p4ej30nu0hl0ua.jpg"></p>
<p>hbase 1.x之后的版本，需要更高版本的hive匹配，最好是hive 2.x,上述的错误是因为hive-0.13.1-bin和hbase-1.0.0-cdh5.4.0，应该是不兼容导致的，莫名bug。于是采用了 hive-2.1.0，查了下这个版本与hadoop其他组件也是兼容的，所以，采用这个。配置仍然采用刚才的方法（上一章和这一章），主要有mysql元数据配置（驱动包别忘了），各种xml配置，测试下。最后，在重启hive之前，<strong>先把hbase重启了</strong>，很重要。</p>
<h3 id="HUE大数据可视化分析"><a href="#HUE大数据可视化分析" class="headerlink" title="HUE大数据可视化分析"></a>HUE大数据可视化分析</h3><p><strong>下载和安装Hue</strong></p>
<p>版本选择： hue-3.9.0-cdh5.15.0<br>1、首先需要利用yum安装依赖包，虚拟机需要联网，这里安装在节点3上。</p>
<figure class="highlight brainfuck"><table><tr><td class="code"><pre><code class="hljs brainfuck"><span class="hljs-comment">yum</span> <span class="hljs-literal">-</span><span class="hljs-comment">y install ant asciidoc cyrus</span><span class="hljs-literal">-</span><span class="hljs-comment">sasl</span><span class="hljs-literal">-</span><span class="hljs-comment">devel</span> <span class="hljs-comment">cyrus</span><span class="hljs-literal">-</span><span class="hljs-comment">sasl</span><span class="hljs-literal">-</span><span class="hljs-comment">gssapi</span> <span class="hljs-comment">gcc</span> <span class="hljs-comment">gcc</span><span class="hljs-literal">-</span><span class="hljs-comment">c</span>++ <span class="hljs-comment">krb5</span><span class="hljs-literal">-</span><span class="hljs-comment">devel</span> <span class="hljs-comment">libtidy</span> <span class="hljs-comment">libxml2</span><span class="hljs-literal">-</span><span class="hljs-comment">devel libxslt-devel openldap-devel python-devel sqlite</span><span class="hljs-literal">-</span><span class="hljs-comment">devel openssl-devel mysql-devel gmp-devel </span> <br></code></pre></td></tr></table></figure>
<p>2、解压<br>tar -zxf hue-3.9.0-cdh5.15.0.tar.gz -C /opt/modules/<br>3、编译<br>cd  hue-3.9.0-cdh5.15.0<br>make apps<br>4、基本配置与测试</p>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-number">1</span>）修改配置文件<br>cd desktop<br>cd conf<br>vi hue.ini<br>#秘钥<br>secret_key=jFE93j;<span class="hljs-number">2</span>[<span class="hljs-number">290</span>-eiw.KEiwN2s3[<span class="hljs-string">&#x27;d;/.q[eIW^y#e=+Iei*@Mn &lt; qW5o</span><br><span class="hljs-string">#host port</span><br><span class="hljs-string">http_host=bigdata-pro03.kfk.com</span><br><span class="hljs-string">http_port=8888</span><br><span class="hljs-string">#时区</span><br><span class="hljs-string">time_zone=Asia/Shanghai</span><br><span class="hljs-string">2）修改desktop.db 文件权限</span><br><span class="hljs-string">chmod o+w desktop/desktop.db</span><br><span class="hljs-string">3）启动Hue服务</span><br><span class="hljs-string">/opt/modules/hue-3.9.0-cdh5.15.0/build/env/bin/supervisor</span><br><span class="hljs-string">4）查看Hue web界面</span><br><span class="hljs-string">bigdata-pro03.kfk.com:8888</span><br></code></pre></td></tr></table></figure>
<p><strong>Hue与HDFS集成</strong></p>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk"><span class="hljs-number">1</span>）修改hadoop中core-site.xml配置文件，添加如下内容<br>&lt;property&gt;<br>    &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt;<br>    &lt;value&gt;*&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;property&gt;<br>    &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt;<br>    &lt;value&gt;*&lt;/value&gt;<br>&lt;/property&gt;<br><br><span class="hljs-number">2</span>）修改hue.ini配置文件<br>fs_defaultfs=hdfs:<span class="hljs-regexp">//</span>ns<br>webhdfs_url=http:<span class="hljs-regexp">//</span>bigdata-pro01.kfk.com:<span class="hljs-number">50070</span><span class="hljs-regexp">/webhdfs/</span>v1<br>hadoop_hdfs_home=<span class="hljs-regexp">/opt/m</span>odules/hadoop-<span class="hljs-number">2.6</span>.<span class="hljs-number">0</span><br>hadoop_bin=<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>bin<br>hadoop_conf_dir=<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc/hadoop<br><span class="hljs-number">3</span>）将core-site.xml配置文件分发到其他节点<br>scp core-site.xml bigdata-pro02.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc/hadoop<br>scp core-site.xml bigdata-pro01.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc/hadoop<br><span class="hljs-number">4</span>）重新启动hue<br>先启动zk,hdfs，再启动hue<br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hue-3.9.0-cdh5.15.0/</span>build<span class="hljs-regexp">/env/</span>bin/supervisor<br></code></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzk8cc0l9rj30ev0ebmx9.jpg"></p>
<p><strong>Hue与YARN集成</strong></p>
<p>1、修改hue.ini配置文件,参考<a href="https://www.cnblogs.com/zlslch/p/6817226.html">https://www.cnblogs.com/zlslch/p/6817226.html</a><br>区分yarn是不是HA</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[[yarn_clusters]]</span><br><br>   <span class="hljs-section">[[[default]]]</span><br>     <span class="hljs-attr">resourcemanager_host</span>=rs<br>     <span class="hljs-attr">resourcemanager_port</span>=<span class="hljs-number">8032</span><br>     <span class="hljs-attr">submit_to</span>=<span class="hljs-literal">True</span><br>     <span class="hljs-attr">logical_name</span>=rm1<br>     <span class="hljs-attr">resourcemanager_api_url</span>=http://bigdata-pro01.kfk.com:<span class="hljs-number">8088</span><br>     <span class="hljs-attr">proxy_api_url</span>=http://bigdata-pro01.kfk.com:<span class="hljs-number">8088</span><br>     <span class="hljs-attr">history_server_api_url</span>=http://bigdata-pro01.kfk.com:<span class="hljs-number">19888</span><br><br>    <span class="hljs-section">[[[ha]]]</span><br>     <span class="hljs-attr">logical_name</span>=rm2<br>     <span class="hljs-attr">submit_to</span>=<span class="hljs-literal">True</span><br>     <span class="hljs-attr">resourcemanager_api_url</span>=http://bigdata-pro02.kfk.com:<span class="hljs-number">8088</span><br>  <span class="hljs-attr">history_server_api_url</span>=http://bigdata-pro01.kfk.com:<span class="hljs-number">19888</span><br></code></pre></td></tr></table></figure>
<p>2、测试<br>启动yarn，再重启hue。<br>图中的任务是我之前进行的任务<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzk8i4ao6kj314r07j74m.jpg"></p>
<p><strong>Hue与mysql、hive集成</strong></p>
<p>1、修改hue.ini配置</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><code class="hljs routeros">  [beeswax]<br><br><br>    <span class="hljs-attribute">hive_server_host</span>=bigdata-pro03.kfk.com<br>    <span class="hljs-attribute">hive_server_port</span>=10000<br>    <span class="hljs-attribute">hive_conf_dir</span>=/opt/modules/hive-2.1.0/conf<br><br><br><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span>.中间其他<span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><br>   [[[mysql]]]<br>    <span class="hljs-attribute">nice_name</span>=<span class="hljs-string">&quot;My SQL DB&quot;</span><br><br>    <span class="hljs-attribute">name</span>=metastore<br>    <span class="hljs-attribute">engine</span>=mysql<br><br>    <span class="hljs-attribute">host</span>=bigdata-pro01.kfk.com<br>    <span class="hljs-attribute">port</span>=3306<br>    <span class="hljs-attribute">user</span>=root<br>    <span class="hljs-attribute">password</span>=123456<br></code></pre></td></tr></table></figure>
<p>2、测试<br>启动节点1的mysql（这是元数据），再启动节点3的hive服<br>/opt/modules/hive-2.1.0/bin/hive –service hiveserver2 &amp;    ##配合hue服务<br>再重启hue。<br>图中是利用hive中的sql查询，hive中的表。但是有一个问题是：我用hive查询hbase中的表，无法查询，出现超时情况，目前还没解决，搞了2天难受，（本来想直接在hue中用hive来处理hbase中的表进行离线计算，但是没法查询，只能查询hive本身自己的表，另外hive的beeline模式也无法查询hbase表，但是hive cli模式可以的查询）<br>问题日志：:java.io.IOException: org.apache.hadoop.hbase.client.RetriesExhaustedException<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzk8rvrxu6j30n20e3q35.jpg"></p>
<p><strong>Hue与hbase集成</strong></p>
<p>1、修改hue.ini配置</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[hbase]</span><br>   <span class="hljs-attr">hbase_clusters</span>=(Cluster|bigdata-pro01.kfk.com:<span class="hljs-number">9090</span>)<br>   <span class="hljs-attr">hbase_conf_dir</span>=/opt/modules/hbase-<span class="hljs-number">1.0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4.0</span>/conf<br>  <span class="hljs-attr">thrift_transport</span>=buffered<br></code></pre></td></tr></table></figure>
<p>2、启动测试<br>先启动hbase,再启动HBase中启动thrift服务<br>/opt/modules/hbase-1.0.0-cdh5.4.0/bin/hbase-daemon.sh start thrift<br>然后重启hue<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzk9mr2i83j30kp0ddaa9.jpg"></p>
<h3 id="Spark2-X集群安装与spark-on-yarn部署"><a href="#Spark2-X集群安装与spark-on-yarn部署" class="headerlink" title="Spark2.X集群安装与spark on yarn部署"></a>Spark2.X集群安装与spark on yarn部署</h3><p><strong>spark集群安装</strong></p>
<p>版本是spark-2.2.0-bin-hadoop2.6.tgz，之前用的是hadoop2.6.0.<br>环境要求：scala-2.11.12.tgz/java8/hadoop2.6.0.<br>1、官网下载<br><a href="https://spark.apache.org/downloads.html">https://spark.apache.org/downloads.html</a><br>2、spark配置<br>配置spark-env.sh</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/opt/modules/jdk1.8.0_191<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SCALA_HOME</span>=/opt/modules/scala-2.11.12<br><br><span class="hljs-built_in">export</span> <span class="hljs-attribute">HADOOP_CONF_DIR</span>=/opt/modules/hadoop-2.6.0/etc/hadoop<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_CONF_DIR</span>=/opt/modules/spark-2.2.0/conf<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_MASTER_HOST</span>=bigdata-pro02.kfk.com<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_MASTER_PORT</span>=7077<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_MASTER_WEBUI_PORT</span>=8080<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_WORKER_CORES</span>=1<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_WORKER_MEMORY</span>=1g<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_WORKER_PORT</span>=7078<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_WORKER_WEBUI_PORT</span>=8081<br></code></pre></td></tr></table></figure>
<p>配置slaves</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span><br>bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span><br>bigdata-pro03<span class="hljs-selector-class">.kfk</span>.com<br></code></pre></td></tr></table></figure>
<p>如果整合hive,hive用到mysql数据库的话，需要将mysql数据库连接驱动jmysql-connector-java-5.1.7-bin.jar放到$SPARK_HOME/jars目录下<br>3、分发至各个节点<br>4、设定的主节点上启动测试(这是standalone模式)<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzogese9lvj30on05o74m.jpg"><br>打开spark服务网址：<a href="http://bigdata-pro02.kfk.com:8080/">http://bigdata-pro02.kfk.com:8080/</a><br>可以查看到各个节点的情况。<br>5、可以stop-all，因为yarn模式下根本不需要。</p>
<p><strong>spark on Yarn</strong></p>
<p>standalonen模式和spark on Yarn模式比较： <a href="https://blog.csdn.net/lxhandlbb/article/details/70214003">https://blog.csdn.net/lxhandlbb/article/details/70214003</a><br>spark on Yarn原理：<a href="https://blog.csdn.net/liuwei0376/article/details/78637732">https://blog.csdn.net/liuwei0376/article/details/78637732</a><br>1、前提条件<br>已经安装了hadoop2.6.0，并可以运行，因为spark运行需要依赖hadoop.<br>2、运行zk、hdfs和yarn<br>高可用下的zk也要运行<br>hadoop:<a href="http://bigdata-pro01.kfk.com:50070/">http://bigdata-pro01.kfk.com:50070</a><br>yarn：<a href="http://bigdata-pro01.kfk.com:8088/">http://bigdata-pro01.kfk.com:8088</a><br>3、主节点运行spark<br>./spark-shell –master yarn –deploy-mode client<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzohi45ckpj30o70auq3g.jpg"><br>在yarn的网页中也可以看到。<br>虚拟机内存小的话，会出现问题：</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">17</span>/<span class="hljs-number">09</span>/<span class="hljs-number">08</span> <span class="hljs-number">10</span>:<span class="hljs-number">36</span>:<span class="hljs-number">08</span> ERROR spark.SparkContext: Error initializing SparkContext.<br><span class="hljs-attribute">org</span>.apache.spark.SparkException: Yarn application has already ended! It might have been killed or unable to launch application master.<br></code></pre></td></tr></table></figure>
<p>解决办法：先停止YARN服务，然后修改yarn-site.xml，分发至各个节点。再重启。<br>增加如下内容</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>false<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>4<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br></code></pre></td></tr></table></figure>
<p>4、测试下程序运行</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">sc</span>.parallelize(<span class="hljs-number">1</span> to <span class="hljs-number">100</span>,<span class="hljs-number">5</span>).count<br></code></pre></td></tr></table></figure>
<p>查看程序运行情况：<br>1）入口yarn的web网页，<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzohlg58qyj31dz0b8tam.jpg"><br>2）点击applicationmaster进入<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzoi538xtkj31ck0bwjrz.jpg"><br>可能出现问题进不去网页：<br>配置显示在主节点：这里配置节点1，那么RM应该在1的时候可以显示，之前我配集群总名称rs，没法用。<br>修改yarn-site.xml，分发至各个节点，然后重启。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs <property>">	&lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;<br>	&lt;value&gt;bigdata-pro01.kfk.com:8088&lt;/value&gt;<br>&lt;/property&gt;<br></code></pre></td></tr></table></figure>

<h3 id="基于IDEA环境下的Spark2-X程序开发"><a href="#基于IDEA环境下的Spark2-X程序开发" class="headerlink" title="基于IDEA环境下的Spark2.X程序开发"></a>基于IDEA环境下的Spark2.X程序开发</h3><p><strong>开发环境配置</strong></p>
<p>1、安装idea<br>2、安装maven<br>官网下载：apache-maven-3.6.0<br>3、安装java8，并配置环境变量<br>4、安装scala，直接从idea插件下载安装<br>5、安装hadoop在Windows中的运行环境，并配置环境变量</p>
<p><strong>IDEA程序开发</strong></p>
<p>可以参考这个链接很全：<a href="https://blog.csdn.net/zkf541076398/article/details/79297820">https://blog.csdn.net/zkf541076398/article/details/79297820</a><br>1、新建maven项目<br>2、配置maven<br>3、选择配置scala和java版本<br>4、新建scala目录并设置为source(看图)<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzokar9bjxj30pj0dc13u.jpg"><br>5、编写pom.xml文件<br>这里主要你需要什么就放什么，可以github上找例子<br><a href="https://github.com/apache/spark/blob/master/examples/pom.xml">https://github.com/apache/spark/blob/master/examples/pom.xml</a><br>我的pom，我自己可以用</p>
<figure class="highlight dust"><table><tr><td class="code"><pre><code class="hljs dust"><span class="language-xml"><span class="hljs-meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">project</span> <span class="hljs-attr">xmlns</span>=<span class="hljs-string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span> <span class="hljs-attr">xmlns:xsi</span>=<span class="hljs-string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span> <span class="hljs-attr">xsi:schemaLocation</span>=<span class="hljs-string">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">modelVersion</span>&gt;</span>4.0.0<span class="hljs-tag">&lt;/<span class="hljs-name">modelVersion</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">packaging</span>&gt;</span>war<span class="hljs-tag">&lt;/<span class="hljs-name">packaging</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>TestSpark<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>com.kfk.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>TestSpark<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>1.0-SNAPSHOT<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml"></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">properties</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">scala.version</span>&gt;</span>2.11.12<span class="hljs-tag">&lt;/<span class="hljs-name">scala.version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">scala.binary.version</span>&gt;</span>2.11<span class="hljs-tag">&lt;/<span class="hljs-name">scala.binary.version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">spark.version</span>&gt;</span>2.2.0<span class="hljs-tag">&lt;/<span class="hljs-name">spark.version</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;/<span class="hljs-name">properties</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">dependencies</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-core_$</span><span class="hljs-template-variable">&#123;scala.binary.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$</span><span class="hljs-template-variable">&#123;spark.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-streaming_$</span><span class="hljs-template-variable">&#123;scala.binary.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$</span><span class="hljs-template-variable">&#123;spark.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-sql_$</span><span class="hljs-template-variable">&#123;scala.binary.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$</span><span class="hljs-template-variable">&#123;spark.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-hive_$</span><span class="hljs-template-variable">&#123;scala.binary.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$</span><span class="hljs-template-variable">&#123;spark.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_$</span><span class="hljs-template-variable">&#123;scala.binary.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$</span><span class="hljs-template-variable">&#123;spark.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.hadoop<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>hadoop-client<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>2.6.0<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;/<span class="hljs-name">dependencies</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">project</span>&gt;</span></span><br><span class="language-xml"></span><br></code></pre></td></tr></table></figure>
<p>6、编写测试程序</p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><code class="hljs reasonml">import org.apache.spark.sql.SparkSession<br><br><span class="hljs-keyword">object</span> test &#123;<br>  def main(args: Array<span class="hljs-literal">[S<span class="hljs-identifier">tring</span>]</span>): Unit = &#123;<br><br>     <span class="hljs-keyword">val</span> spark = SparkSession<br>      .builder<br>       .master(<span class="hljs-string">&quot;yarn-cluster&quot;</span>)<br>     <span class="hljs-comment">//  .master(&quot;local[2]&quot;)</span><br>      .app<span class="hljs-constructor">Name(<span class="hljs-string">&quot;HdfsTest&quot;</span>)</span><br>      .get<span class="hljs-constructor">OrCreate()</span><br><br>    <span class="hljs-keyword">val</span> path = args(<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">val</span> out = args(<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">val</span> rdd = spark.sparkContext.text<span class="hljs-constructor">File(<span class="hljs-params">path</span>)</span><br>    <span class="hljs-keyword">val</span> lines = rdd.flat<span class="hljs-constructor">Map(<span class="hljs-params">_</span>.<span class="hljs-params">split</span>(<span class="hljs-string">&quot; &quot;</span>)</span>).map(x=&gt;(x,<span class="hljs-number">1</span>)).reduce<span class="hljs-constructor">ByKey((<span class="hljs-params">a</span>,<span class="hljs-params">b</span>)</span>=&gt;(a+b)).save<span class="hljs-constructor">AsTextFile(<span class="hljs-params">out</span>)</span><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>
<p>7、本地测试<br>直接master(“local[2]”)，指定windows下的路径就可以了。如果不能运行一定是开发环境有问题，主要看看hadoop环境变量配置了吗<br>8、打成jar包<br>可参考：<a href="https://jingyan.baidu.com/article/c275f6ba0bbb65e33d7567cb.html">https://jingyan.baidu.com/article/c275f6ba0bbb65e33d7567cb.html</a><br>9、上传至虚拟机中进行jar包方式提交到spark on yarn.<br>运行底层还是依赖于hdfs，前提要启动zk /hadoop /yarn.</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk">bin<span class="hljs-regexp">/spark-submit --class  test  --master yarn --deploy-mode cluster /</span>opt<span class="hljs-regexp">/jars/</span>TestSpark.jar  hdfs:<span class="hljs-regexp">//</span>ns<span class="hljs-regexp">/input/</span>stu.txt  hdfs:<span class="hljs-regexp">//</span>ns/out<br></code></pre></td></tr></table></figure>
<p>运行结束去，可以在yarn的web:<a href="http://bigdata-pro01.kfk.com:8088/cluster/">http://bigdata-pro01.kfk.com:8088/cluster/</a><br>看见调度success标志。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzol6uy6oaj30of09hjsa.jpg"><br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzom68klrwj315f0l23zq.jpg"></p>
<p>10、如果运行失败怎么办？看日志<br>有一个比较好的入口上图圈中的logs：<br>先配置yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log.server.url<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>http://bigdata-pro01.kfk.com:19888/jobhistory/logs<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br></code></pre></td></tr></table></figure>
<p>需要重启yarn，<br>并在你配置节点启动历史服务器./mr-jobhistory-daemon.sh start historyserver<br>点击：<a href="http://bigdata-pro01.kfk.com:8088/cluster">http://bigdata-pro01.kfk.com:8088/cluster</a><br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzom51hwo5j30pa0npaci.jpg"></p>
<h3 id="Spark-Streaming实时数据处理"><a href="#Spark-Streaming实时数据处理" class="headerlink" title="Spark Streaming实时数据处理"></a>Spark Streaming实时数据处理</h3><p><strong>Spark Streaming简介</strong></p>
<p>本质上就是利用批处理时间间隔来处理一小批的RDD集合。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzosp71irxj30g108hq75.jpg"></p>
<p><strong>idea中程序测试读取socket</strong></p>
<p>1、在节点1启动nc<br>nc -lk 9999<br>输入一些单词<br>2、在idea中运行程序</p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><code class="hljs reasonml">import org.apache.spark.SparkConf<br>import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;<br><span class="hljs-keyword">object</span> TestStreaming &#123;<br><br>  def main(args: Array<span class="hljs-literal">[S<span class="hljs-identifier">tring</span>]</span>): Unit = &#123;<br><br>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-constructor">SparkConf()</span>.set<span class="hljs-constructor">Master(<span class="hljs-string">&quot;local[2]&quot;</span>)</span>.set<span class="hljs-constructor">AppName(<span class="hljs-string">&quot;NetworkWordCount&quot;</span>)</span><br>    <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-constructor">StreamingContext(<span class="hljs-params">conf</span>, Seconds(5)</span>)<br><br>    <span class="hljs-keyword">val</span> lines = ssc.socket<span class="hljs-constructor">TextStream(<span class="hljs-string">&quot;bigdata-pro01.kfk.com&quot;</span>,9999)</span><br>    <span class="hljs-keyword">val</span> words = lines.flat<span class="hljs-constructor">Map(<span class="hljs-params">_</span>.<span class="hljs-params">split</span>(<span class="hljs-string">&quot; &quot;</span>)</span>)<br>    <span class="hljs-comment">//map reduce 计算</span><br>    <span class="hljs-keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="hljs-number">1</span>)).reduce<span class="hljs-constructor">ByKey(<span class="hljs-params">_</span> + <span class="hljs-params">_</span>)</span><br>    wordCounts.print<span class="hljs-literal">()</span><br>    ssc.start<span class="hljs-literal">()</span><br>    ssc.await<span class="hljs-constructor">Termination()</span><br><br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzot0y7ib8j30tl0a4mxy.jpg"></p>
<p><strong>sparkstreaming和kafka进行集成</strong></p>
<p>版本问题：<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzr9u0l3szj31cl048js0.jpg"><br>遇到了版本问题，之前用的是kafka0.9，现在和idea集成开发一般是kafka0.10了，还好官网里有支持kafka0.9程序案例，要不然就完犊子了，参考官网进行编写：<br><a href="http://spark.apache.org/docs/2.2.0/streaming-kafka-0-8-integration.html">http://spark.apache.org/docs/2.2.0/streaming-kafka-0-8-integration.html</a><br>代码案例：<a href="https://github.com/apache/spark/blob/v2.2.0/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala">https://github.com/apache/spark/blob/v2.2.0/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala</a><br>基于kafka0.9的测试程序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">import</span> kafka.serializer.<span class="hljs-type">StringDecoder</span><br><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span><br><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka.<span class="hljs-type">KafkaUtils</span><br><span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;<span class="hljs-type">Seconds</span>, <span class="hljs-type">StreamingContext</span>&#125;<br><br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">KfkStreaming</span> </span>&#123;<br>   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br><br>     <span class="hljs-keyword">val</span> spark  = <span class="hljs-type">SparkSession</span>.builder()<br>       .master(<span class="hljs-string">&quot;local[2]&quot;</span>)<br>       .appName(<span class="hljs-string">&quot;kfkstreaming&quot;</span>).getOrCreate()<br><br>     <span class="hljs-keyword">val</span> sc =spark.sparkContext<br>     <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sc, <span class="hljs-type">Seconds</span>(<span class="hljs-number">5</span>))<br><br>     <span class="hljs-keyword">val</span> topicsSet = <span class="hljs-type">Set</span>(<span class="hljs-string">&quot;weblogs&quot;</span>)<br>     <span class="hljs-keyword">val</span> kafkaParams = <span class="hljs-type">Map</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<span class="hljs-string">&quot;metadata.broker.list&quot;</span> -&gt; <span class="hljs-string">&quot;bigdata-pro01.kfk.com:9092&quot;</span>)<br>     <span class="hljs-keyword">val</span> messages = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>, <span class="hljs-type">StringDecoder</span>, <span class="hljs-type">StringDecoder</span>](<br>       ssc, kafkaParams, topicsSet)<br><br>     <span class="hljs-keyword">val</span> lines = messages.map(_._2)<br>     <span class="hljs-keyword">val</span> words = lines.flatMap(_.split(<span class="hljs-string">&quot; &quot;</span>))<br>     <span class="hljs-keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="hljs-number">1</span>L)).reduceByKey(_ + _)<br>     wordCounts.print()<br><br>     <span class="hljs-comment">// Start the computation</span><br>     ssc.start()<br>     ssc.awaitTermination()<br><br>   &#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure>
<p>在节点1上启动kafka程序</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">bin/kafka-server-start<span class="hljs-selector-class">.sh</span> config/server<span class="hljs-selector-class">.properties</span><br>bin/kafka-console-producer<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--broker-list</span> bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">9092</span> <span class="hljs-attr">--topic</span> weblogs<br><br></code></pre></td></tr></table></figure>
<p>运行结果：<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzr9tkp4szj30om07paas.jpg"></p>
]]></content>
      <tags>
        <tag>大数据项目</tag>
      </tags>
  </entry>
  <entry>
    <title>随笔</title>
    <url>/2022/03/11/%E9%9A%8F%E7%AC%94/</url>
    <content><![CDATA[<p> offer！offer！offer！！！</p>
<span id="more"></span>

<ul>
<li><p>Zookeeper</p>
</li>
<li><p>Kafka</p>
</li>
<li><p>Hadoop</p>
</li>
<li><p>HBase</p>
</li>
<li><p>Spark</p>
</li>
<li><p>MySQL</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka</title>
    <url>/2022/03/05/Kafka/</url>
    <content><![CDATA[]]></content>
      <tags>
        <tag>中间件</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark</title>
    <url>/2022/03/04/Spark/</url>
    <content><![CDATA[<blockquote>
<p>Spark大数据的计算分析引擎  官网 <a href="https://spark.apache.org/">https://spark.apache.org/</a></p>
</blockquote>
<ul>
<li>本文摘录于《Spark大数据处理技术》</li>
</ul>
<span id="more"></span>

<h4 id="Spark概述"><a href="#Spark概述" class="headerlink" title="Spark概述"></a><strong>Spark概述</strong></h4><blockquote>
<p>Apache Spark is a unified analytics engine for large-scale data processing</p>
</blockquote>
<p>按照现在流行的大数据处理场景划分，可以将大数据处理分为三种情况：</p>
<ul>
<li>复杂的批量数据处理，通常时间跨度为数十分钟到数小时</li>
<li>基于历史数据的交互式查询，通常时间跨度为数十秒到数分钟</li>
<li>基于实时数据流的数据处理，通常时间跨度为数百毫秒到数秒</li>
</ul>
<p>在Spark Core基础上衍生出能同时处理上面三种情形的统一大数据处理平台</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h02j0862b7j30ky04ijrl.jpg"></p>
<p>Spark要做的是将批处理，交互式处理，流式处理融合到一个软件栈中</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h02j2w0ltxj30mu092mxt.jpg"></p>
<h4 id="Spark-RDD及编程接口"><a href="#Spark-RDD及编程接口" class="headerlink" title="Spark RDD及编程接口"></a>Spark RDD及编程接口</h4><p><strong>HelloWorld</strong></p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h02j5gw5b9j30pg047t90.jpg"></p>
<ul>
<li>对于Spark程序来讲，所有的操作的前提是要有一个Spark上下文，创建上下文过程中，程序会向集群申请资源并构建运行环境。</li>
<li>通过sc变量并利用textFile接口从HDFS文件系统读取文件，返回file变量</li>
<li>对file进行过滤操作，生成新的变量filterRDD</li>
<li>对filterRDD进行cache操作(方便重用filterRDD)</li>
<li>对filterRDD进行计数，返回结果</li>
</ul>
<p>这个程序中涉及到众多概念：</p>
<ul>
<li>弹性式分布式数据集 RDD</li>
<li>创建操作(creation operation)：RDD初始创建是SparkContext负责，将内存集合或外部文件系统作为输入源</li>
<li>转换操作(transformation operation)：将一个RDD通过一定操作转变为另一个RDD</li>
<li>控制操作(control operation)：对RDD进行持久化，让RDD保存磁盘或内存中，方便重复使用</li>
<li>行动操作(action operation)：Spark是惰性计算的，RDD所有行动操作，都会触发Spark作业运行</li>
</ul>
<p>RDD和操作之间关系：经过输入操作，转换操作，控制操作，输出操作来完成一个作业</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h02jiomgszj30lh0ckdgp.jpg"></p>
<p><strong>Spark RDD</strong></p>
<p>RDD是弹性分布式数据集，即一个RDD代表一个被分区的只读数据集，RDD有两种生成途径：来自内存集合和外部存储系统；通过RDD的转换操作</p>
<p>RDD继承关系(lineage)构建可以通过记录作用在RDD上的转换操作，可以有效进行容错处理</p>
<p>一般情况下抽象的RDD需要包含这五个接口</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h02jtb7ww4j30ki05jt96.jpg"></p>
<p><strong>RDD分区(partitions)</strong></p>
<p>对于RDD来说，分区数涉及到RDD进行并行计算的粒度，每一个RDD分区的计算操作都在一个单独的任务中被执行。</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">100</span>,<span class="hljs-number">2</span>)<br><span class="hljs-comment">//1-100的数组转为RDD，第二个参数执行分区数</span><br>rdd<span class="hljs-selector-class">.partitions</span><span class="hljs-selector-class">.size</span><br><span class="hljs-comment">//查看RDD被划分的分区数</span><br></code></pre></td></tr></table></figure>

<p><strong>RDD优先位置(preferredLocations)</strong></p>
<p>RDD优先位置属性与Spark中的调度相关，返回的是RDD的每个partition存储的位置，“移动数据不如移动计算”所以Spark任务调度时候尽可能将任务分配到数据块存储位置</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">val rdd = sc<span class="hljs-selector-class">.textFile</span>(<span class="hljs-string">&quot;hdfs://10.0.2.19:8000/bigfile&quot;</span>)<br><span class="hljs-comment">//读取bigfile生成rdd</span><br>val hadoopRDD =rdd<span class="hljs-selector-class">.dependencies</span>(<span class="hljs-number">0</span>)<span class="hljs-selector-class">.rdd</span><br><span class="hljs-comment">//通过rdd依赖关系找到原始hadoopRDD</span><br>hadoopRDD<span class="hljs-selector-class">.partitions</span><span class="hljs-selector-class">.size</span><br>hadoopRDD<span class="hljs-selector-class">.preferredLocations</span>(hadoopRDD<span class="hljs-selector-class">.partition</span>(<span class="hljs-number">0</span>))<br><span class="hljs-comment">//返回partition(0)所在的机器位置</span><br></code></pre></td></tr></table></figure>

<p><strong>RDD依赖关系(dependencies)</strong></p>
<p>由于RDD是粗粒度操作数据集，每个转换操作都会生成一个新的RDD，所以RDD之间会形成类似流水线(pipline)的前后依赖关系，Spark中有两种类型依赖。</p>
<p>窄依赖(Narrow Dependencies)：每一个父RDD的分区最多只能被子RDD的一个分区使用</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h02kef8mpbj30hr0cmq3c.jpg"></p>
<p>宽依赖(Wide Dependencies)：多个子RDD的分区会依赖于同一个父RDD的分区</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h02kewzqs2j30ik07cweq.jpg"></p>
<p>Spark中明确区分宽窄依赖原因？</p>
<ol>
<li>窄依赖可以在集群的一个节点上如流水线一样执行，可以计算所有父RDD分区，相反宽依赖需要取得父RDD的所有分区上的数据进行计算，将执行类似MapReduce一样的Shuffle操作</li>
<li>对于窄依赖，节点计算失败后恢复会更有效，只需重新计算对应父RDD的分区，而且可以在其他节点上并行的计算，相反在宽依赖依赖关系中，一个节点失败会导致其父RDD的多个分区重新计算</li>
</ol>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">val</span> rdd =sc.make<span class="hljs-constructor">RDD(1 <span class="hljs-params">to</span> 10)</span><br><span class="hljs-keyword">val</span> mapRDD= rdd.map(x=&gt;(x,x))<br>mapRDD.dependencies<br><span class="hljs-keyword">val</span> shuffleRDD = mapRDD.partition<span class="hljs-constructor">By(<span class="hljs-params">new</span> <span class="hljs-params">org</span>.<span class="hljs-params">apache</span>.<span class="hljs-params">spark</span>.HashPartitioner(3)</span>)<br>shuffleRDD.dependencies<br></code></pre></td></tr></table></figure>

<p><strong>RDD分区计算(compute)</strong></p>
<p>RDD的计算都是以partition为单位的，而且RDD中的compute函数都是在对迭代器进行复合，不需要保存每次计算结果</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">10</span>,<span class="hljs-number">2</span>)<br>val map_rdd = rdd<span class="hljs-selector-class">.map</span>(a=&gt;a+<span class="hljs-number">1</span>)<br>val filter_rdd = map_rdd<span class="hljs-selector-class">.filter</span>(a=&gt;(a&gt;<span class="hljs-number">3</span>))<br>val context = new org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.TaskContext</span>(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>)<br>val iter0 = filter_rdd<span class="hljs-selector-class">.compute</span>(filter_rdd<span class="hljs-selector-class">.partition</span>(<span class="hljs-number">0</span>),context)<br>iter0<span class="hljs-selector-class">.toList</span><br>val iter1 = filter_rdd<span class="hljs-selector-class">.compute</span>(filter_rdd<span class="hljs-selector-class">.partition</span>(<span class="hljs-number">0</span>),context)<br>iter1<span class="hljs-selector-class">.toList</span><br><span class="hljs-comment">//在rdd上进行map和filter，由于compute函数只返回相应分区数据的迭代器，只有最后实例化才能显示出两个分区最终计算结果</span><br></code></pre></td></tr></table></figure>

<p><strong>RDD分区函数(partitioner)</strong></p>
<p>Spark目前有两种类型分区函数：HashPartitioner(哈希分区)和RangePartitioner(区域分区)，而且partitioner这个属性只存在于(K,V)类型的RDD中，对于非(K,V)类型的partitioner的值就是None，partitioner函数既决定了RDD本身的分区数量，也可作为其父RDD Shuffle输出(MapOutput)中每个分区进行数据切割的依据</p>
<p>使用HashPartitioner说明一下partitioner的功能</p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">val</span> rdd =sc.make<span class="hljs-constructor">RDD(1 <span class="hljs-params">to</span> 10,2)</span>.map(x=&gt;(x,x))<br>rdd.partitioner<br><span class="hljs-keyword">val</span> group_rdd = rdd.group<span class="hljs-constructor">ByKey(<span class="hljs-params">new</span> <span class="hljs-params">org</span>.<span class="hljs-params">apache</span>.<span class="hljs-params">spark</span>.HashPartitioner(3)</span>)<br><span class="hljs-comment">//new 了HashPartitioner对象</span><br>group_rdd.partitioner<br>group_rdd.collect<span class="hljs-constructor">Partitions()</span><br></code></pre></td></tr></table></figure>

<p>HashPartitioner的原理图：</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h02nqgxyjwj3084071mx8.jpg"></p>
<p><strong>集合创建操作</strong></p>
<p>RDD的创建可由内部集合类型来生成，Spark提供了parallelize和makeRDD两类函数实现从集合生成RDD，但makeRDD中提供了一个可以指定每一个分区preferredLocations参数的实现版本</p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">val</span> rdd = sc.make<span class="hljs-constructor">RDD(1 <span class="hljs-params">to</span> 10,3)</span><br>rdd.collect<span class="hljs-constructor">Partitions()</span><br><span class="hljs-keyword">val</span> collect = <span class="hljs-constructor">Seq(1 <span class="hljs-params">to</span> 10,Seq(<span class="hljs-string">&quot;host1&quot;</span>,<span class="hljs-string">&quot;host3&quot;</span>)</span>),(<span class="hljs-number">11</span> <span class="hljs-keyword">to</span> <span class="hljs-number">20</span>, <span class="hljs-constructor">Seq(<span class="hljs-string">&quot;host2&quot;</span>)</span>))<br><span class="hljs-keyword">val</span> rdd =sc.make<span class="hljs-constructor">RDD(<span class="hljs-params">collect</span>)</span><br>rdd.preferreed<span class="hljs-constructor">Locations(<span class="hljs-params">rdd</span>.<span class="hljs-params">partitions</span>(0)</span>)<br>rdd.preferred<span class="hljs-constructor">Locations(<span class="hljs-params">rdd</span>.<span class="hljs-params">partitions</span>(1)</span>)<br></code></pre></td></tr></table></figure>

<p><strong>存储创建操作</strong></p>
<p>主要是hadoopRDD和newHadoopRDD两个编程接口，包含了四个参数(输入格式；键类型；值类型；分区值)</p>
<p><strong>转换操作(transformation operation)</strong></p>
<table>
<thead>
<tr>
<th align="left">Transformation</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>map</strong>(<em>func</em>)</td>
<td align="left">将处理的数据逐条进行映射转换(类型或值)</td>
</tr>
<tr>
<td align="left"><strong>filter</strong>(<em>func</em>)</td>
<td align="left">将数据根据指定的规则进行筛选过滤</td>
</tr>
<tr>
<td align="left"><strong>flatMap</strong>(<em>func</em>)</td>
<td align="left">将处理的数据进行扁平化后再进行映射处理</td>
</tr>
<tr>
<td align="left"><strong>mapPartitions</strong>(<em>func</em>)</td>
<td align="left">将待处理的数据以分区为单位发送到计算节点进行处理</td>
</tr>
<tr>
<td align="left"><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td>
<td align="left">将待处理的数据以分区为单位发送到计算节点进行处理，在处理时同时可以获取当前分区索引。</td>
</tr>
<tr>
<td align="left"><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td>
<td align="left">根据指定的规则从数据集中抽取数据</td>
</tr>
<tr>
<td align="left"><strong>union</strong>(<em>otherDataset</em>)</td>
<td align="left">对源 RDD 和参数 RDD 求并集后返回一个新的 RDD</td>
</tr>
<tr>
<td align="left"><strong>intersection</strong>(<em>otherDataset</em>)</td>
<td align="left">对源 RDD 和参数 RDD 求交集后返回一个新的 RDD</td>
</tr>
<tr>
<td align="left"><strong>distinct</strong>([<em>numPartitions</em>]))</td>
<td align="left">将数据集中重复的数据去重</td>
</tr>
<tr>
<td align="left"><strong>groupByKey</strong>([<em>numPartitions</em>])</td>
<td align="left">将数据源的数据根据 key 对 value 进行分组</td>
</tr>
<tr>
<td align="left"><strong>reduceByKey</strong>(<em>func</em>, [<em>numPartitions</em>])</td>
<td align="left">可以将数据按照相同的 Key 对 Value 进行聚合</td>
</tr>
<tr>
<td align="left"><strong>aggregateByKey</strong>(<em>zeroValue</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numPartitions</em>])</td>
<td align="left">将数据根据不同的规则进行分区内计算和分区间计算</td>
</tr>
<tr>
<td align="left"><strong>sortByKey</strong>([<em>ascending</em>], [<em>numPartitions</em>])</td>
<td align="left">在一个(K,V)的 RDD 上调用，K 必须实现 Ordered 接口(特质)，返回一个按照 key 进行排序的RDD</td>
</tr>
<tr>
<td align="left"><strong>join</strong>(<em>otherDataset</em>, [<em>numPartitions</em>])</td>
<td align="left">在类型为(K,V)和(K,W)的 RDD 上调用，返回一个相同 key 对应的所有元素连接在一起的 (K,(V,W))的 RDD</td>
</tr>
<tr>
<td align="left"><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numPartitions</em>])</td>
<td align="left">在类型为(K,V)和(K,W)的 RDD 上调用，返回一个(K,(Iterable,Iterable))类型的 RDD</td>
</tr>
<tr>
<td align="left"><strong>coalesce</strong>(<em>numPartitions</em>)</td>
<td align="left">根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率 当 spark 程序中，存在过多的小任务的时候，可以通过 coalesce 方法，收缩合并分区，减少 分区的个数，减小任务调度成本</td>
</tr>
<tr>
<td align="left"><strong>repartition</strong>(<em>numPartitions</em>)</td>
<td align="left">该操作内部其实执行的是 coalesce 操作，参数 shuffle 的默认值为 true。无论是将分区数多的 RDD 转换为分区数少的 RDD，还是将分区数少的 RDD 转换为分区数多的 RDD，repartition 操作都可以完成，因为无论如何都会经 shuffle 过程。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
<p><font color="red">reduceByKey 和 groupByKey 的区别？</font></p>
<p>Shuffle角度：reduceByKey 和 groupByKey 都存在 shuffle 的操作，但是 reduceByKey 可以在 shuffle 前对分区内相同 key 的数据进行预聚合（combine）功能，这样会减少落盘的 数据量，而 groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能比较 高。</p>
<p> 功能的角度：reduceByKey 其实包含分组和聚合的功能。GroupByKey 只能分组，不能聚 合，所以在分组聚合的场合下，推荐使用 reduceByKey，如果仅仅是分组而不需要聚合。那 么还是只能使用 groupByKe</p>
<p><strong>控制操作(control operation)</strong></p>
<ul>
<li>cache():RDD[T]</li>
<li>persist():RDD[T]</li>
<li>persist(level:StorageLevel):RDD[T]</li>
</ul>
<p>Spark通过持久化操作将RDD持久化到不同层次的存储介质中，方便重复使用</p>
<ul>
<li>checkpoint</li>
</ul>
<p>checkpoint接口将RDD持久化到HDFS上，于persist(若也存在磁盘上)的区别是checkpoint会切断此RDD之前的依赖关系</p>
<p>checkpoint主要作用：Spark长时间运行，过长的依赖将会耗用大量系统资源，定期将RDD进行checkpoint操作，可有效地节省系统资源；过长的依赖关系会出现节点失败RDD容错重新计算的成本将会大大提升</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql">val rdd =sc.makeRDD(<span class="hljs-number">1</span> <span class="hljs-keyword">to</span> <span class="hljs-number">4</span>,<span class="hljs-number">1</span>)<br>val flatMapRDD= rdd.flatMap(x=&gt;Seq(x,x))<br>sc.setCheckpointDir(&quot;temp&quot;)<br>flatMapRDD.<span class="hljs-keyword">checkpoint</span>()<br>flatMapRDD.dependencies.head.rdd<br>flatMapRDD.collect()<br>flatMapRDD.dependencies.head.rdd<br></code></pre></td></tr></table></figure>



<p><strong>行动操作(action operation)</strong></p>
<p>Spark中触发action operation将会触发一次Spark调度并返回相应结果</p>
<table>
<thead>
<tr>
<th align="left">Action</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>reduce</strong>(<em>func</em>)</td>
<td align="left">聚集 RDD 中的所有元素，先聚合分区内数据，再聚合分区间数据</td>
</tr>
<tr>
<td align="left"><strong>collect</strong>()</td>
<td align="left">在驱动程序中，以数组 Array 的形式返回数据集的所有元素</td>
</tr>
<tr>
<td align="left"><strong>count</strong>()</td>
<td align="left">返回 RDD 中元素的个数</td>
</tr>
<tr>
<td align="left"><strong>first</strong>()</td>
<td align="left">返回 RDD 中的第一个元素</td>
</tr>
<tr>
<td align="left"><strong>take</strong>(<em>n</em>)</td>
<td align="left">返回一个由 RDD 的前 n 个元素组成的数组</td>
</tr>
<tr>
<td align="left"><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td>
<td align="left">返回该 RDD 排序后的前 n 个元素组成的数组</td>
</tr>
<tr>
<td align="left"><strong>saveAsTextFile</strong>(<em>path</em>)</td>
<td align="left">将数据保存到不同格式的文件中</td>
</tr>
<tr>
<td align="left"><strong>countByKey</strong>()</td>
<td align="left">统计每种 key 的个数</td>
</tr>
<tr>
<td align="left"><strong>foreach</strong>(<em>func</em>)</td>
<td align="left">分布式遍历 RDD 中的每一个元素，调用指定函数</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
<h4 id="Spark调度管理原理"><a href="#Spark调度管理原理" class="headerlink" title="Spark调度管理原理"></a>Spark调度管理原理</h4><p><strong>调度概述</strong></p>
<p>调度的前期是判断多个作业任务的依赖关系，这些作业任务之间可能存在因果的依赖关系，所以DAG有向无环图来表示这种关系</p>
<p>DAGScheduler负责将作业拆分为不同阶段的具有依赖关系的多批任务，简单理解为任务的逻辑调度</p>
<p>TaskScheduler负责每个具体任务的实际物理调度</p>
<blockquote>
<p>Task：单个分区数据集上最小处理流程单元</p>
<p>TaskSet：由一组关联的但相互间没有Shuffle依赖关系的任务所组成的任务集</p>
<p>Stage：一个任务集对应的调度阶段</p>
<p>Job：由一个RDD action生成的一个或多个调度阶段所组成的一次计算作业</p>
<p>Application：spark应用程序，由一到多个Job组成</p>
</blockquote>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h03z89yka2j30ds06zgm0.jpg"></p>
<p>在Spark运行时，这些计算操作是延迟执行的，并不是所有的RDD操作都会触发Spark向集群提交实际作业，只有需要返回数据或输出数据操作才会触发实际计算工作，其他变换操作知识生成对应的RDD关系链，用来记录依赖关系和所需执行的运算；另外DAGScheduler内部维度了各种“任务/调度阶段/作业”的状态和互相之间的映射关系表，用于在任务状态更新，集群状态更新等情况下，正确维护作业运行逻辑</p>
<p><strong>作业调度具体工作流程</strong></p>
<p>每个作业从提交到完成，要经历拆分成任务为最小单位，按一定逻辑依赖关系一次提交执行，并返回结果</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h03zhvod2yj30jb0a1gm9.jpg"></p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h03zi7zjjdj30i90bkgmn.jpg"></p>
<p><strong>调度任务拆分：</strong></p>
<p>当一个RDD操作触发计算，向DAGScheduler提交作业时，DAGScheduler需要从RDD依赖链末端的RDD触发，遍历整个RDD依赖链，划分调度阶段，并决定各个调度阶段间的依赖关系，调度阶段的划分是以ShuffleDependency为依据。</p>
<p><strong>调度阶段的提交：</strong></p>
<p>在划分调度阶段步骤中会得到一个或多个有依赖关系的调度阶段，其中直接触发作业的RDD关联的调度阶段叫做FinalStage，DAGScheduler进一步从FinalStage生成一个作业实例，两者关系存储在映射表，用于调度阶段全部完成时做一些处理如报告状态，清理作业相关数据等</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h03zoa22rzj30ne05eaau.jpg"></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h03znoilg5j30kd09k3zc.jpg"></p>
<p>当一个属于中间过程调度阶段的任务完成后，DAGScheduler会检查对应的调度阶段的所有任务是否都完成了，都完成了将再次扫描一次等待列表中的所有调度阶段的列表，检查是否还有任何依赖的调度阶段未完成，若没有，说明调度阶段处理就绪状态，可以再次尝试提交</p>
<p><strong>任务集的提交</strong></p>
<p>调度阶段的提交，最终会被转换成一个任务集的提交，DAGScheduler通过TaskScheduler接口提交任务集，这个任务集最终会触发TaskScheduler构建一个TaskSetManager的实例来管理这个任务集的生命周期，对于DAGScheduler来说提交调度阶段的工作到此完成，而TaskScheduler的具体实现会在得到资源时，通过TaskSetManager调度具体的任务到对应的Executor节点上进行运算</p>
<h4 id="Spark存储管理"><a href="#Spark存储管理" class="headerlink" title="Spark存储管理"></a>Spark存储管理</h4><p><strong>RDD持久化</strong></p>
<p>分区和数据块的关系：</p>
<p>在操作RDD时，这些操作都将施行在每一个分区上，可以说RDD上的运算都是基于分区的，但在存储管理模块，接触到的是数据块(block)，存储管理模块中对数据的存取都是以数据块为单位的，分区其实是一个逻辑上的概念，而数据块是物理上的数据实体</p>
<p>在Spark中，分区和数据块是一一对应的，存储模块只关心数据块，对于数据块和分区之间的映射是通过名称上的约定进行的，ID号+索引号作为块的名称就建立了分区和块的映射</p>
<p>在显示调用函数缓存RDD时，Spark内部就建立了RDD分区和数据块之间的映射，当读取缓存的RDD时，根据映射关系，就能从存储管理模块中获取到对应的数据块</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h03xgwdkm8j30hp06adg2.jpg"></p>
<p>内存缓存：</p>
<p>当Spark基于内存持久化缓存RDD时，RDD中每一个分区对应的数据块都是会被存储管理模块中的内存缓存(Memory Store所管理的)，内存缓存在内部维护了一个以数据块名称为Key，块内容为Value的哈希表</p>
<p><font color="red">当内存不是或达到设置的阈值时如何处理?</font></p>
<p>Spark通过spark.storage.memoryFration来配置(0.6)，JVM内存的60%可被内存缓存用来存储块内容，当占用&gt;60%，spark会丢一些数据块或将一些数据块存储到磁盘上来释放内存缓存空间，若丢掉的RDD 所依赖的父RDD是可被回溯并可用的，是不影响错误恢复机制的</p>
<p>磁盘缓存：</p>
<p>spark会将数据块放到磁盘目录下，通过spark.local.dir就配置了缓存在磁盘的目录</p>
<p>持久化选项：</p>
<p>当需要将RDD持久化，Spark可以调用persist()或cache()函数，对于RDD持久化，Spark提供了多种持久化选项</p>
<ul>
<li>MEMORY_ONLY：RDD以Java对象存储到到JVM内存heap中，超出内存缓存部分将不会缓存，下次需要重新计算</li>
<li>MEMORY_AND_DISK：和MEMORY_ONLY不同点是超出内存缓存部分将存在磁盘，需要将从磁盘读取</li>
<li>MEMORY_ONLY_SER：将序列化后的RDD存储到JVM中，占用空间更小，但读取时候需耗费更多CPU资源进行反序列化</li>
<li>MEMORY_AND_DISK_SER：和MEMORY_ONLY_SER不同是会把内存无法容下分区写入磁盘缓存</li>
<li>DISK_ONLY：将RDD的分区只缓存到磁盘中</li>
</ul>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h03y9juahsj30n80cnmz7.jpg"></p>
<p><strong>Shuffle数据持久化</strong></p>
<p>每一个Map任务会根据Reduce任务的数量创建出相应的桶，桶的数量是M*R。Map任务产生的结果会根据所设置的分区算法填充到每个桶中，当Reduce任务启动时，会根据自己任务ID和所依赖的Map任务的ID从远端或本地存储管理模块中取得相应的桶作为输入处理</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h03yah7hklj30ny092q3o.jpg"></p>
<p>和RDD持久化不同的是：首先Shuffle数据块必须在磁盘进行缓存，而不能选择在内存缓存，其次RDD基于磁盘持久化中，每个数据块对应一个文件，在Shuffle数据块持久化中，Shuffle数据块表示的只是逻辑概念，Shuffle有两种存储方式：</p>
<p>一种是将Shuffle数据块映射成文件，另一种是将Shuffle数据块映射成文件中的一段，这种方式要spark.shuffle.consolidateFiles设置为true，这种方式将分时运行的Map任务产生的Shuffle数据块合并到同一个文件中，来减少Shuffle文件的总数</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h03yisqxnfj30n109rgmk.jpg"></p>
<p><strong>广播(Broadcast)变量持久化</strong></p>
<p>为了加速一些对小块数据的读取，最好数据在所有节点都有一份拷贝，每个任务都能从本节点拷贝读取数据而不用通过远程传输获取数据，广播变量实现了这个功能，广播变量由存储管理模块进行管理的，另外广播变量数据块是以MEMORY_AND_DISK持久化存储本节点的存储管理模块，通过设置过期清洗机制，Spark内部会清理过期的广播变量</p>
<h4 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h4><p>用于快速构建可扩展，高吞吐，高容错的流处理程序</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h04s6ljwgfj30pl08pmxw.jpg"></p>
<p>DStream(离散数据流)代表了一个数据流，数据流可从外部输入源获得也可以通过输入流的转换获得，DStream是通过一组时间序列上连续的RDD来表示</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h04s9jnhcij30hv04zq32.jpg"></p>
<p><strong>DEMO</strong></p>
<p>获取指定端口上的数据并进行词频统计</p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">object</span> NetworkWordCount &#123;<br><br>  def main(args: Array<span class="hljs-literal">[S<span class="hljs-identifier">tring</span>]</span>) &#123;<br><br>    <span class="hljs-comment">/*指定时间间隔为 5s*/</span><br>    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-constructor">SparkConf()</span>.set<span class="hljs-constructor">AppName(<span class="hljs-string">&quot;NetworkWordCount&quot;</span>)</span>.set<span class="hljs-constructor">Master(<span class="hljs-string">&quot;local[2]&quot;</span>)</span><br>    <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-constructor">StreamingContext(<span class="hljs-params">sparkConf</span>, Seconds(5)</span>)<br><br>    <span class="hljs-comment">/*创建文本输入流,并进行词频统计*/</span><br>    <span class="hljs-keyword">val</span> lines = ssc.socket<span class="hljs-constructor">TextStream(<span class="hljs-string">&quot;hadoop001&quot;</span>, 9999)</span><br>    lines.flat<span class="hljs-constructor">Map(<span class="hljs-params">_</span>.<span class="hljs-params">split</span>(<span class="hljs-string">&quot; &quot;</span>)</span>).map(x =&gt; (x, <span class="hljs-number">1</span>)).reduce<span class="hljs-constructor">ByKey(<span class="hljs-params">_</span> + <span class="hljs-params">_</span>)</span>.print<span class="hljs-literal">()</span><br><br>    <span class="hljs-comment">/*启动服务*/</span><br>    ssc.start<span class="hljs-literal">()</span><br>    <span class="hljs-comment">/*等待服务结束*/</span><br>    ssc.await<span class="hljs-constructor">Termination()</span><br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p><code>nc -lk 9999</code>打开端口写入测试数据</p>
<p>Spark Streaming 编程的入口类是 StreamingContext，在创建时候需要指明 <code>sparkConf</code> 和 <code>batchDuration</code>(批次时间)，Spark 流处理本质是将流数据拆分为一个个批次，然后进行微批处理，<code>batchDuration</code> 就是批次拆分的时间间隔。这个时间可以根据业务需求和服务器性能进行指定，如果业务要求低延迟并且服务器性能也允许，则这个时间可以指定得很短。</p>
<p>DStream 是 Spark Streaming 提供的基本抽象。它表示连续的数据流。在内部，DStream 由一系列连续的 RDD 表示。所以从本质上而言，应用于 DStream 的任何操作都会转换为底层 RDD 上的操作。</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h04sd4t22lj30pe083mxw.jpg"></p>
<h4 id="Spark调优"><a href="#Spark调优" class="headerlink" title="Spark调优"></a>Spark调优</h4><p><strong>RDD调优</strong></p>
<ul>
<li>RDD复用</li>
<li>尽早进行filter</li>
<li>读取大量小文件用wholeTextFiles</li>
<li>mapPartition和foreachPartition替代map和foreach操作</li>
<li>filter+coalesce/repartition(减少分区)</li>
<li>设置合理的并行度</li>
<li>repartition/coalesce调节并行度</li>
<li>reduceByKey本地预聚合</li>
<li>使用持久化+checkpoint</li>
<li>使用广播变量使用Kryo序列化</li>
</ul>
<p><strong>Shuffle调优</strong></p>
<ul>
<li>map和reduce端缓冲区大小</li>
<li>reduce端重试次数和等待时间间隔</li>
<li>bypass机制开启阈值</li>
</ul>
]]></content>
      <tags>
        <tag>数据 Spark</tag>
      </tags>
  </entry>
</search>

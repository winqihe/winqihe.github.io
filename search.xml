<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HBase</title>
    <url>/2022/03/05/HBase/</url>
    <content><![CDATA[<h4 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h4><ul>
<li><p>HBase如何写数据</p>
<span id="more"></span>

<p>Client写入，存入MemStore一直到MemStore满，Flush成一个StoreFile增长到一定阈值，出发Compact合并，多个StoreFile合并成一个StoreFile同时进行版本合并和数据删除，当StoreFiles Compact后形成越来与大的StoreFile，单个StoreFile大小超过一定阈值(10G)触发split，把当前Region Split成两个Region，Region会下线，新Split除两个Region会被HMaster分配到相应HRegionServer，Region得以分压</p>
</li>
<li><p>HDFS和HBase使用场景</p>
<p>首先Hbase基于HDFS存储的</p>
<p>HDFS：一次写入多次读取，数据一致性，容错和恢复机制</p>
<p>HBase：瞬间写入量很大，数据需要长久保存，不适用于有join和多级索引及表关系复杂数据模型，大数据量且有快速随机访问的需求，业务场景简单</p>
</li>
<li><p>HBase存储结构</p>
<p>HBase表通过rowkey按照一定范围被分割成多个子表HRegion(一个HRegion超过256要分为2个)，由HRegionServer管理，HMaster管理HRegion</p>
<p>HRegion存取一个子表时会创建一个HRegion对象，对表的每个列簇创建一个store实例，每个store都有0或多个StoreFile对应，每个StoreFile对应一个HFile</p>
</li>
<li><p>HBase RowKey设计原则</p>
<p>长度原则：100 字节以内，8 的倍数最好，可能的情况下越短越好。</p>
<p>散列原则：高位散列，低位时间字段。避免热点问题。</p>
<p>唯一原则：分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一起</p>
</li>
<li><p>HBase列簇设计</p>
<p>原则：在合理范围内能尽量少的减少列簇就尽量减少列簇，因为列簇是共享 region 的，每个列簇数据相差太大导致查询效率低下</p>
<p>最优：将所有相关性很强的 key-value 都放在同一个列簇下，这样既能做到查 询效率最高，也能保持尽可能少的访问不同的磁盘文件。</p>
</li>
<li><p>热点现象怎么解决</p>
<p>某个小的时段内，对 HBase 的读写请求集中到极少数的 Region 上，导致这些 region 所在的 RegionServer 处理请求量骤增，负载量明显偏大，而其他的 RgionServer 明显空闲</p>
<p>热点发生在大量的 client 直接访问集群的一个或极少数个节点</p>
<ol>
<li>加盐：在 rowkey 的前面增加随机数，使得它和之前的 rowkey 的开头不同。加盐之后的 rowkey 就会根据随机生成的前缀分散到各个 region 上， 以避免热点</li>
<li>哈希：哈希可以使负载分散到整个集群，但是读却是可以预测的</li>
<li>反转：反转固定长度或者数字格式的 rowkey，可以使得 rowkey 中经常改变的部分（最没有意义的部分）放在前面。</li>
<li>时间戳反转</li>
<li>HBase建表预分区：创建 HBase 表时，就预先根据可能的 RowKey 划分出多 个 region 而不是默认的一个，从而可以将后续的读写操作负载均衡到不 同的 region 上，避免热点现象。</li>
</ol>
</li>
<li><p>HBase compact（minor major）</p>
<p>在 hbase 中每当有 memstore 数据 flush 到磁盘之后，就形成一个 storefile， 当 storeFile 的数量达到一定程度就需要将 storefile 文件来进行 compaction</p>
<ol>
<li><p>合并文件</p>
</li>
<li><p>清洗过期多余版本数据</p>
</li>
<li><p>提高读写数据的效率</p>
</li>
<li><p>minor用来部分文件的合并以及包括minVersion=0并且设置ttl的过期版本清理，不做删除数据，多版本数据清理</p>
</li>
<li><p>major对Region下的HStore下所有StoreFile执行合并，最终合并出一个文件</p>
</li>
</ol>
</li>
</ul>
]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop</title>
    <url>/2022/03/03/Hadoop/</url>
    <content><![CDATA[<h4 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h4><ul>
<li>HDFS优缺点：</li>
</ul>
<span id="more"></span>

<p>优点：高容错性，适合大规模数据处理，廉价成本</p>
<p>缺点：不适合低延时数据访问，无法高效对大量小文件存储，不支持并发写入，文件随即修改。</p>
<ul>
<li>HDFS组成架构</li>
</ul>
<p>NameNode:管理HDFS名称空间，配置副本策略，管理数据块，处理客户端请求</p>
<p>DataNode:存储实际数据块，执行数据块的读写操作，</p>
<p>Client:文件切分，与NameNode交互获取文件位置信息，与DataNode交互，读写数据</p>
<p>SecondaryNameNode:辅助NameNode为其分担工作量，辅助恢复NameNode</p>
<ul>
<li>Hadoop文件块大小默认为128M</li>
</ul>
<p>文件块太小，会增加寻址时间</p>
<p>文件块太大，从磁盘传输数据的时间将大于定位数据块起始位置所需的时间，导致程序处理数据慢</p>
<ul>
<li>HDFS写数据</li>
</ul>
<ol>
<li>Client向NameNode请求上传文件</li>
<li>NameNode返回是否可以上传</li>
<li>Client请求第一个Block上传到那个DataNode</li>
<li>NameNode返回3个DataNode节点</li>
<li>Client请求datanode1上传数据，datanode1收到请求会继续调用datanode2，datanode2调用datanode3，将通信管道建立完</li>
<li>datanode1，2，3逐级应答Client</li>
<li>Client向datanode1上传第一个Block(Packet)，先从磁盘读数据放到本地进行缓存，datanode1收到一个Packet就会传给datanode2，datanode2传给datanode3，</li>
<li>当一个Block传输完，Client再次请求上传第二个Block</li>
</ol>
<ul>
<li>HDFS读数据</li>
</ul>
<ol>
<li>Client向NameNode请求下载文件，NameNode查询元数据，定位文件块所在DataNode地址</li>
<li>从(先就近后随机)DataNode请求读取数据</li>
<li>DataNode传输数据给Client(Packet)</li>
<li>Client(Packet)接受，先本地缓存后写入目标文件</li>
</ol>
<ul>
<li>NameNode和SecondaryNameNode工作机制</li>
</ul>
<p>NameNode启动：</p>
<ol>
<li>首次启动格式化，创建Fsimage，Edits，之后启动load edits和fsimage到内存</li>
<li>Client对元数据进行增删改请求</li>
<li>NameNode记录操作日志</li>
<li>NameNode在内存中对数据增删改</li>
</ol>
<p>SecondaryNode工作：</p>
<ol>
<li>2NN询问NN是否要CheckPoint</li>
<li>2NN请求执行CheckPoint</li>
<li>NN滚动正在写的Edits，将滚动前的edits和fsimage拷贝到2NN</li>
<li>2NN加载edits和fsimage到内存，并进行合并，生成新的fsimage.checkpoint</li>
<li>拷贝fsimage.checkpoint到NN，NN将fsimage.checkpoint更名为fsimage</li>
</ol>
<ul>
<li>DataNode工作机制</li>
</ul>
<ol>
<li>一个数据块在DataNode上以文件形式存储在磁盘，分别是数据本身和元数据(数据块长度，数据块校验和，时间戳)</li>
<li>DataNode启动后向NameNode注册，通过后周期性(1h)向NameNode上报所有块信息</li>
<li>心跳3s/次，心跳返回结果带有NameNode给DataNode的命令，若超过10min没有收到某个DataNode的心跳，则认为节点已挂</li>
<li>集群运行中可以服役新节点或退役旧节点</li>
</ol>
<ul>
<li>DataNode保证数据完整性</li>
</ul>
<ol>
<li>当DataNode读取Block，会计算CheckSum</li>
<li>若计算后的CheckSum与Block创建时值不同，说明Block已损</li>
<li>Client读取其他DataNode上的Block，DataNode在其文件创建后周期验证CheckSum</li>
</ol>
<ul>
<li>HDFS HA高可用</li>
</ul>
<p>HA即高可用(7*24不间断服务)</p>
<p>实现高可用最关键的策略是消除单点故障，通过配置Active/Standby两个NameNodes实现集群中对NameNode的热备</p>
<h4 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h4><ul>
<li>MapReduce优缺点</li>
</ul>
<p>优点：易于编程，良好扩展性，高容错性，适合PB级以上海量数据离线处理</p>
<p>缺点：不擅长实时计算，不擅长流式计算，不擅长DAG计算</p>
<ul>
<li>MapReduce核心思想</li>
</ul>
<ol>
<li>分布式的计算通常要分为至少两个阶段</li>
<li>第一阶段的MapTask并发实例，完全并行运行</li>
<li>第二阶段的ReduceTask并发实例互不相干，但数据依赖于上个阶段所有MapTask并发实例的输出</li>
<li>MapReduce编程模型只能包含一个Map和Reduce阶段</li>
</ol>
<ul>
<li>Shuffle机制</li>
</ul>
<ol>
<li>Collect阶段：MapTask的数据输出到环形缓冲区</li>
<li>Spill阶段：当内存中数据量到一定阈值，会将数据写入本地磁盘(将数据写入磁盘前需要对数据进行一次排序)，如果配置combiner会将相同分区号和key的数据进行排序</li>
<li>MapTask阶段的Merge：把所有溢出的临时文件进行一次merge，确保一个MapTask最终只产生一个中间数据文件</li>
<li>Copy阶段：ReduceTask启动Fetcher线程到已完成MapTask节点上copy一份数据</li>
<li>ReduceTask阶段的Merge：在ReduceTask复制数据同时，会在后台开启两个线程对内存到本地的数据文件进行merge</li>
<li>Sort阶段：对数据进行merge同时，会进行排序，由于MapTask阶段已经对数据进行了局部排序，ReduceTask只需保证Copy数据的最终整体有效性</li>
</ol>
]]></content>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark</title>
    <url>/2022/03/04/Spark/</url>
    <content><![CDATA[<blockquote>
<p>Spark大数据的计算分析引擎</p>
</blockquote>
]]></content>
      <tags>
        <tag>数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka</title>
    <url>/2022/03/05/Kafka/</url>
    <content><![CDATA[]]></content>
      <tags>
        <tag>中间件</tag>
      </tags>
  </entry>
</search>

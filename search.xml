<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hadoop</title>
    <url>/2022/03/03/Hadoop/</url>
    <content><![CDATA[<h4 id="HDFS概述"><a href="#HDFS概述" class="headerlink" title="HDFS概述"></a>HDFS概述</h4><p><strong>HDFS优缺点</strong></p>
<p>优点：高容错性，适合大规模数据处理，廉价成本</p>
<p>缺点：不适合低延时数据访问，无法高效对大量小文件存储，不支持并发写入，文件随即修改。</p>
<p><strong>HDFS组成架构</strong></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h011gaq5xej30te0lu76b.jpg"></p>
<p>NameNode:管理HDFS名称空间，配置副本策略，管理数据块，处理客户端请求</p>
<p>DataNode:存储实际数据块，执行数据块的读写操作，</p>
<p>Client:文件切分，与NameNode交互获取文件位置信息，与DataNode交互，读写数据</p>
<p>SecondaryNameNode:辅助NameNode为其分担工作量(定期合并FSImage和Edit Log)，辅助恢复NameNode</p>
<p><strong>元数据格式</strong></p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h0128e6g95j30nr0amjss.jpg"></p>
<p><strong>Hadoop文件块大小</strong>默认为128M，文件块太小，会增加寻址时间；文件块太大，从磁盘传输数据的时间将大于定位数据块起始位置所需的时间，导致程序处理数据慢</p>
<p><strong>机架感知(副本节点选择)</strong></p>
<ol>
<li>第一个副本在client所处的节点上。如果客户端在集群外，随机选一个</li>
<li>第二个副本和第一个副本位于相同机架，随机节点。</li>
<li>第三个副本位于不同机架，随机节点</li>
</ol>
<p><strong>常用命令</strong></p>
<blockquote>
<p>-moveFromLocal,-copyFromLocal,-put,-appendToFile,-copyToLocal,-get,-ls,-cat,-cp,-mkdir,-mv,-tail,-rm,-rm -r,-du,-setrep</p>
</blockquote>
<h4 id="HDFS读写流程"><a href="#HDFS读写流程" class="headerlink" title="HDFS读写流程"></a>HDFS读写流程</h4><p><strong>HDFS写数据</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h011xz364sj30gz0b1dgm.jpg"></p>
<ol>
<li>Client向NameNode请求上传文件</li>
<li>NameNode返回是否可以上传</li>
<li>Client请求第一个Block上传到那个DataNode</li>
<li>NameNode返回3个DataNode节点</li>
<li>Client请求datanode1上传数据，datanode1收到请求会继续调用datanode2，datanode2调用datanode3，将通信管道建立完</li>
<li>datanode1，2，3逐级应答Client</li>
<li>Client向datanode1上传第一个Block(Packet)，先从磁盘读数据放到本地进行缓存，datanode1收到一个Packet就会传给datanode2，datanode2传给datanode3，</li>
<li>当一个Block传输完，Client再次请求上传第二个Block</li>
</ol>
<p><strong>HDFS读数据</strong></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h011x3wtgbj30ku0c5q3y.jpg"></p>
<ol>
<li>Client向NameNode请求下载文件，NameNode查询元数据，定位文件块所在DataNode地址</li>
<li>从(先就近后随机)DataNode请求读取数据</li>
<li>DataNode传输数据给Client(Packet)</li>
<li>Client(Packet)接受，先本地缓存后写入目标文件</li>
</ol>
<h4 id="NN和2NN工作机制"><a href="#NN和2NN工作机制" class="headerlink" title="NN和2NN工作机制"></a>NN和2NN工作机制</h4><p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h0143iqerxj30qw0d8tby.jpg"></p>
<p>NameNode启动：</p>
<ol>
<li>首次启动格式化，创建Fsimage，Edits，之后启动load edits和fsimage到内存</li>
<li>Client对元数据进行增删改请求</li>
<li>NameNode记录操作日志</li>
<li>NameNode在内存中对数据增删改</li>
</ol>
<p>SecondaryNode工作：</p>
<ol>
<li>2NN询问NN是否要CheckPoint</li>
<li>2NN请求执行CheckPoint</li>
<li>NN滚动正在写的Edits，将滚动前的edits和fsimage拷贝到2NN</li>
<li>2NN加载edits和fsimage到内存，并进行合并，生成新的fsimage.checkpoint</li>
<li>拷贝fsimage.checkpoint到NN，NN将fsimage.checkpoint更名为fsimage</li>
</ol>
<p><strong>FSImage和Edit Log</strong></p>
<p>fsimage保存了最新的元数据检查点，包含了整个HDFS文件系统的所有目录和文件的信息。</p>
<p>editlog在NameNode已经启动情况下对HDFS进行的各种更新操作进行记录，HDFS客户端执行所有的写操作都会被记录到editlog中</p>
<p>namenode在被格式化后会在/data/tmp/dfs/name/current目录下产生相应文件：</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h0148rc90jj30ba02g3yg.jpg"></p>
<h4 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h4><ol>
<li>一个数据块在DataNode上以文件形式存储在磁盘，分别是数据本身和元数据(数据块长度，数据块校验和，时间戳)</li>
</ol>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h013jtur26j30cf022aa0.jpg"></p>
<ol>
<li>DataNode启动后向NameNode注册，通过后周期性(1h)向NameNode上报所有块信息</li>
<li>心跳3s/次，心跳返回结果带有NameNode给DataNode的命令，若超过10min没有收到某个DataNode的心跳，则认为节点已挂</li>
<li>集群运行中可以服役新节点或退役旧节点</li>
</ol>
<p><strong>Client获取块数据</strong></p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h013kyka07j31f70i0gnj.jpg"></p>
<pre><code> 1. 客户端向namenode发送查询请求
 2. namenode返回数据块所在的节点datanode
 3. 客户端在返回的节点中寻找相应的块数据
 4. 如果某个datanode挂点了，返回相应的副本中寻找
</code></pre>
<p><strong>HDFS保证数据完整性</strong></p>
<p>客户端向HDFS写数据：</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h013s4e4vqj31f70i0gne.jpg"></p>
<ol>
<li>假设客户端发送2KB的数据 </li>
<li>客户端会以字节的方式往datanode发送，所以客户端会计算发送的数据有多少个，而这个单位就是chunk（512字节）。</li>
<li>客户端可以计算出checksum值，checksum = 2KB/512B=4 </li>
<li>然后datanode接收客户端发送来的数据，每接收512B的数据，就让checksum的值+1 </li>
<li>最后比较客户端和datanade的checksum值</li>
</ol>
<p>DataNode读取Block块：</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h013wr7aaoj31f70i0di4.jpg"></p>
<ol>
<li>block创建时会初始checksum值 </li>
<li>DataNode每隔一段时间就会计算block新的checksum值，看block块是否已经丢失 </li>
<li>如果checksum和之前一样，则没丢失，和之前比出现了不一样，那就说明数据丢失（或者异常） </li>
<li>当发生异常的时候，DateNode会报告给NameNode，NameNode会发送一条命令，清除这个异常块，然后找到这个块对应的副本，将完整的副本复制给其他的DataNode节点</li>
</ol>
<h4 id="HA高可用"><a href="#HA高可用" class="headerlink" title="HA高可用"></a>HA高可用</h4><p>HA即高可用(7*24不间断服务)</p>
<p>实现高可用最关键的策略是消除单点故障，通过配置Active/Standby两个NameNodes实现集群中对NameNode的热备</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h012120smaj30hq0bfmy7.jpg"></p>
<p>Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode,只有主 NameNode 才能对外提供读写服务</p>
<p>ZKFailoverController（主备切换控制器，FC）：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换</p>
<p>Zookeeper 集群：为主备切换控制器提供主备选举支持；</p>
<p>共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。</p>
<p>DataNode 节点：主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。</p>
<h4 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h4><ul>
<li>MapReduce优缺点</li>
</ul>
<p>优点：易于编程，良好扩展性，高容错性，适合PB级以上海量数据离线处理</p>
<p>缺点：不擅长实时计算，不擅长流式计算，不擅长DAG计算</p>
<ul>
<li>MapReduce核心思想</li>
</ul>
<ol>
<li>分布式的计算通常要分为至少两个阶段</li>
<li>第一阶段的MapTask并发实例，完全并行运行</li>
<li>第二阶段的ReduceTask并发实例互不相干，但数据依赖于上个阶段所有MapTask并发实例的输出</li>
<li>MapReduce编程模型只能包含一个Map和Reduce阶段</li>
</ol>
<ul>
<li>Shuffle机制</li>
</ul>
<ol>
<li>Collect阶段：MapTask的数据输出到环形缓冲区</li>
<li>Spill阶段：当内存中数据量到一定阈值，会将数据写入本地磁盘(将数据写入磁盘前需要对数据进行一次排序)，如果配置combiner会将相同分区号和key的数据进行排序</li>
<li>MapTask阶段的Merge：把所有溢出的临时文件进行一次merge，确保一个MapTask最终只产生一个中间数据文件</li>
<li>Copy阶段：ReduceTask启动Fetcher线程到已完成MapTask节点上copy一份数据</li>
<li>ReduceTask阶段的Merge：在ReduceTask复制数据同时，会在后台开启两个线程对内存到本地的数据文件进行merge</li>
<li>Sort阶段：对数据进行merge同时，会进行排序，由于MapTask阶段已经对数据进行了局部排序，ReduceTask只需保证Copy数据的最终整体有效性</li>
</ol>
]]></content>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka</title>
    <url>/2022/03/05/Kafka/</url>
    <content><![CDATA[]]></content>
      <tags>
        <tag>中间件</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark</title>
    <url>/2022/03/04/Spark/</url>
    <content><![CDATA[<blockquote>
<p>Spark大数据的计算分析引擎</p>
</blockquote>
]]></content>
      <tags>
        <tag>数据</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase</title>
    <url>/2022/03/05/HBase/</url>
    <content><![CDATA[<h2 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h2><p>本文内容摘录于《HBase原理与实践》</p>
<span id="more"></span>

<h4 id="概述一览"><a href="#概述一览" class="headerlink" title="概述一览"></a>概述一览</h4><p><strong>基本概念：</strong></p>
<p>table：表，一个表包含多行数据。</p>
<p>row：行，一行数据包含一个唯一标识rowkey、多个column以及对应的值。在HBase中，一张表中所有row都按照rowkey的字典序由小到大排序。</p>
<p>column：列，HBase中的column由column family（列簇）以及qualif ier（列名）两部分组成，两者中间使用”:”相连。column family在表创建的时候需要指定，用户不能随意增减。一个column family下可以设置任意多个qualif ier，因此可以理解为HBase中的列可以动态增加，理论上甚至可以扩展到上百万列。</p>
<p>timestamp：时间戳，每个cell在写入HBase的时候都会默认分配一个时间戳作为该cell的版本，当然，用户也可以在写入的时候自带时间戳。HBase支持多版本特性，即同一rowkey、column下可以有多个value存在，这些value使用timestamp作为版本号，版本越大，表示数据越新。</p>
<p>cell：单元格，由五元组（row, column, timestamp, type, value）组成的结构，其中type表示Put/Delete这样的操作类型，timestamp代表这个cell的版本。这个结构在数据库中实际是以KV结构存储的，其中（row, column, timestamp, type）是K，value字段对应KV结构的V。</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/078bfb0511e176a0.png"></p>
<p>​                                                                            <center>（HBase逻辑视图）</center></p>
<p><strong>体系架构：</strong></p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/0576dba2ceb0e8a3.png"></p>
<p>​                                                                                <center>（体系架构）</center>                                            </p>
<ol>
<li><p>Client：丰富的编程接口，Client访问数据行前要先通过元数据表定位目标数据所在RegionServer，发请求给RegionServer，这些元数据也会被缓存到本地。</p>
</li>
<li><p>Zookeeper：实现Master高可用(异常宕机进行选举)，管理系统核心元数据(保存元数据表所在的RegionServer地址)，参与RegionServer宕机恢复(心跳机制)，实现分布式表锁</p>
</li>
<li><p>Master：处理各种管理请求(建表、修改表、权限操作、切分表、合并数据分片以及Compaction)，管理集群中的RegionServer(RegionServer中Region的负载均衡、RegionServer的宕机恢复以及Region的迁移)，清理过期日志以及文件，Master会每隔一段时间检查HDFS中HLog是否过期、HFile是否已经被删除，并在过期之后将其删除</p>
</li>
<li><p>RegionServer：响应用户的IO请求，由WAL(HLog),BlockCache,多个Region构成</p>
<p>WAL(HLog):实现数据高可靠性，数据写入缓存前先顺序写入HLog，实现HBase集群间主从复制通过回放主集群推送过来的HLog日志实现主从复制。</p>
<p>BlockCache:HBase系统中的读缓存，客户端从磁盘读取数据之后通常会将数据缓存到系统内存中，后续访问同一行数据可以直接从内存中获取而不需要访问磁盘。</p>
<p>Region:数据表的一个分片，当数据表大小超过阈值就会水平切分，分裂为两个Region，一个Region由一个或多个Store构成，Store个数取决于列簇个数，每个列簇的数据存放在一起形成一个存储单元Store，每个Store由一个MemStore和一或多个HFile组成，写入数据先写入MemStore，到一定阈值(128M)将Flush成HFile，随着HFile数越来越多，将会执行Compact合并成大文件</p>
</li>
<li><p>HDFS：HBase底层依赖HDFS存储数据，用户数据文件、HLog日志文件等最终都会写入HDFS落盘</p>
</li>
</ol>
<p><strong>HBase特性：</strong></p>
<p>优点：容量大，良好可扩展性，稀疏性，高性能，多版本，支持过期，Hadoop原生支持</p>
<p>缺点：不支持复杂聚合运算，没有二级索引功能，不支持全局跨行事务</p>
<h4 id="依赖服务"><a href="#依赖服务" class="headerlink" title="依赖服务"></a>依赖服务</h4><p><strong>ZooKeeper</strong>：一个分布式HBase集群的部署运行强烈依赖于ZooKeeper</p>
<p><strong>HDFS</strong>：HBase的文件都存放在HDFS（Hadoop Distribuited File System）文件系统上</p>
<h4 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h4><p>通过ThriftServer的Java HBase客户端来请求HBase集群。</p>
<p>定位Meta表：</p>
<p>客户端在做任何数据操作时，都要先确定数据在哪个Region上，然后再根据Region的RegionServer信息，去对应的RegionServer上读取数据，HBase系统内部设计了一张特殊的表——hbase:meta表，专门用来存放整个集群所有的Region信息。hbase:meta中的hbase指的是namespace，HBase容许针对不同的业务设计不同的namespace，系统表采用统一的namespace，即hbase；meta指的是hbase这个namespace下的表名。</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h00gqa64zqj30r507vdgm.jpg"></p>
<p>​                                                                    <center>Hbase:meta结构</center></p>
<p><font color="red"> HBase作为一个分布式数据库系统，一个大的集群可能承担数千万的查询写入请求，而hbase:meta表只有一个Region，如果所有的流量都先请求hbase:meta表找到Region，再请求Region所在的RegionServer，那么hbase:meta表的将承载巨大的压力，这个Region将马上成为热点Region，且根本无法承担数千万的流量。那么，如何解决这个问题呢</font></p>
<p>把hbase:meta表的Region信息缓存在HBase客户端</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h00gu71x0zj30q309ldgz.jpg"></p>
<p>​                                                                                <center>客户端定义Region</center></p>
<p>少量写和批量写：</p>
<p>HBase提供了3种常见的数据写入API：</p>
<p>table.put(put)：在服务端先写WAL，然后写MemStore，一旦MemStore写满就f lush到磁盘上。特点是，默认每次写入都需要执行一次RPC和磁盘持久化。因此，写入吞吐量受限于磁盘带宽、网络带宽以及f lush的速度。但它能保证每次写入操作都持久化到磁盘，不会有任何数据丢失。最重要的是，它能保证put操作的原子性。</p>
<p>table.put(List<Put> puts)：在客户端缓存put，等凑足了一批put，就将这些数据打包成一次RPC发送到服务端，一次性写WAL，并写MemStore。相比第一种方式省去了多次往返RPC以及多次刷盘的开销，吞吐量大大提升。不过，这个RPC操作耗时一般都会长一点，因为一次写入了多行数据。这些put中，可能有一部分失败，失败的那些put操作会经历若干次重试</Put></p>
<p>bulk load：直接将待写入数据生成HFile，将这些HFile直接加载到对应的Region下的CF内。在生成HFile时，在HBase服务端没有任何RPC调用，只在load HFile时会调用RPC，一种完全离线的快速写入方式。bulk load应该是最快的批量写手段，同时不会对线上的集群产生巨大压力。在load完HFile之后，CF内部会进行Compaction，但是Compaction是异步的且可以限速，所以产生的IO压力是可控的。bulk load对线上集群非常友好。</p>
<h4 id="RegionServer结构"><a href="#RegionServer结构" class="headerlink" title="RegionServer结构"></a>RegionServer结构</h4><p>一个RegionServer由一或多个HLog，BlockCache及多个Region组成，RegionServer负责Region的数据读写，一个Region由多个store组成，Store存放对应列簇数据，Store包含一个MemStore和多个HFile。</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/8343fa265a93d486.png"></p>
<p>​                                                                                <center>（RegionServer结构）</center></p>
<p><strong>HLog</strong>：用来保证数据写入的可靠性</p>
<p>HBase中系统故障恢复及主从复制都基于HLog实现，数据以追加的形式先写入HLog，再写入MemStore，如果RegionServer宕机，此时写入MemStore中但尚未Flush磁盘数据丢失，需要回访HLog补救丢失数据，HBase主从复制需要主集群将HLog日志发送给从集群，完成集群间复制</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/fb81be8e4743af98.png"></p>
<p>​                                                                               <center> （HLog文件结构）</center></p>
<p>HLog文件生成后不会永久存储在系统中，使命完成后，文件会失效最终删除</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/eebdaf515bf189bf.png"></p>
<p>​                                                                                <center>（HLog声明周期）</center></p>
<p><strong>MemStore：</strong>MemStore作为一个缓存级的存储组件，缓存最近写入的数据</p>
<p>此外HFile中KeyValue数据需按Key排序，根据有序Key建立索引树，提升数据读取效率，数据在落盘生成HFile前应完成排序，MemStore执行KeyValue排序</p>
<p><strong>HFile：</strong>MemStore数据落盘会形成文件写入HDFS，即HFile</p>
<p>HFile文件由各种不同类型的Block构成，虽然Block类型不同，但却有相同数据结构</p>
<p><strong>BlockCache</strong>：将数据块缓存在内存提高数据读取性能</p>
<p>(通过这种方式来提升读取性能)客户端通过读取某个Block，先检查Block是否存在于BlockCache，存在直接加载不存在从HFile中加载，加载后缓存到BlockCache。</p>
<h4 id="读写流程"><a href="#读写流程" class="headerlink" title="读写流程"></a>读写流程</h4><p><strong>写入流程</strong></p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/dc380798368f4b30.png"></p>
<p>​                                                                                <center>（HBase写入流程）</center></p>
<p>整体架构上可分为三阶段：</p>
<p>客户端处理：客户端将写入请求预处理，根据元数据定位到写入数据的RegionServer，将请求发送给对应RegionServer</p>
<p>Region写入：RegionServer收到请求将数据解析，先写入WAL，在写入对应Region列簇对应的MemStore</p>
<p>MemStore Flush：当Region中的MemStore达到一定阈值时，将会Flush操作将内存中数据写入文件(HFile)</p>
<p><strong>BulkLoad</strong></p>
<p>首先使用MapReduce将待写入数据转换为HFile文件，再直接将这些HFile文件加载到集群中，BulkLoad没有将写请求发送给RegionServer处理，有效避免了给RegionServer带来较大的写入压力</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/94168ab67f4cc9c0.png"></p>
<p>​                                                                                <center>(BulkLoad工作流程)</center></p>
<p>BulkLoad主要有两个阶段：</p>
<p>HFile生成阶段：会跑一个MR任务，mapper由自己实现，reducer由HBase负责，这阶段将为每个Region生成一个对应的HFile文件</p>
<p>HFile导入阶段：HFile准备好后，使用工具completebulkload将HFile加载到HBase集群。</p>
<p><strong>读取流程</strong></p>
<p>相比写流程，读数据却复杂的多，查询可能涉及到多个Region，多块缓存甚至多个数据存储文件；由于更新和删除操作的实现导致读取却加大了难度，读取过程需要根据版本进行过滤，对已经标记删除的数据也要进行过滤。</p>
<p>大致分为四个流程</p>
<p>Client-Server读取交互逻辑：Client首先会从ZooKeeper中获取元数据hbase:meta表所在的RegionServer，然后根据待读写rowkey发送请求到元数据所在RegionServer，获取数据所在的目标RegionServer和Region（并将这部分元数据信息缓存到本地），最后将请求进行封装发送到目标RegionServer进行处理。</p>
<p>Server端Scan框架体系：一次scan可能会同时扫描一张表的多个Region，客户端会根据hbase:meta元数据将扫描的起始区间[startKey, stopKey)进行切分，切分成多个互相独立的查询子区间，每个子区间对应一个Region，RegionServer接收到客户端的get/scan请求之后做了两件事情：首先构建scanner iterator体系；然后执行next函数获取KeyValue，并对其进行条件过滤。</p>
<p>过滤淘汰不符合查询条件的HFile：经过Scanner体系的构建，KeyValue此时已经可以由小到大依次经过KeyValueScanner获得，但这些KeyValue是否满足用户设定的TimeRange条件、版本号条件以及Filter条件还需要进一步的检查—&gt;根据KeyRange过滤；根据TimeRange过滤；根据布隆过滤器进行过滤</p>
<p>从HFile中读取待查找Key：根据HFile索引树定位目标Block；BlockCache中检索目标Block； HDFS文件中检索目标Block；从Block中读取待查找KeyValue</p>
<h4 id="Compaction实现"><a href="#Compaction实现" class="headerlink" title="Compaction实现"></a>Compaction实现</h4><p>我们知道HFile小文件过多会导致读取效率过低，所以Compaction应运而生，Compaction核心功能是将小文件合并成大文件，提升读取效率</p>
<p><strong>Compaction工作原理</strong></p>
<p>Compaction是从Region的一个Store中选部分HFile来合并，合并原理：先从待合并文件中读出keyvalue，再由小到大排序成一个新文件，新文件代替之前文件对外提供服务</p>
<p>HBase中有两种Compaction：Minor Compaction和Major Compaction</p>
<p>Minor模式选取部分小的相邻的HFile，合成一个大的HFile，Major模式是将一个Store中所有的HFile合并成一个HFile，还会清理三种无意义数据(被删除数据，TTL过期数据，版本号超过设定版本号数据)，Major Compaction消耗资源大，所以通常手动触发</p>
<p>Compaction核心作用：合并小文件，减少文件数，稳定随机读延迟；提高数据的本地化率；清除无效数据，减小数据存储量，当然它的副作用是会消耗一定系统资源，影响上层业务的读取响应</p>
<p>compaction基本流程</p>
<p>HBase将该Compaction交给一个独立线程处理，该线程首先从对应Store中选择合适HFile文件进行合并。</p>
<p><img src="https://i.bmp.ovh/imgs/2022/03/b5b5c60918ac87f9.png"></p>
<p>​                                                                                <center>Compaction基本流程</center></p>
<p>Compaction触发时机：MemStore Flush；后台线程周期性检查及手动触发</p>
<p>HFile文件合并执行：(先选待合并HFile集合,再选合适处理线程,接着执行合并流程)</p>
<ol>
<li>分别读出待合并HFile文件的KeyValue，归并排序处理后写到./tmp目录下临时文件中</li>
<li>将临时文件移动到对应Store的数据目录</li>
<li>将Compaction的输入文件路径和输出文件路径封装为KV写入HLog日志,打Compaction标记，最后强制sync</li>
<li>将对应Store数据目录下的Compaction输入文件全部删除</li>
</ol>
<h4 id="负载均衡实现"><a href="#负载均衡实现" class="headerlink" title="负载均衡实现"></a>负载均衡实现</h4><p><strong>Region迁移</strong></p>
<p>HBase系统中，分片迁移就是Region迁移，Region在迁移的过程中不需迁移实际数据，只需将读写服务迁移</p>
<p>迁移过程涉及到Region迁移过程涉及多种状态的改变；迁移过程中涉及Master、ZooKeeper（ZK）以及RegionServer等多个组件的相互协调。</p>
<p><img src="https://i.bmp.ovh/imgs/2022/03/d9e83ebcda3a28bd.png"></p>
<p>​                                                                    <center>Region状态</center></p>
<p>执行过程中，Region分为unassign阶段和assign阶段</p>
<p>unassign表示Region从源RegionServer上下线</p>
<p><img src="https://i.bmp.ovh/imgs/2022/03/f6bc751c1dc2bfff.png"></p>
<p>​                                                                            <center>Region unassign阶段</center></p>
<p>assign表示Region在目标RegionServer上上线</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00fd07wtfj30q00deab0.jpg"></p>
<p>​                                                                    <center>Region assign阶段</center></p>
<p>Region迁移操作伴随着Region状态的不断变迁：思考一下</p>
<p>为什么需要这些状态？</p>
<p>无论是unassign操作还是assign操作，都是由多个子操作组成，涉及多个组件的协调合作，只有通过记录Region状态才能知道当前unassign或者assign的进度，在异常发生后才能根据具体进度继续执行</p>
<p>如何管理这些状态？</p>
<p>Region的这些状态会存储在三个区域：meta表，Master内存，ZooKeeper的region-in-transition节点，并且作用不同，只有这三个状态保持一致，对应的Region才处于正常的工作状态</p>
<p><strong>Region合并</strong></p>
<p>在线合并功能将空闲Region与相邻的Region合并，减少集群中空闲Region的个数</p>
<p>合并流程涉及到以下操作：</p>
<ol>
<li>客户端发送merge请求给Master。</li>
<li>Master将待合并的所有Region都move到同一个RegionServer上。</li>
<li>Master发送merge请求给该RegionServer。</li>
<li>RegionServer启动一个本地事务执行merge操作。</li>
<li>merge操作将待合并的两个Region下线，并将两个Region的文件进行合并。</li>
<li>将这两个Region从hbase:meta中删除，并将新生成的Region添加到hbase:meta中。</li>
<li>将新生成的Region上线。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">merge_region &#x27;encoded_regionname&#x27;,&#x27;encoded_regionname&#x27;</span><br></pre></td></tr></table></figure>

<p><strong>Region分裂</strong></p>
<p>Region分裂是HBase最核心的功能之一，是实现分布式可扩展性的基础。HBase中，Region分裂有多种触发策略可以配置，一旦触发，HBase会寻找分裂点，然后执行真正的分裂操作</p>
<p>Region分裂触发策略：</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00fbwjmt0j30pp06mjrt.jpg"></p>
<p>​                                                                            <center>分裂触发策略</center></p>
<p>满足Region分裂策略之后就会触发Region分裂。分裂被触发后的第一件事是寻找分裂点，HBase对于分裂点的定义为：整个Region中最大Store中的最大文件中最中心的一个Block的首个rowkey。如果定位到的rowkey是整个文件的首个rowkey或者最后一个rowkey，则认为没有分裂点</p>
<p>HBase将整个分裂过程包装成了一个事务，目的是保证分裂事务的原子性。整个分裂事务过程分为三个阶段：prepare、execute和rollback</p>
<p>prepare阶段：在内存中初始化两个子Region，具体生成两个HRegionInfo对象，包含tableName、regionName、startkey、endkey等。同时会生成一个transaction journal，这个对象用来记录分裂的进展</p>
<p>execute阶段：</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00g7f5hbgj30p20i6mzi.jpg"></p>
<p>​                                                                                    <center>分裂核心操作</center></p>
<p>rollback阶段：如果execute阶段出现异常，则执行rollback操作，为了实现回滚，整个分裂过程分为很多子阶段，回滚程序会根据当前进展到哪个子阶段清理对应的垃圾数据。</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h00g93t12hj30q80ag763.jpg"></p>
<p>​                                                                                <center>Region分裂子阶段</center></p>
<h4 id="宕机恢复原理"><a href="#宕机恢复原理" class="headerlink" title="宕机恢复原理"></a>宕机恢复原理</h4><p>常见故障分析(RegionServer)：Full GC异常；HDFS异常；HBase bug；机器宕机。</p>
<p>故障恢复基本原理：</p>
<p>Master：采用基本的热备方式来实现Master高可用，</p>
<p>RegionServer：RegionServer发生宕机，HBase马上会检测到，并在检测到宕机后将宕机RegionServer上所有Region重新分配到集群其他正常RegionServer，再根据HLog恢复丢失数据。</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h00g0bqfc9j30hv0bot9g.jpg"></p>
<p>​                                                            <center>RegionServer故障恢复</center></p>
<p>Master检测RegionServer宕机：</p>
<p>HBase使用ZooKeeper协助Master检测RegionServer宕机，使用ZooKeeper临时节点机制，RegionServer注册成临时节点之后，Master会watch在/rs节点上（该节点下的所有子节点一旦发生离线就会通知Master），这样一旦RegionServer发生宕机，RegionServer注册到/rs节点下的临时节点就会离线，这个消息会马上通知给Master，Master检测到RegionServer宕机。</p>
<p> 切分未持久化数据的HLog：</p>
<p>日志回放需要以Region为单元进行，一个Region一个Region地回放，回放之前首先需要将HLog按照Region进行分组，每个Region的日志数据合并放在一起，方便后面按照Region进行回放。这个分组合并过程称为HLog切分。</p>
<ol>
<li>LogSplitting策略：HBase最初阶段日志切分的整个过程都由Master控制执行</li>
</ol>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h00ghvcu7cj30t00chab6.jpg"></p>
<p>​                                                                                    <center>LogSplitting</center></p>
<ol start="2">
<li>Distributed Log Splitting：Log Splitting的分布式实现，借助Master和所有RegionServer的计算能力进行日志切分，其中Master是协调者，RegionServer是实际的工作者</li>
</ol>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00gjapb9rj30s9080q3e.jpg"></p>
<p>​                                                                            <center>Distributed Log Splitting</center></p>
<p>Distributed Log Replay：</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h00gk7vr1ij30q202fglu.jpg"></p>
<p>​                                                                            <center>Distributed Log Replay</center></p>
<h4 id="备份与恢复"><a href="#备份与恢复" class="headerlink" title="备份与恢复"></a>备份与恢复</h4><p>使用Snapshot在线备份。Snapshot备份以快照技术为基础原理，备份过程不需要拷贝任何数据，因此对当前集群几乎没有任何影响，备份速度非常快而且可以保证数据一致性，Snapshot是HBase非常核心的一个功能，使用在线Snapshot备份可以满足用户很多需求，比如增量备份和数据迁移。</p>
<p><strong>在线Snapshot备份与恢复的用法</strong></p>
<p>snapshot：可以为表打一个快照，但并不涉及数据移动</p>
<blockquote>
<p>snapshot ‘sourceTable’,’snapshotName’</p>
</blockquote>
<p>restore_snapshot：用于恢复指定快照,恢复过程会替代原有数据,将表还原到快照点,快照点之后所有更新将会丢失</p>
<blockquote>
<p>restore_snapshot ‘snatshotName’</p>
</blockquote>
<p>clone_snapshot，可以根据快照恢复出一个新表，恢复过程不涉及数据移动，可以在秒级完成</p>
<blockquote>
<p>clone_snapshot ‘snatshotName’,’tableName’</p>
</blockquote>
<p>ExportSnapshot，可以将A集群的快照数据迁移到B集群。ExportSnapshot是HDFS层面的操作，需使用MapReduce进行数据的并行迁移，因此需要在开启MapReduce的机器上进行迁移</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00h79o51tj30ou03e3z7.jpg"></p>
<p><strong>Snatshot原理</strong></p>
<p>Snatshot流程：将MemStore中的缓存数据f lush到文件中；为所有HFile文件分别新建引用指针，这些指针元数据就是Snapshot。</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00h8k684kj30ps0bejsf.jpg"></p>
<p>​                                                                        <center>snatshot原理</center></p>
<p><strong>两阶段提交</strong></p>
<p>HBase使用两阶段提交（Two-Phase Commit，2PC）协议来保证Snapshot的分布式原子性</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h00haw7033j30ro0bcgmm.jpg"></p>
<p>​                                                                <center>两阶段提交原理</center></p>
<p><strong>Snapshot核心实现</strong></p>
<p> Region实现Snapshot</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h00hcn988gj30p70agq3u.jpg"></p>
<p>​                                                                <center>snatshot基本流程</center></p>
<blockquote>
<p>Region生成的Snapshot文件是临时文件，在/hbase/.hbase-snapshot/.tmp目录下，因为Snapshot过程通常特别快，所以很难看到单个Region生成的Snapshot文件。</p>
</blockquote>
<p>Master汇总所有Region Snapshot的结果：Master会在所有Region完成Snapshot之后执行一个汇总操作（consolidate），将所有regionsnapshot manifest汇总成一个单独manifest，汇总后的Snapshot文件可以在HDFS目录下看到的，路径为：/hbase/.hbase-snapshot/snapshotname/data.manifest。</p>
<p><strong>洞见clone_snapshot</strong></p>
<ol>
<li>预检查。确认当前目标表没有执行snapshot以及restore等操作，否则直接返回错误</li>
<li>在tmp文件夹下新建目标表目录并在表目录下新建.tabledesc文件，在该文件中写入表schema信息。</li>
<li>新建region目录。根据snapshot manifest中的信息新建Region相关目录以及HFile文件。</li>
<li>将表目录从tmp文件夹下移到HBase Root Location。</li>
<li>修改hbase:meta表，将克隆表的Region信息添加到hbase:meta表中，注意克隆表的Region名称和原数据表的Region名称并不相同（Region名称与table名称相关，table名不同，Region名则肯定不同）</li>
<li>将这些Region通过round-robin方式均匀分配到整个集群中，并在ZooKeeper上将克隆表的状态设置为enabled，正式对外提供服务。</li>
</ol>
<h4 id="面试题解"><a href="#面试题解" class="headerlink" title="面试题解"></a>面试题解</h4><p><strong>HBase如何写数据</strong></p>
<p>Client写入，存入MemStore一直到MemStore满，Flush成一个StoreFile增长到一定阈值，出发Compact合并，多个StoreFile合并成一个StoreFile同时进行版本合并和数据删除，当StoreFiles Compact后形成越来与大的StoreFile，单个StoreFile大小超过一定阈值(10G)触发split，把当前Region Split成两个Region，Region会下线，新Split除两个Region会被HMaster分配到相应HRegionServer，Region得以分压</p>
<p><strong>HDFS和HBase使用场景</strong></p>
<p>首先Hbase基于HDFS存储的</p>
<p>HDFS：一次写入多次读取，数据一致性，容错和恢复机制</p>
<p>HBase：瞬间写入量很大，数据需要长久保存，不适用于有join和多级索引及表关系复杂数据模型，大数据量且有快速随机访问的需求，业务场景简单</p>
<p><strong>HBase存储结构</strong></p>
<p>HBase表通过rowkey按照一定范围被分割成多个子表HRegion(一个HRegion超过256要分为2个)，由HRegionServer管理，HMaster管理HRegion</p>
<p>HRegion存取一个子表时会创建一个HRegion对象，对表的每个列簇创建一个store实例，每个store都有0或多个StoreFile对应，每个StoreFile对应一个HFile</p>
<p><strong>HBase RowKey设计原则</strong></p>
<p>长度原则：100 字节以内，8 的倍数最好，可能的情况下越短越好。</p>
<p>散列原则：高位散列，低位时间字段。避免热点问题。</p>
<p>唯一原则：分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一起</p>
<p><strong>HBase列簇设计</strong></p>
<p>原则：在合理范围内能尽量少的减少列簇就尽量减少列簇，因为列簇是共享 region 的，每个列簇数据相差太大导致查询效率低下</p>
<p>最优：将所有相关性很强的 key-value 都放在同一个列簇下，这样既能做到查 询效率最高，也能保持尽可能少的访问不同的磁盘文件。</p>
<p><strong>热点现象怎么解决</strong></p>
<p>某个小的时段内，对 HBase 的读写请求集中到极少数的 Region 上，导致这些 region 所在的 RegionServer 处理请求量骤增，负载量明显偏大，而其他的 RgionServer 明显空闲</p>
<p>热点发生在大量的 client 直接访问集群的一个或极少数个节点</p>
<ol>
<li>加盐：在 rowkey 的前面增加随机数，使得它和之前的 rowkey 的开头不同。加盐之后的 rowkey 就会根据随机生成的前缀分散到各个 region 上， 以避免热点</li>
<li>哈希：哈希可以使负载分散到整个集群，但是读却是可以预测的</li>
<li>反转：反转固定长度或者数字格式的 rowkey，可以使得 rowkey 中经常改变的部分（最没有意义的部分）放在前面。</li>
<li>时间戳反转</li>
<li>HBase建表预分区：创建 HBase 表时，就预先根据可能的 RowKey 划分出多 个 region 而不是默认的一个，从而可以将后续的读写操作负载均衡到不 同的 region 上，避免热点现象。</li>
</ol>
<p><strong>HBase compact（minor major）</strong></p>
<p>在 hbase 中每当有 memstore 数据 flush 到磁盘之后，就形成一个 storefile， 当 storeFile 的数量达到一定程度就需要将 storefile 文件来进行 compaction</p>
<ol>
<li><p>合并文件</p>
</li>
<li><p>清洗过期多余版本数据</p>
</li>
<li><p>提高读写数据的效率</p>
</li>
<li><p>minor用来部分文件的合并以及包括minVersion=0并且设置ttl的过期版本清理，不做删除数据，多版本数据清理</p>
</li>
<li><p>major对Region下的HStore下所有StoreFile执行合并，最终合并出一个文件</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
</search>

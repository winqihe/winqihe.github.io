<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HBase</title>
    <url>/2022/03/05/HBase/</url>
    <content><![CDATA[<h2 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h2><p>本文内容摘录至《HBase原理与实践》</p>
<span id="more"></span>

<h4 id="概述一览"><a href="#概述一览" class="headerlink" title="概述一览"></a>概述一览</h4><p><strong>基本概念：</strong></p>
<p>table：表，一个表包含多行数据。</p>
<p>row：行，一行数据包含一个唯一标识rowkey、多个column以及对应的值。在HBase中，一张表中所有row都按照rowkey的字典序由小到大排序。</p>
<p>column：列，HBase中的column由column family（列簇）以及qualif ier（列名）两部分组成，两者中间使用”:”相连。column family在表创建的时候需要指定，用户不能随意增减。一个column family下可以设置任意多个qualif ier，因此可以理解为HBase中的列可以动态增加，理论上甚至可以扩展到上百万列。</p>
<p>timestamp：时间戳，每个cell在写入HBase的时候都会默认分配一个时间戳作为该cell的版本，当然，用户也可以在写入的时候自带时间戳。HBase支持多版本特性，即同一rowkey、column下可以有多个value存在，这些value使用timestamp作为版本号，版本越大，表示数据越新。</p>
<p>cell：单元格，由五元组（row, column, timestamp, type, value）组成的结构，其中type表示Put/Delete这样的操作类型，timestamp代表这个cell的版本。这个结构在数据库中实际是以KV结构存储的，其中（row, column, timestamp, type）是K，value字段对应KV结构的V。</p>
<p><img src="/.com//image-20220306154752069.png" alt="image-20220306154752069"></p>
<p>​                                                                            （HBase逻辑视图）</p>
<p><strong>体系架构：</strong></p>
<p><img src="/.com//image-20220306154811916.png" alt="image-20220306154811916"></p>
<p>​                                                                                （体系架构）                                                                    </p>
<ol>
<li><p>Client：丰富的编程接口，Client访问数据行前要先通过元数据表定位目标数据所在RegionServer，发请求给RegionServer，这些元数据也会被缓存到本地。</p>
</li>
<li><p>Zookeeper：实现Master高可用(异常宕机进行选举)，管理系统核心元数据(保存元数据表所在的RegionServer地址)，参与RegionServer宕机恢复(心跳机制)，实现分布式表锁</p>
</li>
<li><p>Master：处理各种管理请求(建表、修改表、权限操作、切分表、合并数据分片以及Compaction)，管理集群中的RegionServer(RegionServer中Region的负载均衡、RegionServer的宕机恢复以及Region的迁移)，清理过期日志以及文件，Master会每隔一段时间检查HDFS中HLog是否过期、HFile是否已经被删除，并在过期之后将其删除</p>
</li>
<li><p>RegionServer：响应用户的IO请求，由WAL(HLog),BlockCache,多个Region构成</p>
<p>WAL(HLog):实现数据高可靠性，数据写入缓存前先顺序写入HLog，实现HBase集群间主从复制通过回放主集群推送过来的HLog日志实现主从复制。</p>
<p>BlockCache:HBase系统中的读缓存，客户端从磁盘读取数据之后通常会将数据缓存到系统内存中，后续访问同一行数据可以直接从内存中获取而不需要访问磁盘。</p>
<p>Region:数据表的一个分片，当数据表大小超过阈值就会水平切分，分裂为两个Region，一个Region由一个或多个Store构成，Store个数取决于列簇个数，每个列簇的数据存放在一起形成一个存储单元Store，每个Store由一个MemStore和一或多个HFile组成，写入数据先写入MemStore，到一定阈值(128M)将Flush成HFile，随着HFile数越来越多，将会执行Compact合并成大文件</p>
</li>
<li><p>HDFS：HBase底层依赖HDFS存储数据，用户数据文件、HLog日志文件等最终都会写入HDFS落盘</p>
</li>
</ol>
<p><strong>HBase特性：</strong></p>
<p>优点：容量大，良好可扩展性，稀疏性，高性能，多版本，支持过期，Hadoop原生支持</p>
<p>缺点：不支持复杂聚合运算，没有二级索引功能，不支持全局跨行事务</p>
<h4 id="依赖服务"><a href="#依赖服务" class="headerlink" title="依赖服务"></a>依赖服务</h4><p><strong>ZooKeeper</strong></p>
<p><strong>HDFS</strong></p>
<h4 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h4><h4 id="RegionServer结构"><a href="#RegionServer结构" class="headerlink" title="RegionServer结构"></a>RegionServer结构</h4><p>一个RegionServer由一或多个HLog，BlockCache及多个Region组成，RegionServer负责Region的数据读写，一个Region由多个store组成，Store存放对应列簇数据，Store包含一个MemStore和多个HFile。</p>
<p><img src="/.com//image-20220306154025407.png" alt="image-20220306154025407"></p>
<p>​                                                                                （RegionServer结构）</p>
<p><strong>HLog</strong>：用来保证数据写入的可靠性</p>
<p>HBase中系统故障恢复及主从复制都基于HLog实现，数据以追加的形式先写入HLog，再写入MemStore，如果RegionServer宕机，此时写入MemStore中但尚未Flush磁盘数据丢失，需要回访HLog补救丢失数据，HBase主从复制需要主集群将HLog日志发送给从集群，完成集群间复制</p>
<p><img src="/.com//image-20220306155654549.png" alt="image-20220306155654549"></p>
<p>​                                                                                （HLog文件结构）</p>
<p>HLog文件生成后不会永久存储在系统中，使命完成后，文件会失效最终删除</p>
<p><img src="/.com//image-20220306160001897.png" alt="image-20220306160001897"></p>
<p>​                                                                                （HLog声明周期）</p>
<p><strong>MemStore：</strong>MemStore作为一个缓存级的存储组件，缓存最近写入的数据</p>
<p>此外HFile中KeyValue数据需按Key排序，根据有序Key建立索引树，提升数据读取效率，数据在落盘生成HFile前应完成排序，MemStore执行KeyValue排序</p>
<p><strong>HFile：</strong>MemStore数据落盘会形成文件写入HDFS，即HFile</p>
<p>HFile文件由各种不同类型的Block构成，虽然Block类型不同，但却有相同数据结构</p>
<p><strong>BlockCache</strong>：将数据块缓存在内存提高数据读取性能</p>
<p>(通过这种方式来提升读取性能)客户端通过读取某个Block，先检查Block是否存在于BlockCache，存在直接加载不存在从HFile中加载，加载后缓存到BlockCache。</p>
<h4 id="读写流程"><a href="#读写流程" class="headerlink" title="读写流程"></a>读写流程</h4><h4 id="HBase面试题解"><a href="#HBase面试题解" class="headerlink" title="HBase面试题解"></a>HBase面试题解</h4><p><strong>HBase如何写数据</strong></p>
<p>Client写入，存入MemStore一直到MemStore满，Flush成一个StoreFile增长到一定阈值，出发Compact合并，多个StoreFile合并成一个StoreFile同时进行版本合并和数据删除，当StoreFiles Compact后形成越来与大的StoreFile，单个StoreFile大小超过一定阈值(10G)触发split，把当前Region Split成两个Region，Region会下线，新Split除两个Region会被HMaster分配到相应HRegionServer，Region得以分压</p>
<p><strong>HDFS和HBase使用场景</strong></p>
<p>首先Hbase基于HDFS存储的</p>
<p>HDFS：一次写入多次读取，数据一致性，容错和恢复机制</p>
<p>HBase：瞬间写入量很大，数据需要长久保存，不适用于有join和多级索引及表关系复杂数据模型，大数据量且有快速随机访问的需求，业务场景简单</p>
<p><strong>HBase存储结构</strong></p>
<p>HBase表通过rowkey按照一定范围被分割成多个子表HRegion(一个HRegion超过256要分为2个)，由HRegionServer管理，HMaster管理HRegion</p>
<p>HRegion存取一个子表时会创建一个HRegion对象，对表的每个列簇创建一个store实例，每个store都有0或多个StoreFile对应，每个StoreFile对应一个HFile</p>
<p><strong>HBase RowKey设计原则</strong></p>
<p>长度原则：100 字节以内，8 的倍数最好，可能的情况下越短越好。</p>
<p>散列原则：高位散列，低位时间字段。避免热点问题。</p>
<p>唯一原则：分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一起</p>
<p><strong>HBase列簇设计</strong></p>
<p>原则：在合理范围内能尽量少的减少列簇就尽量减少列簇，因为列簇是共享 region 的，每个列簇数据相差太大导致查询效率低下</p>
<p>最优：将所有相关性很强的 key-value 都放在同一个列簇下，这样既能做到查 询效率最高，也能保持尽可能少的访问不同的磁盘文件。</p>
<p><strong>热点现象怎么解决</strong></p>
<p>某个小的时段内，对 HBase 的读写请求集中到极少数的 Region 上，导致这些 region 所在的 RegionServer 处理请求量骤增，负载量明显偏大，而其他的 RgionServer 明显空闲</p>
<p>热点发生在大量的 client 直接访问集群的一个或极少数个节点</p>
<ol>
<li>加盐：在 rowkey 的前面增加随机数，使得它和之前的 rowkey 的开头不同。加盐之后的 rowkey 就会根据随机生成的前缀分散到各个 region 上， 以避免热点</li>
<li>哈希：哈希可以使负载分散到整个集群，但是读却是可以预测的</li>
<li>反转：反转固定长度或者数字格式的 rowkey，可以使得 rowkey 中经常改变的部分（最没有意义的部分）放在前面。</li>
<li>时间戳反转</li>
<li>HBase建表预分区：创建 HBase 表时，就预先根据可能的 RowKey 划分出多 个 region 而不是默认的一个，从而可以将后续的读写操作负载均衡到不 同的 region 上，避免热点现象。</li>
</ol>
<p><strong>HBase compact（minor major）</strong></p>
<p>在 hbase 中每当有 memstore 数据 flush 到磁盘之后，就形成一个 storefile， 当 storeFile 的数量达到一定程度就需要将 storefile 文件来进行 compaction</p>
<ol>
<li><p>合并文件</p>
</li>
<li><p>清洗过期多余版本数据</p>
</li>
<li><p>提高读写数据的效率</p>
</li>
<li><p>minor用来部分文件的合并以及包括minVersion=0并且设置ttl的过期版本清理，不做删除数据，多版本数据清理</p>
</li>
<li><p>major对Region下的HStore下所有StoreFile执行合并，最终合并出一个文件</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop</title>
    <url>/2022/03/03/Hadoop/</url>
    <content><![CDATA[<h4 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h4><ul>
<li>HDFS优缺点：</li>
</ul>
<span id="more"></span>

<p>优点：高容错性，适合大规模数据处理，廉价成本</p>
<p>缺点：不适合低延时数据访问，无法高效对大量小文件存储，不支持并发写入，文件随即修改。</p>
<ul>
<li>HDFS组成架构</li>
</ul>
<p>NameNode:管理HDFS名称空间，配置副本策略，管理数据块，处理客户端请求</p>
<p>DataNode:存储实际数据块，执行数据块的读写操作，</p>
<p>Client:文件切分，与NameNode交互获取文件位置信息，与DataNode交互，读写数据</p>
<p>SecondaryNameNode:辅助NameNode为其分担工作量，辅助恢复NameNode</p>
<ul>
<li>Hadoop文件块大小默认为128M</li>
</ul>
<p>文件块太小，会增加寻址时间</p>
<p>文件块太大，从磁盘传输数据的时间将大于定位数据块起始位置所需的时间，导致程序处理数据慢</p>
<ul>
<li>HDFS写数据</li>
</ul>
<ol>
<li>Client向NameNode请求上传文件</li>
<li>NameNode返回是否可以上传</li>
<li>Client请求第一个Block上传到那个DataNode</li>
<li>NameNode返回3个DataNode节点</li>
<li>Client请求datanode1上传数据，datanode1收到请求会继续调用datanode2，datanode2调用datanode3，将通信管道建立完</li>
<li>datanode1，2，3逐级应答Client</li>
<li>Client向datanode1上传第一个Block(Packet)，先从磁盘读数据放到本地进行缓存，datanode1收到一个Packet就会传给datanode2，datanode2传给datanode3，</li>
<li>当一个Block传输完，Client再次请求上传第二个Block</li>
</ol>
<ul>
<li>HDFS读数据</li>
</ul>
<ol>
<li>Client向NameNode请求下载文件，NameNode查询元数据，定位文件块所在DataNode地址</li>
<li>从(先就近后随机)DataNode请求读取数据</li>
<li>DataNode传输数据给Client(Packet)</li>
<li>Client(Packet)接受，先本地缓存后写入目标文件</li>
</ol>
<ul>
<li>NameNode和SecondaryNameNode工作机制</li>
</ul>
<p>NameNode启动：</p>
<ol>
<li>首次启动格式化，创建Fsimage，Edits，之后启动load edits和fsimage到内存</li>
<li>Client对元数据进行增删改请求</li>
<li>NameNode记录操作日志</li>
<li>NameNode在内存中对数据增删改</li>
</ol>
<p>SecondaryNode工作：</p>
<ol>
<li>2NN询问NN是否要CheckPoint</li>
<li>2NN请求执行CheckPoint</li>
<li>NN滚动正在写的Edits，将滚动前的edits和fsimage拷贝到2NN</li>
<li>2NN加载edits和fsimage到内存，并进行合并，生成新的fsimage.checkpoint</li>
<li>拷贝fsimage.checkpoint到NN，NN将fsimage.checkpoint更名为fsimage</li>
</ol>
<ul>
<li>DataNode工作机制</li>
</ul>
<ol>
<li>一个数据块在DataNode上以文件形式存储在磁盘，分别是数据本身和元数据(数据块长度，数据块校验和，时间戳)</li>
<li>DataNode启动后向NameNode注册，通过后周期性(1h)向NameNode上报所有块信息</li>
<li>心跳3s/次，心跳返回结果带有NameNode给DataNode的命令，若超过10min没有收到某个DataNode的心跳，则认为节点已挂</li>
<li>集群运行中可以服役新节点或退役旧节点</li>
</ol>
<ul>
<li>DataNode保证数据完整性</li>
</ul>
<ol>
<li>当DataNode读取Block，会计算CheckSum</li>
<li>若计算后的CheckSum与Block创建时值不同，说明Block已损</li>
<li>Client读取其他DataNode上的Block，DataNode在其文件创建后周期验证CheckSum</li>
</ol>
<ul>
<li>HDFS HA高可用</li>
</ul>
<p>HA即高可用(7*24不间断服务)</p>
<p>实现高可用最关键的策略是消除单点故障，通过配置Active/Standby两个NameNodes实现集群中对NameNode的热备</p>
<h4 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h4><ul>
<li>MapReduce优缺点</li>
</ul>
<p>优点：易于编程，良好扩展性，高容错性，适合PB级以上海量数据离线处理</p>
<p>缺点：不擅长实时计算，不擅长流式计算，不擅长DAG计算</p>
<ul>
<li>MapReduce核心思想</li>
</ul>
<ol>
<li>分布式的计算通常要分为至少两个阶段</li>
<li>第一阶段的MapTask并发实例，完全并行运行</li>
<li>第二阶段的ReduceTask并发实例互不相干，但数据依赖于上个阶段所有MapTask并发实例的输出</li>
<li>MapReduce编程模型只能包含一个Map和Reduce阶段</li>
</ol>
<ul>
<li>Shuffle机制</li>
</ul>
<ol>
<li>Collect阶段：MapTask的数据输出到环形缓冲区</li>
<li>Spill阶段：当内存中数据量到一定阈值，会将数据写入本地磁盘(将数据写入磁盘前需要对数据进行一次排序)，如果配置combiner会将相同分区号和key的数据进行排序</li>
<li>MapTask阶段的Merge：把所有溢出的临时文件进行一次merge，确保一个MapTask最终只产生一个中间数据文件</li>
<li>Copy阶段：ReduceTask启动Fetcher线程到已完成MapTask节点上copy一份数据</li>
<li>ReduceTask阶段的Merge：在ReduceTask复制数据同时，会在后台开启两个线程对内存到本地的数据文件进行merge</li>
<li>Sort阶段：对数据进行merge同时，会进行排序，由于MapTask阶段已经对数据进行了局部排序，ReduceTask只需保证Copy数据的最终整体有效性</li>
</ol>
]]></content>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka</title>
    <url>/2022/03/05/Kafka/</url>
    <content><![CDATA[]]></content>
      <tags>
        <tag>中间件</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark</title>
    <url>/2022/03/04/Spark/</url>
    <content><![CDATA[<blockquote>
<p>Spark大数据的计算分析引擎</p>
</blockquote>
]]></content>
      <tags>
        <tag>数据</tag>
      </tags>
  </entry>
</search>

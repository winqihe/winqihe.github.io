<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HBase</title>
    <url>/2022/03/05/HBase/</url>
    <content><![CDATA[<h2 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h2><p>本文内容摘录于《HBase原理与实践》</p>
<span id="more"></span>

<h4 id="概述一览"><a href="#概述一览" class="headerlink" title="概述一览"></a>概述一览</h4><p><strong>基本概念：</strong></p>
<p>table：表，一个表包含多行数据。</p>
<p>row：行，一行数据包含一个唯一标识rowkey、多个column以及对应的值。在HBase中，一张表中所有row都按照rowkey的字典序由小到大排序。</p>
<p>column：列，HBase中的column由column family（列簇）以及qualif ier（列名）两部分组成，两者中间使用”:”相连。column family在表创建的时候需要指定，用户不能随意增减。一个column family下可以设置任意多个qualif ier，因此可以理解为HBase中的列可以动态增加，理论上甚至可以扩展到上百万列。</p>
<p>timestamp：时间戳，每个cell在写入HBase的时候都会默认分配一个时间戳作为该cell的版本，当然，用户也可以在写入的时候自带时间戳。HBase支持多版本特性，即同一rowkey、column下可以有多个value存在，这些value使用timestamp作为版本号，版本越大，表示数据越新。</p>
<p>cell：单元格，由五元组（row, column, timestamp, type, value）组成的结构，其中type表示Put/Delete这样的操作类型，timestamp代表这个cell的版本。这个结构在数据库中实际是以KV结构存储的，其中（row, column, timestamp, type）是K，value字段对应KV结构的V。</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/078bfb0511e176a0.png"></p>
<p>​                                                                            <center>（HBase逻辑视图）</center></p>
<p><strong>体系架构：</strong></p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/0576dba2ceb0e8a3.png"></p>
<p>​                                                                                <center>（体系架构）</center>                                            </p>
<ol>
<li><p>Client：丰富的编程接口，Client访问数据行前要先通过元数据表定位目标数据所在RegionServer，发请求给RegionServer，这些元数据也会被缓存到本地。</p>
</li>
<li><p>Zookeeper：实现Master高可用(异常宕机进行选举)，管理系统核心元数据(保存元数据表所在的RegionServer地址)，参与RegionServer宕机恢复(心跳机制)，实现分布式表锁</p>
</li>
<li><p>Master：处理各种管理请求(建表、修改表、权限操作、切分表、合并数据分片以及Compaction)，管理集群中的RegionServer(RegionServer中Region的负载均衡、RegionServer的宕机恢复以及Region的迁移)，清理过期日志以及文件，Master会每隔一段时间检查HDFS中HLog是否过期、HFile是否已经被删除，并在过期之后将其删除</p>
</li>
<li><p>RegionServer：响应用户的IO请求，由WAL(HLog),BlockCache,多个Region构成</p>
<p>WAL(HLog):实现数据高可靠性，数据写入缓存前先顺序写入HLog，实现HBase集群间主从复制通过回放主集群推送过来的HLog日志实现主从复制。</p>
<p>BlockCache:HBase系统中的读缓存，客户端从磁盘读取数据之后通常会将数据缓存到系统内存中，后续访问同一行数据可以直接从内存中获取而不需要访问磁盘。</p>
<p>Region:数据表的一个分片，当数据表大小超过阈值就会水平切分，分裂为两个Region，一个Region由一个或多个Store构成，Store个数取决于列簇个数，每个列簇的数据存放在一起形成一个存储单元Store，每个Store由一个MemStore和一或多个HFile组成，写入数据先写入MemStore，到一定阈值(128M)将Flush成HFile，随着HFile数越来越多，将会执行Compact合并成大文件</p>
</li>
<li><p>HDFS：HBase底层依赖HDFS存储数据，用户数据文件、HLog日志文件等最终都会写入HDFS落盘</p>
</li>
</ol>
<p><strong>HBase特性：</strong></p>
<p>优点：容量大，良好可扩展性，稀疏性，高性能，多版本，支持过期，Hadoop原生支持</p>
<p>缺点：不支持复杂聚合运算，没有二级索引功能，不支持全局跨行事务</p>
<h4 id="依赖服务"><a href="#依赖服务" class="headerlink" title="依赖服务"></a>依赖服务</h4><p><strong>ZooKeeper</strong>：一个分布式HBase集群的部署运行强烈依赖于ZooKeeper</p>
<p><strong>HDFS</strong>：HBase的文件都存放在HDFS（Hadoop Distribuited File System）文件系统上</p>
<h4 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h4><p>通过ThriftServer的Java HBase客户端来请求HBase集群。</p>
<p>定位Meta表：</p>
<p>客户端在做任何数据操作时，都要先确定数据在哪个Region上，然后再根据Region的RegionServer信息，去对应的RegionServer上读取数据，HBase系统内部设计了一张特殊的表——hbase:meta表，专门用来存放整个集群所有的Region信息。hbase:meta中的hbase指的是namespace，HBase容许针对不同的业务设计不同的namespace，系统表采用统一的namespace，即hbase；meta指的是hbase这个namespace下的表名。</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h00gqa64zqj30r507vdgm.jpg"></p>
<p>​                                                                    <center>Hbase:meta结构</center></p>
<p><font color="red"> HBase作为一个分布式数据库系统，一个大的集群可能承担数千万的查询写入请求，而hbase:meta表只有一个Region，如果所有的流量都先请求hbase:meta表找到Region，再请求Region所在的RegionServer，那么hbase:meta表的将承载巨大的压力，这个Region将马上成为热点Region，且根本无法承担数千万的流量。那么，如何解决这个问题呢</font></p>
<p>把hbase:meta表的Region信息缓存在HBase客户端</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h00gu71x0zj30q309ldgz.jpg"></p>
<p>​                                                                                <center>客户端定义Region</center></p>
<p>少量写和批量写：</p>
<p>HBase提供了3种常见的数据写入API：</p>
<p>table.put(put)：在服务端先写WAL，然后写MemStore，一旦MemStore写满就f lush到磁盘上。特点是，默认每次写入都需要执行一次RPC和磁盘持久化。因此，写入吞吐量受限于磁盘带宽、网络带宽以及f lush的速度。但它能保证每次写入操作都持久化到磁盘，不会有任何数据丢失。最重要的是，它能保证put操作的原子性。</p>
<p>table.put(List<Put> puts)：在客户端缓存put，等凑足了一批put，就将这些数据打包成一次RPC发送到服务端，一次性写WAL，并写MemStore。相比第一种方式省去了多次往返RPC以及多次刷盘的开销，吞吐量大大提升。不过，这个RPC操作耗时一般都会长一点，因为一次写入了多行数据。这些put中，可能有一部分失败，失败的那些put操作会经历若干次重试</Put></p>
<p>bulk load：直接将待写入数据生成HFile，将这些HFile直接加载到对应的Region下的CF内。在生成HFile时，在HBase服务端没有任何RPC调用，只在load HFile时会调用RPC，一种完全离线的快速写入方式。bulk load应该是最快的批量写手段，同时不会对线上的集群产生巨大压力。在load完HFile之后，CF内部会进行Compaction，但是Compaction是异步的且可以限速，所以产生的IO压力是可控的。bulk load对线上集群非常友好。</p>
<h4 id="RegionServer结构"><a href="#RegionServer结构" class="headerlink" title="RegionServer结构"></a>RegionServer结构</h4><p>一个RegionServer由一或多个HLog，BlockCache及多个Region组成，RegionServer负责Region的数据读写，一个Region由多个store组成，Store存放对应列簇数据，Store包含一个MemStore和多个HFile。</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/8343fa265a93d486.png"></p>
<p>​                                                                                <center>（RegionServer结构）</center></p>
<p><strong>HLog</strong>：用来保证数据写入的可靠性</p>
<p>HBase中系统故障恢复及主从复制都基于HLog实现，数据以追加的形式先写入HLog，再写入MemStore，如果RegionServer宕机，此时写入MemStore中但尚未Flush磁盘数据丢失，需要回访HLog补救丢失数据，HBase主从复制需要主集群将HLog日志发送给从集群，完成集群间复制</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/fb81be8e4743af98.png"></p>
<p>​                                                                               <center> （HLog文件结构）</center></p>
<p>HLog文件生成后不会永久存储在系统中，使命完成后，文件会失效最终删除</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/eebdaf515bf189bf.png"></p>
<p>​                                                                                <center>（HLog声明周期）</center></p>
<p><strong>MemStore：</strong>MemStore作为一个缓存级的存储组件，缓存最近写入的数据</p>
<p>此外HFile中KeyValue数据需按Key排序，根据有序Key建立索引树，提升数据读取效率，数据在落盘生成HFile前应完成排序，MemStore执行KeyValue排序</p>
<p><strong>HFile：</strong>MemStore数据落盘会形成文件写入HDFS，即HFile</p>
<p>HFile文件由各种不同类型的Block构成，虽然Block类型不同，但却有相同数据结构</p>
<p><strong>BlockCache</strong>：将数据块缓存在内存提高数据读取性能</p>
<p>(通过这种方式来提升读取性能)客户端通过读取某个Block，先检查Block是否存在于BlockCache，存在直接加载不存在从HFile中加载，加载后缓存到BlockCache。</p>
<h4 id="读写流程"><a href="#读写流程" class="headerlink" title="读写流程"></a>读写流程</h4><p><strong>写入流程</strong></p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/dc380798368f4b30.png"></p>
<p>​                                                                                <center>（HBase写入流程）</center></p>
<p>整体架构上可分为三阶段：</p>
<p>客户端处理：客户端将写入请求预处理，根据元数据定位到写入数据的RegionServer，将请求发送给对应RegionServer</p>
<p>Region写入：RegionServer收到请求将数据解析，先写入WAL，在写入对应Region列簇对应的MemStore</p>
<p>MemStore Flush：当Region中的MemStore达到一定阈值时，将会Flush操作将内存中数据写入文件(HFile)</p>
<p><strong>BulkLoad</strong></p>
<p>首先使用MapReduce将待写入数据转换为HFile文件，再直接将这些HFile文件加载到集群中，BulkLoad没有将写请求发送给RegionServer处理，有效避免了给RegionServer带来较大的写入压力</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/03/94168ab67f4cc9c0.png"></p>
<p>​                                                                                <center>(BulkLoad工作流程)</center></p>
<p>BulkLoad主要有两个阶段：</p>
<p>HFile生成阶段：会跑一个MR任务，mapper由自己实现，reducer由HBase负责，这阶段将为每个Region生成一个对应的HFile文件</p>
<p>HFile导入阶段：HFile准备好后，使用工具completebulkload将HFile加载到HBase集群。</p>
<p><strong>读取流程</strong></p>
<p>相比写流程，读数据却复杂的多，查询可能涉及到多个Region，多块缓存甚至多个数据存储文件；由于更新和删除操作的实现导致读取却加大了难度，读取过程需要根据版本进行过滤，对已经标记删除的数据也要进行过滤。</p>
<p>大致分为四个流程</p>
<p>Client-Server读取交互逻辑：Client首先会从ZooKeeper中获取元数据hbase:meta表所在的RegionServer，然后根据待读写rowkey发送请求到元数据所在RegionServer，获取数据所在的目标RegionServer和Region（并将这部分元数据信息缓存到本地），最后将请求进行封装发送到目标RegionServer进行处理。</p>
<p>Server端Scan框架体系：一次scan可能会同时扫描一张表的多个Region，客户端会根据hbase:meta元数据将扫描的起始区间[startKey, stopKey)进行切分，切分成多个互相独立的查询子区间，每个子区间对应一个Region，RegionServer接收到客户端的get/scan请求之后做了两件事情：首先构建scanner iterator体系；然后执行next函数获取KeyValue，并对其进行条件过滤。</p>
<p>过滤淘汰不符合查询条件的HFile：经过Scanner体系的构建，KeyValue此时已经可以由小到大依次经过KeyValueScanner获得，但这些KeyValue是否满足用户设定的TimeRange条件、版本号条件以及Filter条件还需要进一步的检查—&gt;根据KeyRange过滤；根据TimeRange过滤；根据布隆过滤器进行过滤</p>
<p>从HFile中读取待查找Key：根据HFile索引树定位目标Block；BlockCache中检索目标Block； HDFS文件中检索目标Block；从Block中读取待查找KeyValue</p>
<h4 id="Compaction实现"><a href="#Compaction实现" class="headerlink" title="Compaction实现"></a>Compaction实现</h4><p>我们知道HFile小文件过多会导致读取效率过低，所以Compaction应运而生，Compaction核心功能是将小文件合并成大文件，提升读取效率</p>
<p><strong>Compaction工作原理</strong></p>
<p>Compaction是从Region的一个Store中选部分HFile来合并，合并原理：先从待合并文件中读出keyvalue，再由小到大排序成一个新文件，新文件代替之前文件对外提供服务</p>
<p>HBase中有两种Compaction：Minor Compaction和Major Compaction</p>
<p>Minor模式选取部分小的相邻的HFile，合成一个大的HFile，Major模式是将一个Store中所有的HFile合并成一个HFile，还会清理三种无意义数据(被删除数据，TTL过期数据，版本号超过设定版本号数据)，Major Compaction消耗资源大，所以通常手动触发</p>
<p>Compaction核心作用：合并小文件，减少文件数，稳定随机读延迟；提高数据的本地化率；清除无效数据，减小数据存储量，当然它的副作用是会消耗一定系统资源，影响上层业务的读取响应</p>
<p>compaction基本流程</p>
<p>HBase将该Compaction交给一个独立线程处理，该线程首先从对应Store中选择合适HFile文件进行合并。</p>
<p><img src="https://i.bmp.ovh/imgs/2022/03/b5b5c60918ac87f9.png"></p>
<p>​                                                                                <center>Compaction基本流程</center></p>
<p>Compaction触发时机：MemStore Flush；后台线程周期性检查及手动触发</p>
<p>HFile文件合并执行：(先选待合并HFile集合,再选合适处理线程,接着执行合并流程)</p>
<ol>
<li>分别读出待合并HFile文件的KeyValue，归并排序处理后写到./tmp目录下临时文件中</li>
<li>将临时文件移动到对应Store的数据目录</li>
<li>将Compaction的输入文件路径和输出文件路径封装为KV写入HLog日志,打Compaction标记，最后强制sync</li>
<li>将对应Store数据目录下的Compaction输入文件全部删除</li>
</ol>
<h4 id="负载均衡实现"><a href="#负载均衡实现" class="headerlink" title="负载均衡实现"></a>负载均衡实现</h4><p><strong>Region迁移</strong></p>
<p>HBase系统中，分片迁移就是Region迁移，Region在迁移的过程中不需迁移实际数据，只需将读写服务迁移</p>
<p>迁移过程涉及到Region迁移过程涉及多种状态的改变；迁移过程中涉及Master、ZooKeeper（ZK）以及RegionServer等多个组件的相互协调。</p>
<p><img src="https://i.bmp.ovh/imgs/2022/03/d9e83ebcda3a28bd.png"></p>
<p>​                                                                    <center>Region状态</center></p>
<p>执行过程中，Region分为unassign阶段和assign阶段</p>
<p>unassign表示Region从源RegionServer上下线</p>
<p><img src="https://i.bmp.ovh/imgs/2022/03/f6bc751c1dc2bfff.png"></p>
<p>​                                                                            <center>Region unassign阶段</center></p>
<p>assign表示Region在目标RegionServer上上线</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00fd07wtfj30q00deab0.jpg"></p>
<p>​                                                                    <center>Region assign阶段</center></p>
<p>Region迁移操作伴随着Region状态的不断变迁：思考一下</p>
<p>为什么需要这些状态？</p>
<p>无论是unassign操作还是assign操作，都是由多个子操作组成，涉及多个组件的协调合作，只有通过记录Region状态才能知道当前unassign或者assign的进度，在异常发生后才能根据具体进度继续执行</p>
<p>如何管理这些状态？</p>
<p>Region的这些状态会存储在三个区域：meta表，Master内存，ZooKeeper的region-in-transition节点，并且作用不同，只有这三个状态保持一致，对应的Region才处于正常的工作状态</p>
<p><strong>Region合并</strong></p>
<p>在线合并功能将空闲Region与相邻的Region合并，减少集群中空闲Region的个数</p>
<p>合并流程涉及到以下操作：</p>
<ol>
<li>客户端发送merge请求给Master。</li>
<li>Master将待合并的所有Region都move到同一个RegionServer上。</li>
<li>Master发送merge请求给该RegionServer。</li>
<li>RegionServer启动一个本地事务执行merge操作。</li>
<li>merge操作将待合并的两个Region下线，并将两个Region的文件进行合并。</li>
<li>将这两个Region从hbase:meta中删除，并将新生成的Region添加到hbase:meta中。</li>
<li>将新生成的Region上线。</li>
</ol>
<figure class="highlight nginx"><table><tr><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">merge_region</span> <span class="hljs-string">&#x27;encoded_regionname&#x27;</span>,<span class="hljs-string">&#x27;encoded_regionname&#x27;</span><br></code></pre></td></tr></table></figure>

<p><strong>Region分裂</strong></p>
<p>Region分裂是HBase最核心的功能之一，是实现分布式可扩展性的基础。HBase中，Region分裂有多种触发策略可以配置，一旦触发，HBase会寻找分裂点，然后执行真正的分裂操作</p>
<p>Region分裂触发策略：</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00fbwjmt0j30pp06mjrt.jpg"></p>
<p>​                                                                            <center>分裂触发策略</center></p>
<p>满足Region分裂策略之后就会触发Region分裂。分裂被触发后的第一件事是寻找分裂点，HBase对于分裂点的定义为：整个Region中最大Store中的最大文件中最中心的一个Block的首个rowkey。如果定位到的rowkey是整个文件的首个rowkey或者最后一个rowkey，则认为没有分裂点</p>
<p>HBase将整个分裂过程包装成了一个事务，目的是保证分裂事务的原子性。整个分裂事务过程分为三个阶段：prepare、execute和rollback</p>
<p>prepare阶段：在内存中初始化两个子Region，具体生成两个HRegionInfo对象，包含tableName、regionName、startkey、endkey等。同时会生成一个transaction journal，这个对象用来记录分裂的进展</p>
<p>execute阶段：</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00g7f5hbgj30p20i6mzi.jpg"></p>
<p>​                                                                                    <center>分裂核心操作</center></p>
<p>rollback阶段：如果execute阶段出现异常，则执行rollback操作，为了实现回滚，整个分裂过程分为很多子阶段，回滚程序会根据当前进展到哪个子阶段清理对应的垃圾数据。</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h00g93t12hj30q80ag763.jpg"></p>
<p>​                                                                                <center>Region分裂子阶段</center></p>
<h4 id="宕机恢复原理"><a href="#宕机恢复原理" class="headerlink" title="宕机恢复原理"></a>宕机恢复原理</h4><p>常见故障分析(RegionServer)：Full GC异常；HDFS异常；HBase bug；机器宕机。</p>
<p>故障恢复基本原理：</p>
<p>Master：采用基本的热备方式来实现Master高可用，</p>
<p>RegionServer：RegionServer发生宕机，HBase马上会检测到，并在检测到宕机后将宕机RegionServer上所有Region重新分配到集群其他正常RegionServer，再根据HLog恢复丢失数据。</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h00g0bqfc9j30hv0bot9g.jpg"></p>
<p>​                                                            <center>RegionServer故障恢复</center></p>
<p>Master检测RegionServer宕机：</p>
<p>HBase使用ZooKeeper协助Master检测RegionServer宕机，使用ZooKeeper临时节点机制，RegionServer注册成临时节点之后，Master会watch在/rs节点上（该节点下的所有子节点一旦发生离线就会通知Master），这样一旦RegionServer发生宕机，RegionServer注册到/rs节点下的临时节点就会离线，这个消息会马上通知给Master，Master检测到RegionServer宕机。</p>
<p> 切分未持久化数据的HLog：</p>
<p>日志回放需要以Region为单元进行，一个Region一个Region地回放，回放之前首先需要将HLog按照Region进行分组，每个Region的日志数据合并放在一起，方便后面按照Region进行回放。这个分组合并过程称为HLog切分。</p>
<ol>
<li>LogSplitting策略：HBase最初阶段日志切分的整个过程都由Master控制执行</li>
</ol>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h00ghvcu7cj30t00chab6.jpg"></p>
<p>​                                                                                    <center>LogSplitting</center></p>
<ol start="2">
<li>Distributed Log Splitting：Log Splitting的分布式实现，借助Master和所有RegionServer的计算能力进行日志切分，其中Master是协调者，RegionServer是实际的工作者</li>
</ol>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00gjapb9rj30s9080q3e.jpg"></p>
<p>​                                                                            <center>Distributed Log Splitting</center></p>
<p>Distributed Log Replay：</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h00gk7vr1ij30q202fglu.jpg"></p>
<p>​                                                                            <center>Distributed Log Replay</center></p>
<h4 id="备份与恢复"><a href="#备份与恢复" class="headerlink" title="备份与恢复"></a>备份与恢复</h4><p>使用Snapshot在线备份。Snapshot备份以快照技术为基础原理，备份过程不需要拷贝任何数据，因此对当前集群几乎没有任何影响，备份速度非常快而且可以保证数据一致性，Snapshot是HBase非常核心的一个功能，使用在线Snapshot备份可以满足用户很多需求，比如增量备份和数据迁移。</p>
<p><strong>在线Snapshot备份与恢复的用法</strong></p>
<p>snapshot：可以为表打一个快照，但并不涉及数据移动</p>
<blockquote>
<p>snapshot ‘sourceTable’,’snapshotName’</p>
</blockquote>
<p>restore_snapshot：用于恢复指定快照,恢复过程会替代原有数据,将表还原到快照点,快照点之后所有更新将会丢失</p>
<blockquote>
<p>restore_snapshot ‘snatshotName’</p>
</blockquote>
<p>clone_snapshot，可以根据快照恢复出一个新表，恢复过程不涉及数据移动，可以在秒级完成</p>
<blockquote>
<p>clone_snapshot ‘snatshotName’,’tableName’</p>
</blockquote>
<p>ExportSnapshot，可以将A集群的快照数据迁移到B集群。ExportSnapshot是HDFS层面的操作，需使用MapReduce进行数据的并行迁移，因此需要在开启MapReduce的机器上进行迁移</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00h79o51tj30ou03e3z7.jpg"></p>
<p><strong>Snatshot原理</strong></p>
<p>Snatshot流程：将MemStore中的缓存数据f lush到文件中；为所有HFile文件分别新建引用指针，这些指针元数据就是Snapshot。</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h00h8k684kj30ps0bejsf.jpg"></p>
<p>​                                                                        <center>snatshot原理</center></p>
<p><strong>两阶段提交</strong></p>
<p>HBase使用两阶段提交（Two-Phase Commit，2PC）协议来保证Snapshot的分布式原子性</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h00haw7033j30ro0bcgmm.jpg"></p>
<p>​                                                                <center>两阶段提交原理</center></p>
<p><strong>Snapshot核心实现</strong></p>
<p> Region实现Snapshot</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h00hcn988gj30p70agq3u.jpg"></p>
<p>​                                                                <center>snatshot基本流程</center></p>
<blockquote>
<p>Region生成的Snapshot文件是临时文件，在/hbase/.hbase-snapshot/.tmp目录下，因为Snapshot过程通常特别快，所以很难看到单个Region生成的Snapshot文件。</p>
</blockquote>
<p>Master汇总所有Region Snapshot的结果：Master会在所有Region完成Snapshot之后执行一个汇总操作（consolidate），将所有regionsnapshot manifest汇总成一个单独manifest，汇总后的Snapshot文件可以在HDFS目录下看到的，路径为：/hbase/.hbase-snapshot/snapshotname/data.manifest。</p>
<p><strong>洞见clone_snapshot</strong></p>
<ol>
<li>预检查。确认当前目标表没有执行snapshot以及restore等操作，否则直接返回错误</li>
<li>在tmp文件夹下新建目标表目录并在表目录下新建.tabledesc文件，在该文件中写入表schema信息。</li>
<li>新建region目录。根据snapshot manifest中的信息新建Region相关目录以及HFile文件。</li>
<li>将表目录从tmp文件夹下移到HBase Root Location。</li>
<li>修改hbase:meta表，将克隆表的Region信息添加到hbase:meta表中，注意克隆表的Region名称和原数据表的Region名称并不相同（Region名称与table名称相关，table名不同，Region名则肯定不同）</li>
<li>将这些Region通过round-robin方式均匀分配到整个集群中，并在ZooKeeper上将克隆表的状态设置为enabled，正式对外提供服务。</li>
</ol>
<h4 id="面试题解"><a href="#面试题解" class="headerlink" title="面试题解"></a>面试题解</h4><p><strong>HDFS和HBase使用场景</strong></p>
<p>首先Hbase基于HDFS存储的</p>
<p>HDFS：一次写入多次读取，数据一致性，容错和恢复机制</p>
<p>HBase：瞬间写入量很大，数据需要长久保存，不适用于有join和多级索引及表关系复杂数据模型，大数据量且有快速随机访问的需求，业务场景简单</p>
<p><strong>HBase存储结构</strong></p>
<p>HBase表通过rowkey按照一定范围被分割成多个子表HRegion(一个HRegion超过256要分为2个)，由HRegionServer管理，HMaster管理HRegion</p>
<p>HRegion存取一个子表时会创建一个HRegion对象，对表的每个列簇创建一个store实例，每个store都有0或多个StoreFile对应，每个StoreFile对应一个HFile</p>
<p><strong>HBase RowKey设计原则</strong></p>
<p>长度原则：100 字节以内，8 的倍数最好，可能的情况下越短越好。</p>
<p>散列原则：高位散列，低位时间字段。避免热点问题。</p>
<p>唯一原则：分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一起</p>
<p><strong>HBase列簇设计</strong></p>
<p>原则：在合理范围内能尽量少的减少列簇就尽量减少列簇，因为列簇是共享 region 的，每个列簇数据相差太大导致查询效率低下</p>
<p>最优：将所有相关性很强的 key-value 都放在同一个列簇下，这样既能做到查 询效率最高，也能保持尽可能少的访问不同的磁盘文件。</p>
<p><strong>热点现象怎么解决</strong></p>
<p>某个小的时段内，对 HBase 的读写请求集中到极少数的 Region 上，导致这些 region 所在的 RegionServer 处理请求量骤增，负载量明显偏大，而其他的 RgionServer 明显空闲</p>
<p>热点发生在大量的 client 直接访问集群的一个或极少数个节点</p>
<ol>
<li>加盐：在 rowkey 的前面增加随机数，使得它和之前的 rowkey 的开头不同。加盐之后的 rowkey 就会根据随机生成的前缀分散到各个 region 上， 以避免热点</li>
<li>哈希：哈希可以使负载分散到整个集群，但是读却是可以预测的</li>
<li>反转：反转固定长度或者数字格式的 rowkey，可以使得 rowkey 中经常改变的部分（最没有意义的部分）放在前面。</li>
<li>时间戳反转</li>
<li>HBase建表预分区：创建 HBase 表时，就预先根据可能的 RowKey 划分出多 个 region 而不是默认的一个，从而可以将后续的读写操作负载均衡到不 同的 region 上，避免热点现象。</li>
</ol>
<p><strong>HBase compact（minor major）</strong></p>
<p>在 hbase 中每当有 memstore 数据 flush 到磁盘之后，就形成一个 storefile， 当 storeFile 的数量达到一定程度就需要将 storefile 文件来进行 compaction</p>
<ol>
<li>合并文件</li>
<li>清洗过期多余版本数据</li>
<li>提高读写数据的效率</li>
<li>minor用来部分文件的合并以及包括minVersion=0并且设置ttl的过期版本清理，不做删除数据，多版本数据清理</li>
<li>major对Region下的HStore下所有StoreFile执行合并，最终合并出一个文件</li>
</ol>
<p><font color="red">HBase中两种缓存机制BlockCache和MemStore</font></p>
<p><strong>MemStore：</strong></p>
<p>MemStore叫做写缓存；HBase执行写操作会先将数据写入MemStore，并顺序写入HLog；MemStore达到一定阈值将数据刷新到磁盘</p>
<p><strong>BlockCache</strong></p>
<p>BlockCache叫做读缓存；HBase会将查找的Block块缓存到BlockCache中，方便后续同一请求或临近数据查找请求，可直接从内存中获取</p>
<p><strong>HBase读数据</strong></p>
<ol>
<li>zookeeper存储了meta表的region信息，先从zookeeper中找到meta表region位置，读取meta表数据，meta表中存储了用户表的region信息</li>
<li>根据namespace，表名，rowkey在meta表中找到对应的region信息</li>
<li>找到这个region对应的regionserver</li>
<li>查找对应的region</li>
<li>先从MemStore中查找数据，如果没有数据，将从StoreFile中读取(读取之后将数据缓存到MemStore上)</li>
</ol>
<p><strong>HBase写数据</strong></p>
<ol>
<li><p>Zookeeper保存了meta表的region信息，从meta表获取相应的meta信息，找到对应meta表数据</p>
</li>
<li><p>根据namespace，表名，rowkey从meta表的数据中找到写入数据对应的region信息，找到对应的regionserver</p>
</li>
<li><p>数据分别写入到WAL(HLog)和MemStore上各一份</p>
</li>
<li><p>MemStore数据达到一定阈值后就会Flush成一个StoreFile文件，如果数据有丢失，可以从WAL中恢复</p>
</li>
<li><p>多个StoreFile文件数到一定值时，触发compact合并操作，合并为一个StoreFile，并进行版本的合并和数据删除</p>
</li>
<li><p>compact后，逐渐形成越来越大的StoreFile，将会触发Split操作，将当前的StoreFile 分割为两个，相当于把一个大的region切割成两个region</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop</title>
    <url>/2022/03/03/Hadoop/</url>
    <content><![CDATA[<p><strong>HDFS MapReduce YARN</strong></p>
<span id="more"></span>

<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h01kfgx6g2j30ud0gp0u5.jpg"></p>
<h4 id="HDFS概述"><a href="#HDFS概述" class="headerlink" title="HDFS概述"></a>HDFS概述</h4><p><strong>HDFS优缺点</strong></p>
<p>优点：高容错性，适合大规模数据处理，廉价成本</p>
<p>缺点：不适合低延时数据访问，无法高效对大量小文件存储，不支持并发写入，文件随即修改。</p>
<p><strong>HDFS组成架构</strong></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h011gaq5xej30te0lu76b.jpg"></p>
<p>NameNode:管理HDFS名称空间，配置副本策略，管理数据块，处理客户端请求</p>
<p>DataNode:存储实际数据块，执行数据块的读写操作，</p>
<p>Client:文件切分，与NameNode交互获取文件位置信息，与DataNode交互，读写数据</p>
<p>SecondaryNameNode:辅助NameNode为其分担工作量(定期合并FSImage和Edit Log)，辅助恢复NameNode</p>
<p><strong>元数据格式</strong></p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h0128e6g95j30nr0amjss.jpg"></p>
<p><strong>Hadoop文件块大小</strong>默认为128M，文件块太小，会增加寻址时间；文件块太大，从磁盘传输数据的时间将大于定位数据块起始位置所需的时间，导致程序处理数据慢</p>
<p><strong>机架感知(副本节点选择)</strong></p>
<ol>
<li>第一个副本在client所处的节点上。如果客户端在集群外，随机选一个</li>
<li>第二个副本和第一个副本位于相同机架，随机节点。</li>
<li>第三个副本位于不同机架，随机节点</li>
</ol>
<p><strong>常用命令</strong></p>
<blockquote>
<p>-moveFromLocal,-copyFromLocal,-put,-appendToFile,-copyToLocal,-get,-ls,-cat,-cp,-mkdir,-mv,-tail,-rm,-rm -r,-du,-setrep</p>
</blockquote>
<h4 id="HDFS读写流程"><a href="#HDFS读写流程" class="headerlink" title="HDFS读写流程"></a>HDFS读写流程</h4><p><strong>HDFS写数据</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h011xz364sj30gz0b1dgm.jpg"></p>
<ol>
<li>Client向NameNode请求上传文件</li>
<li>NameNode返回是否可以上传</li>
<li>Client请求第一个Block上传到那个DataNode</li>
<li>NameNode返回3个DataNode节点</li>
<li>Client请求datanode1上传数据，datanode1收到请求会继续调用datanode2，datanode2调用datanode3，将通信管道建立完</li>
<li>datanode1，2，3逐级应答Client</li>
<li>Client向datanode1上传第一个Block(Packet)，先从磁盘读数据放到本地进行缓存，datanode1收到一个Packet就会传给datanode2，datanode2传给datanode3，</li>
<li>当一个Block传输完，Client再次请求上传第二个Block</li>
</ol>
<p><strong>HDFS读数据</strong></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h011x3wtgbj30ku0c5q3y.jpg"></p>
<ol>
<li>Client向NameNode请求下载文件，NameNode查询元数据，定位文件块所在DataNode地址</li>
<li>从(先就近后随机)DataNode请求读取数据</li>
<li>DataNode传输数据给Client(Packet)</li>
<li>Client(Packet)接受，先本地缓存后写入目标文件</li>
</ol>
<h4 id="NN和2NN工作机制"><a href="#NN和2NN工作机制" class="headerlink" title="NN和2NN工作机制"></a>NN和2NN工作机制</h4><p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h0143iqerxj30qw0d8tby.jpg"></p>
<p>NameNode启动：</p>
<ol>
<li>首次启动格式化，创建Fsimage，Edits，之后启动load edits和fsimage到内存</li>
<li>Client对元数据进行增删改请求</li>
<li>NameNode记录操作日志</li>
<li>NameNode在内存中对数据增删改</li>
</ol>
<p>SecondaryNode工作：</p>
<ol>
<li>2NN询问NN是否要CheckPoint</li>
<li>2NN请求执行CheckPoint</li>
<li>NN滚动正在写的Edits，将滚动前的edits和fsimage拷贝到2NN</li>
<li>2NN加载edits和fsimage到内存，并进行合并，生成新的fsimage.checkpoint</li>
<li>拷贝fsimage.checkpoint到NN，NN将fsimage.checkpoint更名为fsimage</li>
</ol>
<p><strong>FSImage和Edit Log</strong></p>
<p>fsimage保存了最新的元数据检查点，包含了整个HDFS文件系统的所有目录和文件的信息。</p>
<p>editlog在NameNode已经启动情况下对HDFS进行的各种更新操作进行记录，HDFS客户端执行所有的写操作都会被记录到editlog中</p>
<p>namenode在被格式化后会在/data/tmp/dfs/name/current目录下产生相应文件：</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h0148rc90jj30ba02g3yg.jpg"></p>
<h4 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h4><p>​    一个数据块在DataNode上以文件形式存储在磁盘，分别是数据本身和元数据(数据块长度，数据块校验和，时间戳)</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h013jtur26j30cf022aa0.jpg"></p>
<ol>
<li>DataNode启动后向NameNode注册，通过后周期性(1h)向NameNode上报所有块信息</li>
<li>心跳3s/次，心跳返回结果带有NameNode给DataNode的命令，若超过10min没有收到某个DataNode的心跳，则认为节点已挂</li>
<li>集群运行中可以服役新节点或退役旧节点</li>
</ol>
<p><strong>Client获取块数据</strong></p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h013kyka07j31f70i0gnj.jpg"></p>
<pre><code class="hljs"> 1. 客户端向namenode发送查询请求
 2. namenode返回数据块所在的节点datanode
 3. 客户端在返回的节点中寻找相应的块数据
 4. 如果某个datanode挂点了，返回相应的副本中寻找
</code></pre>
<p><strong>HDFS保证数据完整性</strong></p>
<p>客户端向HDFS写数据：</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h013s4e4vqj31f70i0gne.jpg"></p>
<ol>
<li>假设客户端发送2KB的数据 </li>
<li>客户端会以字节的方式往datanode发送，所以客户端会计算发送的数据有多少个，而这个单位就是chunk（512字节）。</li>
<li>客户端可以计算出checksum值，checksum = 2KB/512B=4 </li>
<li>然后datanode接收客户端发送来的数据，每接收512B的数据，就让checksum的值+1 </li>
<li>最后比较客户端和datanade的checksum值</li>
</ol>
<p>DataNode读取Block块：</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h013wr7aaoj31f70i0di4.jpg"></p>
<ol>
<li>block创建时会初始checksum值 </li>
<li>DataNode每隔一段时间就会计算block新的checksum值，看block块是否已经丢失 </li>
<li>如果checksum和之前一样，则没丢失，和之前比出现了不一样，那就说明数据丢失（或者异常） </li>
<li>当发生异常的时候，DateNode会报告给NameNode，NameNode会发送一条命令，清除这个异常块，然后找到这个块对应的副本，将完整的副本复制给其他的DataNode节点</li>
</ol>
<h4 id="HA高可用"><a href="#HA高可用" class="headerlink" title="HA高可用"></a>HA高可用</h4><p>HA即高可用(7*24不间断服务)</p>
<p>实现高可用最关键的策略是消除单点故障，通过配置Active/Standby两个NameNodes实现集群中对NameNode的热备</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h012120smaj30hq0bfmy7.jpg"></p>
<p>Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode,只有主 NameNode 才能对外提供读写服务</p>
<p>ZKFailoverController（主备切换控制器，FC）：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换</p>
<p>Zookeeper 集群：为主备切换控制器提供主备选举支持；</p>
<p>共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。</p>
<p>DataNode 节点：主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。</p>
<h4 id="MapReduce概述"><a href="#MapReduce概述" class="headerlink" title="MapReduce概述"></a>MapReduce概述</h4><p>官方文档的解释是：Hadoop Map/Reduce是一个使用简易的软件框架，基于它写出来的应用程序能够运行在由上千个商用机器组成的大型集群上，并以一种可靠容错的方式并行处理上T级别的数据集。</p>
<p>一个Map/Reduce <em>作业（job）</em> 通常会把输入的数据集切分为若干独立的数据块，由 <em>map任务（task）</em>以完全并行的方式处理它们。框架会对map的输出先进行排序， 然后把结果输入给<em>reduce任务</em>。通常作业的输入和输出都会被存储在文件系统中。 整个框架负责任务的调度和监控，以及重新执行已经失败的任务。</p>
<p>Map/Reduce框架由一个单独的master JobTracker 和每个集群节点一个slave TaskTracker共同组成。master负责调度构成一个作业的所有任务，这些任务分布在不同的slave上，master监控它们的执行，重新执行已经失败的任务。而slave仅负责执行由master指派的任务。</p>
<p><strong>MapReduce核心思想</strong></p>
<ol>
<li>分布式的计算通常要分为至少两个阶段</li>
<li>第一阶段的MapTask并发实例，完全并行运行</li>
<li>第二阶段的ReduceTask并发实例互不相干，但数据依赖于上个阶段所有MapTask并发实例的输出</li>
<li>MapReduce编程模型只能包含一个Map和Reduce阶段</li>
</ol>
<h4 id="MapReduce架构"><a href="#MapReduce架构" class="headerlink" title="MapReduce架构"></a>MapReduce架构</h4><p>MapReduce体系结构主要由四个部分组成，分别是：Client、JobTracker、TaskTracker以及Task</p>
<p>JobTracker是用于调度工作的，TaskTracker是用于执行工作的。<strong>一个Hadoop集群中只有一台JobTracker</strong><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h018835rytj30kc0cadgl.jpg"></p>
<p>客户端向JobTracker提交一个作业，JobTracker把作业拆分为多份分配给TaskTracker执行，TaskTracker每隔一段时间会向JobTracker发送Heartbeat，如果一段时间内JobTracker没有收到来至TaskTracker的Heartbeat，将认为TaskTracker挂掉了，会把该TaskTracker的作业分配给其他TaskTracker</p>
<h4 id="MapReduce执行过程"><a href="#MapReduce执行过程" class="headerlink" title="MapReduce执行过程"></a>MapReduce执行过程</h4><p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h018evc6mwj30jw0f1aav.jpg"></p>
<p>1、客户端启动一个job<br>2、向JobTracker请求一个JobID<br>3、将运行作业所需要的资源文件复制到HDFS上，包括MapReduce程序打包的JAR文件、配置文件和客户端计算所得的输入划分信息。这些文件都存放在JobTracker专门为该作业创建的文件夹中，文件夹名为该作业JobID。JAR文件默认会有10个副本，输入划分信息告诉JobTracker应该为这个作业启动多少个map任务等信息。<br>4、JobTracker接收到作业后将其放在作业队列中，等待JobTracker对其进行调度。当JobTracker根据自己的调度算法调度该作业时，会根据输入划分信息为每个划分创建一个map任务，并将map任务分配给TaskTracker执行。这里需要注意的是，map任务不是随便分配给某个TaskTracker的，Data-Local（数据本地化）将map任务分配给含有该map处理的数据库的TaskTracker上，同时将程序JAR包复制到该TaskTracker上运行，但是分配reducer任务时不考虑数据本地化。<br>5、TaskTracker每隔一段时间给JobTracker发送一个Heartbeat告诉JobTracker它仍然在运行，同时心跳还携带很多比如map任务完成的进度等信息。当JobTracker收到作业的最后一个任务完成信息时，便把作业设置成“成功”，JobClient再传达信息给用户。</p>
<p><strong>输入输出</strong></p>
<p>Map/Reduce框架运转在&lt;key, value&gt; 键值对上， 框架把作业的输入看为是一组&lt;key, value&gt; 键值对，同样也产出一组 &lt;key, value&gt; 键值对做为作业的输出，这两组键值对的类型可能不同。</p>
<blockquote>
<p>(input) &lt;k1, v1&gt; -&gt; <strong>map</strong> -&gt; &lt;k2, v2&gt; -&gt; <strong>combine</strong> -&gt; &lt;k2, v2&gt; -&gt; <strong>reduce</strong> -&gt; &lt;k3, v3&gt; (output)</p>
</blockquote>
<p><strong>MapReduce优缺点</strong></p>
<p>优点：易于编程，良好扩展性，高容错性，适合PB级以上海量数据离线处理</p>
<p>缺点：不擅长实时计算，不擅长流式计算，不擅长DAG计算</p>
<h4 id="Map和Reduce"><a href="#Map和Reduce" class="headerlink" title="Map和Reduce"></a>Map和Reduce</h4><p><strong>(input) -&gt;map-&gt; -&gt;combine-&gt; -&gt;reduce-&gt; (output)</strong></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h018j4wxuyj30kc0b60u1.jpg"></p>
<p><strong>map</strong>：map任务可细分四个阶段：record reader、mapper、combiner和partitioner。map任务的输出被称为中间键和中间值，会被发送到reducer做后续处理。</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h018kcz9j0j30k509q0tc.jpg"></p>
<ol>
<li>读取HDFS中的文件。每一行解析成一个&lt;k,v&gt;。每一个键值对调用一次map函数。&lt;0,helloyou&gt; &lt;10,hello me&gt;</li>
<li>覆盖map()，接收（1）中产生的&lt;k,v&gt;，进行处理，转换为新的&lt;k,v&gt;输出。&lt;hello,1&gt;&lt;you,1&gt;&lt;hello,1&gt;&lt;me,1&gt;</li>
<li>对（2）输出的&lt;k,v&gt;进行<strong>分区</strong>，默认分为一个区。</li>
<li>对不同分区中的数据进行<strong>按照Key排序、分组</strong>。分组指的是相同key的value放到一个集合中。排序后：&lt;hello,1&gt;&lt;hello,1&gt;&lt;me,1&gt; &lt;you,1&gt;，分组后：&lt;hello,{1,1}&gt;&lt;me,{1}&gt;&lt;you,{1}&gt;</li>
<li>对分组后的数据进行<strong>合并归约</strong></li>
</ol>
<p><strong>reduce</strong>：reduce任务也分为四个阶段：混排（shuffle）、排序（sort）、reducer和输出格式（output format）</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h018m68e0sj30k509q0tc.jpg"></p>
<ol>
<li>多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点上。（shuffle）</li>
<li>对多个map的输出进行<strong>合并、排序</strong>。覆盖reduce函数，接收的是分组后的数据，实现自己的业务逻辑，&lt;hello,2&gt;&lt;me,1&gt;&lt;you,1&gt;处理后，产生新的&lt;k,v&gt;输出。</li>
<li>对reduce输出的&lt;k,v&gt;写到HDFS中。</li>
</ol>
<h4 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a><strong>Shuffle机制</strong></h4><p>mapper 阶段处理的数据如何传递给 reducer 阶段，是 MapReduce 框架中 最关键的一个流程，就叫 Shuffle</p>
<p>Shuffle: 数据混洗（核心机制：数据分区，排序，局部聚合，缓存，拉取，再合并 排序）,具体说就是就是将 MapTask 输出的处理结果数据，按照 Partitioner 组件制定的规则分发 给 ReduceTask，并在分发的过程中，对数据按 key 进行了分区和排序</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h018q2xrp5j30p60djq4e.jpg"></p>
<ol>
<li>Collect阶段：MapTask的数据输出到环形缓冲区</li>
<li>Spill阶段：当内存中数据量到一定阈值，会将数据写入本地磁盘(将数据写入磁盘前需要对数据进行一次排序)，如果配置combiner会将相同分区号和key的数据进行排序</li>
<li>MapTask阶段的Merge：把所有溢出的临时文件进行一次merge，确保一个MapTask最终只产生一个中间数据文件</li>
<li>Copy阶段：ReduceTask启动Fetcher线程到已完成MapTask节点上copy一份数据</li>
<li>ReduceTask阶段的Merge：在ReduceTask复制数据同时，会在后台开启两个线程对内存到本地的数据文件进行merge</li>
<li>Sort阶段：对数据进行merge同时，会进行排序，由于MapTask阶段已经对数据进行了局部排序，ReduceTask只需保证Copy数据的最终整体有效性</li>
</ol>
<h4 id="YARN概述"><a href="#YARN概述" class="headerlink" title="YARN概述"></a>YARN概述</h4><p>是一个资源调度平台，负责为运算程序提供服务器运算资源</p>
<h4 id="YARN架构"><a href="#YARN架构" class="headerlink" title="YARN架构"></a>YARN架构</h4><p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h01gppj9o5j30pr0gb405.jpg"></p>
<p><strong>Container</strong>：Container是YARN对资源的抽象，封装了节点上多维度资源，像内存，cpu，磁盘，网络；容器由 NodeManager 启动和管理，并被它所监控；容器被 ResourceManager 进行调度。</p>
<p><strong>NodeManager</strong>：管理单个节点上的资源(负责启动和管理节点上的容器)；处理来自ResourceManager和ApplicationMaster的命令</p>
<p><strong>ResourceManager</strong>：处理客户端请求；监控NodeManager；启动和监控ApplicationMaster；资源的分配与调度；RM有定时调用器(Scheduler)和应用管理器(ApplicationManager)两个主要组件</p>
<p>定时调度器(Scheduler)：当 Client 提交一个任务的时候，它会根据所需要的资源以及当前集群的资源状况进行分配，只负责向应用程序分配资源，并不做监控以及应用程序的状态跟踪</p>
<p>应用管理器(ApplicationManager)：负责管理 Client 用户提交的应用</p>
<p><strong>ApplicationMaster</strong>：与 RM 调度器协商以获取资源（用 Container 表示）； 将得到的任务进一步分配给内部的任务；与 NM 通信以启动 / 停止任务；监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务。</p>
<h4 id="YARN工作机制"><a href="#YARN工作机制" class="headerlink" title="YARN工作机制"></a>YARN工作机制</h4><p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h01hap4w61j31d60u0k2i.jpg"></p>
<h4 id="作业提交流程"><a href="#作业提交流程" class="headerlink" title="作业提交流程"></a>作业提交流程</h4><p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h01h4jackzj30rs0rdn03.jpg"></p>
<ol>
<li>Client向Yarn提交Application，这里我们假设是一个MapReduce作业。</li>
<li>ResourceManager向NodeManager通信，为该Application分配第一个容器。并在这个容器中运行这个应用程序对应的ApplicationMaster。</li>
<li>ApplicationMaster启动以后，对作业（也就是Application）进行拆分，拆分task出来，这些task可以运行在一个或多个容器中。然后向ResourceManager申请要运行程序的容器，并定时向ResourceManager发送心跳。</li>
<li>申请到容器后，ApplicationMaster会去和容器对应的NodeManager通信，而后将作业分发到对应的NodeManager中的容器去运行，这里会将拆分后的MapReduce进行分发，对应容器中运行的可能是Map任务，也可能是Reduce任务。</li>
<li>容器中运行的任务会向ApplicationMaster发送心跳，汇报自身情况。当程序运行完成后，ApplicationMaster再向ResourceManager注销并释放容器资源。</li>
</ol>
<h4 id="YARN调度器"><a href="#YARN调度器" class="headerlink" title="YARN调度器"></a>YARN调度器</h4><p>FIFO 、Capacity Scheduler（容量调度器）和Fair Sceduler（公平调度器）</p>
<p>FIFO调度器：支持单队列 、先进先出  生产环境不会用。</p>
<p>容量调度器：支持多队列，保证先进入的任务优先执行。</p>
<p>公平调度器：支持多队列，保证每个任务公平享有队列资源。</p>
]]></content>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>数据留存的计算</title>
    <url>/2022/04/18/%E6%95%B0%E6%8D%AE%E7%95%99%E5%AD%98%E7%9A%84%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<p>留存率是用户在某段时间内开始使用网站/应用（一般定义是注册），一段时间后，仍然使用的人被认作是留存用户。</p>
<span id="more"></span>

<p><strong>留存率计算：</strong>留存率=登录用户数/新增用户数*100%<br>一般会计算如次日留存，三日留存，七日，30日等等</p>
<p>面试题：<br>手机中的相机是深受大家喜爱的应用之一，下图是某手机厂商数据库中的用户行为信息表中部分数据</p>
<p><img src="https://812105878-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MTuZ3PxkXf2a7UbRybW%2F-MV5itKjJAsDr-9780YA%2F-MV5jBBpMrcni_HLs6KU%2F%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E7%95%99%E5%AD%98%E7%8E%87_2.png?alt=media&token=4918360e-9591-4b67-ab3e-ae97ca2a18b4" alt="img"></p>
<p><img src="https://812105878-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MTuZ3PxkXf2a7UbRybW%2F-MV5itKjJAsDr-9780YA%2F-MV5jBBpMrcni_HLs6KU%2F%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E7%95%99%E5%AD%98%E7%8E%87_2.png?alt=media&token=4918360e-9591-4b67-ab3e-ae97ca2a18b4" alt="img"></p>
<p>用户 id：用户唯一标识；<br>应用名称：是手机中的某个应用，例如相机、微信、大众点评等。<br>启动时长：某一天中使用某应用多长时间（分钟）。<br>启动次数：某一天中启动了某应用多少次。<br>登陆时间：使用手机的日期。例如 2018-05-01。<br>现在该手机厂商想要分析手机中的应用（相机）的活跃情况，需统计如下数据：<br>某日活跃用户（用户 id）在后续的一周内的留存情况（计算次日留存用户数，3 日留存用户数，7 日留存用户数）<br>指标定义：<br>某日活跃用户数，某日活跃的去重用户数。<br>N 日活跃用户数，某日活跃的用户数在之后的第 N 日活跃用户数。<br>N 日活跃留存率，N 日留存用户数 / 某日活跃用户数<br>例：登陆时间（20180501 日）去重用户数 10000，这批用户在 20180503 日仍有 7000 人活跃，则 3 日活跃留存率为 7000/10000=70%<br>所需获得结果格式如下：</p>
<p><img src="https://812105878-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MTuZ3PxkXf2a7UbRybW%2F-MV5itKjJAsDr-9780YA%2F-MV5jEe_MsEI4JTk58qv%2F%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E7%95%99%E5%AD%98%E7%8E%87_3.webp?alt=media&token=68aaf526-4ec7-4def-94b8-f80a03d933e6" alt="img"></p>
<p>思路：该业务分析要求查询结果中包括：日期（说明是按每天来汇总数据）、用户活跃数、N 日留存数、N 日留存率。<br><strong>1. 每天的活跃用户数</strong><br>先来分析出活跃用户数</p>
<p><img src="https://812105878-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MTuZ3PxkXf2a7UbRybW%2F-MV5itKjJAsDr-9780YA%2F-MV5jRf1OoSMX0UWsEHG%2F%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E7%95%99%E5%AD%98%E7%8E%87_5.png?alt=media&token=6e95a292-b116-48a9-981d-c354c637b14d" alt="img"></p>
<p>活跃用户数对应的日期，表示每一行记录的是当天的活跃用户数。<br>按每天（登陆时间）分组（group by ），统计应用（相机）每天的活跃用户数（计数函数 count）。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> 登陆时间,<span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> 用户id) <span class="hljs-keyword">as</span> 活跃用户数 <br><span class="hljs-keyword">from</span> 用户行为信息表 <br><span class="hljs-keyword">where</span> 应用名称 <span class="hljs-operator">=</span><span class="hljs-string">&#x27;相机&#x27;</span> <br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> 登陆时间;<br></code></pre></td></tr></table></figure>

<p>查询结果如下：</p>
<p><img src="https://812105878-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MTuZ3PxkXf2a7UbRybW%2F-MV5itKjJAsDr-9780YA%2F-MV5jWKT6bo3WWrwrgik%2F%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E7%95%99%E5%AD%98%E7%8E%87_6.png?alt=media&token=c230febb-0abd-4c78-9f6e-7d7f7489296e" alt="img"></p>
<p><strong>2. 次日留存用户数</strong><br>再来看查询结果中的次日留存用户数</p>
<p><img src="https://812105878-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MTuZ3PxkXf2a7UbRybW%2F-MV5itKjJAsDr-9780YA%2F-MV5jZpIxMc0pWli8WRq%2F%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E7%95%99%E5%AD%98%E7%8E%87_7.png?alt=media&token=71509662-01e2-4fa7-82a8-6124baa407e7" alt="img"></p>
<p>次日留存用户数：在今日登录，明天也有登录的用户数。也就是<strong>时间间隔 = 1</strong>。一个表如果涉及到时间间隔，就需要用到自联结，也就是将两个相同的表进行联结。</p>
<p><img src="https://812105878-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MTuZ3PxkXf2a7UbRybW%2F-MV5itKjJAsDr-9780YA%2F-MV5jcI7WG--4sPOOpih%2F%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E7%95%99%E5%AD%98%E7%8E%87_8.png?alt=media&token=56933a97-80cb-48e9-8fbb-844e05d51402" alt="img"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> a.用户id,a.登陆时间,b.登陆时间<br><span class="hljs-keyword">from</span> 用户行为信息表 <span class="hljs-keyword">as</span> a  <br><span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> 用户行为信息表 <span class="hljs-keyword">as</span> b<br><span class="hljs-keyword">on</span> a.用户id <span class="hljs-operator">=</span> b.用户id<br><span class="hljs-keyword">where</span> a.应用名称<span class="hljs-operator">=</span> <span class="hljs-string">&#x27;相机&#x27;</span>;<br></code></pre></td></tr></table></figure>

<p>联结后的临时表记为表 c，那么如何从表 c 中查找出时间间隔（明天登陆时间 - 今天登陆时间）=1 的数据呢？这涉及到计算两个日期之间的差值,，timestampdiff</p>
<p><img src="https://812105878-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MTuZ3PxkXf2a7UbRybW%2F-MV5itKjJAsDr-9780YA%2F-MV5jhdoTd5EsStBZYWg%2F%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E7%95%99%E5%AD%98%E7%8E%87_9.png?alt=media&token=2ea534cd-18e9-467b-b1e5-1216a898a505" alt="img"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span>,timestampdiff(<span class="hljs-keyword">day</span>,a.登陆时间,b.登陆时间) <span class="hljs-keyword">as</span> 时间间隔<br><span class="hljs-keyword">from</span> c;<br></code></pre></td></tr></table></figure>

<p>用 case 语句选出时间间隔 = 1 的数据，并计数就是次日留存用户数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> <span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> 时间间隔<span class="hljs-operator">=</span><span class="hljs-number">1</span> <span class="hljs-keyword">then</span> 用户id<br>     <span class="hljs-keyword">else</span> <span class="hljs-keyword">null</span><br>     <span class="hljs-keyword">end</span>) <span class="hljs-keyword">as</span>  次日留存数<br></code></pre></td></tr></table></figure>

<p>代入上面的 sql 就是：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span>,<span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> <span class="hljs-keyword">when</span> 时间间隔<span class="hljs-operator">=</span><span class="hljs-number">1</span> <span class="hljs-keyword">then</span> 用户id<br>     <span class="hljs-keyword">else</span> <span class="hljs-keyword">null</span><br>     <span class="hljs-keyword">end</span>) <span class="hljs-keyword">as</span>  次日留存数<br> <span class="hljs-keyword">from</span><br>(<span class="hljs-keyword">select</span> <span class="hljs-operator">*</span>,timestampdiff(<span class="hljs-keyword">day</span>,a.登陆时间,b.登陆时间) <span class="hljs-keyword">as</span> 时间间隔<br><span class="hljs-keyword">from</span> c);<br></code></pre></td></tr></table></figure>

<p>将临时表 c 的 sql 代入上面就得到了查询结果如下：</p>
<p><img src="https://812105878-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MTuZ3PxkXf2a7UbRybW%2F-MV5itKjJAsDr-9780YA%2F-MV5jnrxpisHutAXFXjw%2F%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E7%95%99%E5%AD%98%E7%8E%87_10.png?alt=media&token=603ec06e-cebd-42aa-b0e5-fee8e0b2a8dd" alt="img"></p>
<p><strong>3. 次日留存率</strong></p>
<p><img src="https://812105878-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MTuZ3PxkXf2a7UbRybW%2F-MV5itKjJAsDr-9780YA%2F-MV5jt2KganCqLZHX_NI%2F%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E7%95%99%E5%AD%98%E7%8E%87_11.png?alt=media&token=87be94e1-77a5-4526-9d33-fcb8bb00a9dc" alt="img"></p>
<p>留存率 = 新增用户中登录用户数 / 新增用户数，所以次日留存率 = 次日留存用户数 / 当日用户活跃数当日活跃用户数是 <code>count(distinct 用户 id)</code><br>在上面分析次日留存数中，用次日留存用户数 / 当日用户活跃数就是次日留存率</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span>,<span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> <span class="hljs-keyword">when</span> 时间间隔<span class="hljs-operator">=</span><span class="hljs-number">1</span> <span class="hljs-keyword">then</span> 用户id<br>     <span class="hljs-keyword">else</span> <span class="hljs-keyword">null</span><br>     <span class="hljs-keyword">end</span>) <span class="hljs-keyword">as</span>  次日留存数 <span class="hljs-operator">/</span> <span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> 用户id) <span class="hljs-keyword">as</span> 次日留存率<br> <span class="hljs-keyword">from</span><br>(<span class="hljs-keyword">select</span> <span class="hljs-operator">*</span>,timestampdiff(<span class="hljs-keyword">day</span>,a.登陆时间,b.登陆时间) <span class="hljs-keyword">as</span> 时间间隔<br><span class="hljs-keyword">from</span> c);<br></code></pre></td></tr></table></figure>

<p><img src="https://812105878-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MTuZ3PxkXf2a7UbRybW%2F-MV5itKjJAsDr-9780YA%2F-MV5jwyMXxCIe3p13nQq%2F%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E7%95%99%E5%AD%98%E7%8E%87_12.png?alt=media&token=66d62f0a-b9f9-4c6c-8506-eb9c7e161284" alt="img"></p>
<p>将临时表 c 的 sql 代入就是：</p>
<p><img src="https://812105878-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MTuZ3PxkXf2a7UbRybW%2F-MV5itKjJAsDr-9780YA%2F-MV5k36U212rul95IbmQ%2F%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E7%95%99%E5%AD%98%E7%8E%87_13.png?alt=media&token=85be843c-72f0-420a-9fed-5eb69a5fde7d" alt="img"></p>
<p>查询结果：</p>
<p><img src="https://812105878-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MTuZ3PxkXf2a7UbRybW%2F-MV5itKjJAsDr-9780YA%2F-MV5k76jpbVXOUlXjbl6%2F%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E7%95%99%E5%AD%98%E7%8E%87_14.webp?alt=media&token=22d75985-3eeb-4425-aa49-8bf5fa68b9da" alt="img"></p>
<p>查询结果：</p>
<p><img src="https://812105878-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MTuZ3PxkXf2a7UbRybW%2F-MV5itKjJAsDr-9780YA%2F-MV5k76jpbVXOUlXjbl6%2F%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E7%95%99%E5%AD%98%E7%8E%87_14.webp?alt=media&token=22d75985-3eeb-4425-aa49-8bf5fa68b9da" alt="img"></p>
<p><strong>4. 三日的留存数，三日留存率, 七日的留存数, 七日留存率</strong></p>
<p><img src="https://812105878-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MTuZ3PxkXf2a7UbRybW%2F-MV5itKjJAsDr-9780YA%2F-MV5kBM3IhsnEN8AZQiT%2F%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E7%95%99%E5%AD%98%E7%8E%87_15.webp?alt=media&token=c97e8d0f-04b3-42c0-aa94-b02998d5dc42" alt="img"></p>
<p>和次日留存用户数，次日留存率分析思路一样，只需要更改时间间隔 =N（日留存）即可。</p>
<p>最终 sql 代码如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> a.登陆时间,<span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> a.用户id) <span class="hljs-keyword">as</span> 活跃用户数,<br><span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> <span class="hljs-keyword">when</span> 时间间隔<span class="hljs-operator">=</span><span class="hljs-number">1</span> <span class="hljs-keyword">then</span> 用户id <span class="hljs-keyword">else</span> <span class="hljs-keyword">null</span> <span class="hljs-keyword">end</span>) <span class="hljs-keyword">as</span>  次日留存数,<br><span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> <span class="hljs-keyword">when</span> 时间间隔<span class="hljs-operator">=</span><span class="hljs-number">1</span> <span class="hljs-keyword">then</span> 用户id <span class="hljs-keyword">else</span> <span class="hljs-keyword">null</span> <span class="hljs-keyword">end</span>) <span class="hljs-keyword">as</span>  次日留存数 <span class="hljs-operator">/</span> <span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> a.用户id) <span class="hljs-keyword">as</span> 次日留存率,<br><span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> <span class="hljs-keyword">when</span> 时间间隔<span class="hljs-operator">=</span><span class="hljs-number">3</span> <span class="hljs-keyword">then</span> 用户id <span class="hljs-keyword">else</span> <span class="hljs-keyword">null</span> <span class="hljs-keyword">end</span>) <span class="hljs-keyword">as</span>  三日留存数,<br><span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> <span class="hljs-keyword">when</span> 时间间隔<span class="hljs-operator">=</span><span class="hljs-number">3</span> <span class="hljs-keyword">then</span> 用户id <span class="hljs-keyword">else</span> <span class="hljs-keyword">null</span> <span class="hljs-keyword">end</span>) <span class="hljs-keyword">as</span>  三日留存数 <span class="hljs-operator">/</span> <span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> a.用户id) <span class="hljs-keyword">as</span> 三日留存率,<br><span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> <span class="hljs-keyword">when</span> 时间间隔<span class="hljs-operator">=</span><span class="hljs-number">7</span> <span class="hljs-keyword">then</span> 用户id <span class="hljs-keyword">else</span> <span class="hljs-keyword">null</span> <span class="hljs-keyword">end</span>) <span class="hljs-keyword">as</span>  七日留存数,<br><span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> <span class="hljs-keyword">when</span> 时间间隔<span class="hljs-operator">=</span><span class="hljs-number">7</span> <span class="hljs-keyword">then</span> 用户id <span class="hljs-keyword">else</span> <span class="hljs-keyword">null</span> <span class="hljs-keyword">end</span>) <span class="hljs-keyword">as</span>  七日留存数 <span class="hljs-operator">/</span> <span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> a.用户id) <span class="hljs-keyword">as</span> 七日留存率<br> <span class="hljs-keyword">from</span><br>(<span class="hljs-keyword">select</span> <span class="hljs-operator">*</span>,timestampdiff(<span class="hljs-keyword">day</span>,a.登陆时间,b.登陆时间) <span class="hljs-keyword">as</span> 时间间隔<br><span class="hljs-keyword">from</span> <br>(<span class="hljs-keyword">select</span> a.用户id,a.登陆时间,b.登陆时间<br><span class="hljs-keyword">from</span> 用户行为信息表 <span class="hljs-keyword">as</span> a  <br><span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> 用户行为信息表 <span class="hljs-keyword">as</span> b<br><span class="hljs-keyword">on</span> a.用户id <span class="hljs-operator">=</span> b.用户id<br><span class="hljs-keyword">where</span> a.应用名称<span class="hljs-operator">=</span> <span class="hljs-string">&#x27;相机&#x27;</span>) <span class="hljs-keyword">as</span> c<br>) <span class="hljs-keyword">as</span> d<br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> a.登陆时间;<br></code></pre></td></tr></table></figure>

<p>查询结果：</p>
<p><img src="https://812105878-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MTuZ3PxkXf2a7UbRybW%2F-MV5itKjJAsDr-9780YA%2F-MV5kG45e_d4iWEuQ6Uz%2F%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E7%95%99%E5%AD%98%E7%8E%87_16.webp?alt=media&token=ec39580c-4f69-45f8-b246-7cea045f2350" alt="img"></p>
]]></content>
      <tags>
        <tag>留存计算</tag>
      </tags>
  </entry>
  <entry>
    <title>每日一阅</title>
    <url>/2022/04/08/%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A/</url>
    <content><![CDATA[<p><strong>Java</strong>：</p>
<ul>
<li>java容器</li>
<li>hashmap</li>
<li>arraylist</li>
<li>jvm垃圾回收</li>
<li>jvm内存模型</li>
</ul>
<p>Mysql</p>
<ul>
<li><p>b树和b+树</p>
</li>
<li><p>innodb引擎</p>
</li>
</ul>
<p>系统</p>
<ul>
<li>进程和线程</li>
</ul>
<p>网络</p>
<ul>
<li>http和https的区别</li>
<li>三次握手和四次挥手</li>
<li>计算机网络七层模型</li>
<li>tcp和udp协议</li>
</ul>
<p>hadoop</p>
<ul>
<li>hadoop常用端口号</li>
<li>hdfs读写流程</li>
<li>hdfs小文件处理</li>
<li>shuffle优化</li>
<li>yarn工作机制</li>
<li>yarn调度器</li>
<li>hadoop解决数据倾斜问题</li>
<li>hadoop宕机</li>
</ul>
<p>zookeeper</p>
<ul>
<li>leader选举机制</li>
<li>常用命令</li>
</ul>
<p>Kafka</p>
<ul>
<li>kafka架构</li>
<li>kafka副本数</li>
<li>kafka日志保存时间</li>
<li>kafka分区数</li>
<li>kafka有多少个topic</li>
<li>kafka的ISR副本同步机制</li>
<li>kafka分区分配策略</li>
<li>kafka挂掉</li>
<li>kafka消息不丢失</li>
<li>kafka消息消费重复</li>
<li>kafka消息数据积压，kafka消费能力不足怎么办</li>
<li>kafka保证数据有序性</li>
</ul>
<p>hive</p>
<ul>
<li><p>hive的架构</p>
</li>
<li><p>hive和传统数据的区别</p>
</li>
<li><p>内部表和外部表</p>
</li>
<li><p>udf，udtf函数</p>
</li>
<li><p>窗口函数</p>
</li>
<li><p>hive优化</p>
</li>
<li><p>hive解决数据倾斜问题</p>
</li>
<li><p>Order By，Sort By，Distrbute By，Cluster By区别</p>
</li>
</ul>
<p>hbase：</p>
<ul>
<li>hbase存储结构</li>
<li>rowkey设计原则</li>
<li>简单的操作命令</li>
<li>hbase读写流程</li>
</ul>
<p>spark</p>
<ul>
<li>spark部署方式</li>
<li>spark作业提交流程</li>
<li>spark中的lineage(rdd)</li>
<li>spark的宽窄依赖，spark如何划分stage，每个stage根据什么决定task数</li>
<li>spark的行动和转换算子</li>
<li>简述Spark的两种核心Shuffle（HashShuffle与SortShuffle）的工作流程（包括未优化的HashShuffle、优化的HashShuffle、普通的SortShuffle与bypass的SortShuffle）</li>
<li>Spark常用算子reduceByKey与groupByKey的区别，哪一种更具优势？</li>
<li>分别简述Spark中的缓存机制（cache和persist）与checkpoint机制，并指出两者的区别与联系</li>
<li>简述Spark中共享变量（广播变量和累加器）的基本原理与用途。</li>
<li>当Spark涉及到数据库的操作时，如何减少Spark运行中的数据库连接数？</li>
<li>如何使用Spark实现TopN的获取</li>
</ul>
]]></content>
      <tags>
        <tag>琢磨</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据常用的一些脚本</title>
    <url>/2022/03/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B8%B8%E7%94%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E8%84%9A%E6%9C%AC/</url>
    <content><![CDATA[<p>Zookeeper群起脚本：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="language-bash">!/bin/bash</span><br><br>case $1 in<br>&quot;start&quot;)&#123;<br>	for i in hadoop102 hadoop103 hadoop104<br>	do<br>        echo ---------- zookeeper $i 启动 ------------<br>		ssh $i &quot;/opt/module/zookeeper-3.5.7/bin/zkServer.sh start&quot;<br>	done<br>&#125;;;<br>&quot;stop&quot;)&#123;<br>	for i in hadoop102 hadoop103 hadoop104<br>	do<br>        echo ---------- zookeeper $i 停止 ------------    <br>		ssh $i &quot;/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop&quot;<br>	done<br>&#125;;;<br>&quot;status&quot;)&#123;<br>	for i in hadoop102 hadoop103 hadoop104<br>	do<br>        echo ---------- zookeeper $i 状态 ------------    <br>		ssh $i &quot;/opt/module/zookeeper-3.5.7/bin/zkServer.sh status&quot;<br>	done<br>&#125;;;<br>esac<br></code></pre></td></tr></table></figure>

<p>集群分发脚本：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="language-bash">!/bin/bash</span><br><span class="hljs-meta">#</span><span class="language-bash">1. 判断参数个数</span><br>if [ $# -lt 1 ]<br>then<br>  echo Not Enough Arguement!<br>  exit;<br>fi<br><span class="hljs-meta">#</span><span class="language-bash">2. 遍历集群所有机器</span><br>for host in hadoop102 hadoop103 hadoop104<br>do<br>  echo ====================  $host  ====================<br><span class="hljs-meta">  #</span><span class="language-bash">3. 遍历所有目录，挨个发送</span><br>  for file in $@<br>  do<br>    #4 判断文件是否存在<br>    if [ -e $file ]<br>    then<br>      #5. 获取父目录<br>      pdir=$(cd -P $(dirname $file); pwd)<br>      #6. 获取当前文件的名称<br>      fname=$(basename $file)<br>      ssh $host &quot;mkdir -p $pdir&quot;<br>      rsync -av $pdir/$fname $host:$pdir<br>    else<br>      echo $file does not exists!<br>    fi<br>  done<br>done<br></code></pre></td></tr></table></figure>

<p>Flume群起：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="language-bash">! /bin/bash</span><br><br>case $1 in<br>&quot;start&quot;)&#123;<br>        for i in hadoop102 hadoop103<br>        do<br>                echo &quot; --------启动 $i 采集flume-------&quot;<br>                ssh $i &quot;nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE &gt;/opt/module/flume/log1.txt 2&gt;&amp;1  &amp;&quot;<br>        done<br>&#125;;;	<br>&quot;stop&quot;)&#123;<br>        for i in hadoop102 hadoop103<br>        do<br>                echo &quot; --------停止 $i 采集flume-------&quot;<br>                ssh $i &quot;ps -ef | grep file-flume-kafka | grep -v grep |awk  &#x27;&#123;print \$2&#125;&#x27; | xargs -n1 kill -9 &quot;<br>        done<br><br>&#125;;;<br>esac	<br></code></pre></td></tr></table></figure>

<p>xcall.sh jps</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="language-bash">! /bin/bash</span><br> <br>for i in hadoop102 hadoop103 hadoop104<br>do<br>    echo --------- $i ----------<br>    ssh $i &quot;$*&quot;<br>done<br></code></pre></td></tr></table></figure>

<p>Kafka群起：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="language-bash">! /bin/bash</span><br>case $1 in<br>&quot;start&quot;)&#123;<br>    for i in hadoop102 hadoop103 hadoop104<br>    do<br>        echo &quot; --------启动 $i Kafka-------&quot;<br>        ssh $i &quot;/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties&quot;<br>    done<br>&#125;;;<br>&quot;stop&quot;)&#123;<br>    for i in hadoop102 hadoop103 hadoop104<br>    do<br>        echo &quot; --------停止 $i Kafka-------&quot;<br>        ssh $i &quot;/opt/module/kafka/bin/kafka-server-stop.sh stop&quot;<br>    done<br>&#125;;;<br>esac<br></code></pre></td></tr></table></figure>

<p>组件群起脚本：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="language-bash">!/bin/bash</span><br><br>case $1 in<br>&quot;start&quot;)&#123;<br>        echo ================== 启动 集群 ==================<br><br>        #启动 Zookeeper集群<br>        zk.sh start<br><br>        #启动 Hadoop集群<br>        hdp.sh start<br><br>        #启动 Kafka采集集群<br>        kf.sh start<br><br>        #启动 Flume采集集群<br>        f1.sh start<br><br>        #启动 Flume消费集群<br>        f2.sh start<br><br>        &#125;;;<br>&quot;stop&quot;)&#123;<br>        echo ================== 停止 集群 ==================<br><br>        #停止 Flume消费集群<br>        f2.sh stop<br><br>        #停止 Flume采集集群<br>        f1.sh stop<br><br>        #停止 Kafka采集集群<br>        kf.sh stop<br><br>        #停止 Hadoop集群<br>        hdp.sh stop<br><br>        #停止 Zookeeper集群<br>        zk.sh stop<br><br>&#125;;;<br>esac<br></code></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL</title>
    <url>/2022/03/25/MySQL/</url>
    <content><![CDATA[<h2 id="MySQL-基础"><a href="#MySQL-基础" class="headerlink" title="MySQL 基础"></a>MySQL 基础</h2><h3 id="关系型数据库介绍"><a href="#关系型数据库介绍" class="headerlink" title="关系型数据库介绍"></a>关系型数据库介绍</h3><p>顾名思义，关系型数据库就是一种建立在关系模型的基础上的数据库。关系模型表明了数据库中所存储的数据之间的联系（一对一、一对多、多对多）。</p>
<p>关系型数据库中，我们的数据都被存放在了各种表中（比如用户表），表中的每一行就存放着一条数据（比如一个用户的信息）。</p>
<p><img src="https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/java-guide-blog/5e3c1a71724a38245aa43b02_99bf70d46cc247be878de9d3a88f0c44.png"></p>
<p>大部分关系型数据库都使用 SQL 来操作数据库中的数据。并且，大部分关系型数据库都支持事务的四大特性(ACID)。</p>
<p><strong>有哪些常见的关系型数据库呢？</strong></p>
<p>MySQL、PostgreSQL、Oracle、SQL Server、SQLite（微信本地的聊天记录的存储就是用的 SQLite） ……。</p>
<h3 id="MySQL-介绍"><a href="#MySQL-介绍" class="headerlink" title="MySQL 介绍"></a>MySQL 介绍</h3><p><img src="https://img-blog.csdnimg.cn/20210327143351823.png"></p>
<p><strong>MySQL 是一种关系型数据库，主要用于持久化存储我们的系统中的一些数据比如用户信息。</strong></p>
<p>由于 MySQL 是开源免费并且比较成熟的数据库，因此，MySQL 被大量使用在各种系统中。任何人都可以在 GPL(General Public License) 的许可下下载并根据个性化的需要对其进行修改。MySQL 的默认端口号是<strong>3306</strong>。</p>
<h2 id="存储引擎"><a href="#存储引擎" class="headerlink" title="存储引擎"></a>存储引擎</h2><h3 id="存储引擎相关的命令"><a href="#存储引擎相关的命令" class="headerlink" title="存储引擎相关的命令"></a>存储引擎相关的命令</h3><p><strong>查看 MySQL 提供的所有存储引擎</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql">mysql<span class="hljs-operator">&gt;</span> <span class="hljs-keyword">show</span> engines;<br></code></pre></td></tr></table></figure>

<p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/mysql-engines.png" alt="查看MySQL提供的所有存储引擎"></p>
<p>从上图我们可以查看出 MySQL 当前默认的存储引擎是 InnoDB，并且在 5.7 版本所有的存储引擎中只有 InnoDB 是事务性存储引擎，也就是说只有 InnoDB 支持事务。</p>
<p><strong>查看 MySQL 当前默认的存储引擎</strong></p>
<p>我们也可以通过下面的命令查看默认的存储引擎。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql">mysql<span class="hljs-operator">&gt;</span> <span class="hljs-keyword">show</span> variables <span class="hljs-keyword">like</span> <span class="hljs-string">&#x27;%storage_engine%&#x27;</span>;<br></code></pre></td></tr></table></figure>

<p><strong>查看表的存储引擎</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">show</span> <span class="hljs-keyword">table</span> status <span class="hljs-keyword">like</span> &quot;table_name&quot; ;<br></code></pre></td></tr></table></figure>

<p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/%E6%9F%A5%E7%9C%8B%E8%A1%A8%E7%9A%84%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E.png" alt="查看表的存储引擎"></p>
<h3 id="MyISAM-和-InnoDB-的区别"><a href="#MyISAM-和-InnoDB-的区别" class="headerlink" title="MyISAM 和 InnoDB 的区别"></a>MyISAM 和 InnoDB 的区别</h3><p><img src="https://img-blog.csdnimg.cn/20210327145248960.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MzM3Mjcy,size_16,color_FFFFFF,t_70"></p>
<p>MySQL 5.5 之前，MyISAM 引擎是 MySQL 的默认存储引擎，可谓是风光一时。</p>
<p>虽然，MyISAM 的性能还行，各种特性也还不错（比如全文索引、压缩、空间函数等）。但是，MyISAM 不支持事务和行级锁，而且最大的缺陷就是崩溃后无法安全恢复。</p>
<p>5.5 版本之后，MySQL 引入了 InnoDB（事务性数据库引擎），MySQL 5.5 版本后默认的存储引擎为 InnoDB。小伙子，一定要记好这个 InnoDB ，你每次使用 MySQL 数据库都是用的这个存储引擎吧？</p>
<p>言归正传！咱们下面还是来简单对比一下两者：</p>
<p><strong>1.是否支持行级锁</strong></p>
<p>MyISAM 只有表级锁(table-level locking)，而 InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁。</p>
<p>也就说，MyISAM 一锁就是锁住了整张表，这在并发写的情况下是多么滴憨憨啊！这也是为什么 InnoDB 在并发写的时候，性能更牛皮了！</p>
<p><strong>2.是否支持事务</strong></p>
<p>MyISAM 不提供事务支持。</p>
<p>InnoDB 提供事务支持，具有提交(commit)和回滚(rollback)事务的能力。</p>
<p><strong>3.是否支持外键</strong></p>
<p>MyISAM 不支持，而 InnoDB 支持。</p>
<p>🌈 拓展一下：</p>
<p>一般我们也是不建议在数据库层面使用外键的，应用层面可以解决。不过，这样会对数据的一致性造成威胁。具体要不要使用外键还是要根据你的项目来决定。</p>
<p><strong>4.是否支持数据库异常崩溃后的安全恢复</strong></p>
<p>MyISAM 不支持，而 InnoDB 支持。</p>
<p>使用 InnoDB 的数据库在异常崩溃后，数据库重新启动的时候会保证数据库恢复到崩溃前的状态。这个恢复的过程依赖于 <code>redo log</code> 。</p>
<p>🌈 拓展一下：</p>
<ul>
<li>MySQL InnoDB 引擎使用 <strong>redo log(重做日志)</strong> 保证事务的<strong>持久性</strong>，使用 <strong>undo log(回滚日志)</strong> 来保证事务的<strong>原子性</strong>。</li>
<li>MySQL InnoDB 引擎通过 <strong>锁机制</strong>、<strong>MVCC</strong> 等手段来保证事务的隔离性（ 默认支持的隔离级别是 <strong><code>REPEATABLE-READ</code></strong> ）。</li>
<li>保证了事务的持久性、原子性、隔离性之后，一致性才能得到保障。</li>
</ul>
<p><strong>5.是否支持 MVCC</strong></p>
<p>MyISAM 不支持，而 InnoDB 支持。</p>
<p>讲真，这个对比有点废话，毕竟 MyISAM 连行级锁都不支持。</p>
<p>MVCC 可以看作是行级锁的一个升级，可以有效减少加锁操作，提高性能。</p>
<h3 id="关于-MyISAM-和-InnoDB-的选择问题"><a href="#关于-MyISAM-和-InnoDB-的选择问题" class="headerlink" title="关于 MyISAM 和 InnoDB 的选择问题"></a>关于 MyISAM 和 InnoDB 的选择问题</h3><p>大多数时候我们使用的都是 InnoDB 存储引擎，在某些读密集的情况下，使用 MyISAM 也是合适的。不过，前提是你的项目不介意 MyISAM 不支持事务、崩溃恢复等缺点（可是~我们一般都会介意啊！）。</p>
<p>《MySQL 高性能》上面有一句话这样写到:</p>
<blockquote>
<p>不要轻易相信“MyISAM 比 InnoDB 快”之类的经验之谈，这个结论往往不是绝对的。在很多我们已知场景中，InnoDB 的速度都可以让 MyISAM 望尘莫及，尤其是用到了聚簇索引，或者需要访问的数据都可以放入内存的应用。</p>
</blockquote>
<p>一般情况下我们选择 InnoDB 都是没有问题的，但是某些情况下你并不在乎可扩展能力和并发能力，也不需要事务支持，也不在乎崩溃后的安全恢复问题的话，选择 MyISAM 也是一个不错的选择。但是一般情况下，我们都是需要考虑到这些问题的。</p>
<p>因此，对于咱们日常开发的业务系统来说，你几乎找不到什么理由再使用 MyISAM 作为自己的 MySQL 数据库的存储引擎。</p>
<h2 id="锁机制与-InnoDB-锁算法"><a href="#锁机制与-InnoDB-锁算法" class="headerlink" title="锁机制与 InnoDB 锁算法"></a>锁机制与 InnoDB 锁算法</h2><p><strong>MyISAM 和 InnoDB 存储引擎使用的锁：</strong></p>
<ul>
<li>MyISAM 采用表级锁(table-level locking)。</li>
<li>InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁</li>
</ul>
<p><strong>表级锁和行级锁对比：</strong></p>
<ul>
<li><strong>表级锁：</strong> MySQL 中锁定 <strong>粒度最大</strong> 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM 和 InnoDB 引擎都支持表级锁。</li>
<li><strong>行级锁：</strong> MySQL 中锁定 <strong>粒度最小</strong> 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。</li>
</ul>
<p><strong>InnoDB 存储引擎的锁的算法有三种：</strong></p>
<ul>
<li>Record lock：记录锁，单个行记录上的锁</li>
<li>Gap lock：间隙锁，锁定一个范围，不包括记录本身</li>
<li>Next-key lock：record+gap 临键锁，锁定一个范围，包含记录本身</li>
</ul>
<h2 id="查询缓存"><a href="#查询缓存" class="headerlink" title="查询缓存"></a>查询缓存</h2><p>执行查询语句的时候，会先查询缓存。不过，MySQL 8.0 版本后移除，因为这个功能不太实用</p>
<p><code>my.cnf</code> 加入以下配置，重启 MySQL 开启查询缓存</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><code class="hljs properties"><span class="hljs-attr">query_cache_type</span>=<span class="hljs-string">1</span><br><span class="hljs-attr">query_cache_size</span>=<span class="hljs-string">600000</span><br></code></pre></td></tr></table></figure>

<p>MySQL 执行以下命令也可以开启查询缓存</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><code class="hljs properties"><span class="hljs-attr">set</span> <span class="hljs-string">global  query_cache_type=1;</span><br><span class="hljs-attr">set</span> <span class="hljs-string">global  query_cache_size=600000;</span><br></code></pre></td></tr></table></figure>

<p>如上，<strong>开启查询缓存后在同样的查询条件以及数据情况下，会直接在缓存中返回结果</strong>。这里的查询条件包括查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息。（<strong>查询缓存不命中的情况：（1）</strong>）因此任何两个查询在任何字符上的不同都会导致缓存不命中。此外，（<strong>查询缓存不命中的情况：（2）</strong>）如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、MySQL 库中的系统表，其查询结果也不会被缓存。</p>
<p>（<strong>查询缓存不命中的情况：（3）</strong>）<strong>缓存建立之后</strong>，MySQL 的查询缓存系统会跟踪查询中涉及的每张表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。</p>
<p><strong>缓存虽然能够提升数据库的查询性能，但是缓存同时也带来了额外的开销，每次查询后都要做一次缓存操作，失效后还要销毁。</strong> 因此，开启查询缓存要谨慎，尤其对于写密集的应用来说更是如此。如果开启，要注意合理控制缓存空间大小，一般来说其大小设置为几十 MB 比较合适。此外，<strong>还可以通过 sql_cache 和 sql_no_cache 来控制某个查询语句是否需要缓存：</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> sql_no_cache <span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>) <span class="hljs-keyword">from</span> usr;<br></code></pre></td></tr></table></figure>

<h2 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h2><h3 id="何为事务？"><a href="#何为事务？" class="headerlink" title="何为事务？"></a>何为事务？</h3><p>一言蔽之，<strong>事务是逻辑上的一组操作，要么都执行，要么都不执行。</strong></p>
<p><strong>可以简单举一个例子不？</strong></p>
<p>事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账 1000 元，这个转账会涉及到两个关键操作就是：</p>
<ol>
<li>将小明的余额减少 1000 元</li>
<li>将小红的余额增加 1000 元。</li>
</ol>
<p>事务会把这两个操作就可以看成逻辑上的一个整体，这个整体包含的操作要么都成功，要么都要失败。</p>
<p>这样就不会出现小明余额减少而小红的余额却并没有增加的情况。</p>
<h3 id="何为数据库事务？"><a href="#何为数据库事务？" class="headerlink" title="何为数据库事务？"></a>何为数据库事务？</h3><p>数据库事务在我们日常开发中接触的最多了。如果你的项目属于单体架构的话，你接触到的往往就是数据库事务了。</p>
<p>平时，我们在谈论事务的时候，如果没有特指<strong>分布式事务</strong>，往往指的就是<strong>数据库事务</strong>。</p>
<p><strong>那数据库事务有什么作用呢？</strong></p>
<p>简单来说：数据库事务可以保证多个对数据库的操作（也就是 SQL 语句）构成一个逻辑上的整体。构成这个逻辑上的整体的这些数据库操作遵循：<strong>要么全部执行成功,要么全部不执行</strong> 。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"># 开启一个事务<br><span class="hljs-keyword">START</span> TRANSACTION;<br># 多条 <span class="hljs-keyword">SQL</span> 语句<br>SQL1,SQL2...<br>## 提交事务<br><span class="hljs-keyword">COMMIT</span>;<br></code></pre></td></tr></table></figure>

<p><img src="https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/2020-12/640-20201207160554677.png"></p>
<p>另外，关系型数据库（例如：<code>MySQL</code>、<code>SQL Server</code>、<code>Oracle</code> 等）事务都有 <strong>ACID</strong> 特性：</p>
<p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/%E4%BA%8B%E5%8A%A1%E7%89%B9%E6%80%A7.png" alt="事务的特性"></p>
<h3 id="何为-ACID-特性呢？"><a href="#何为-ACID-特性呢？" class="headerlink" title="何为 ACID 特性呢？"></a>何为 ACID 特性呢？</h3><ol>
<li><strong>原子性</strong>（<code>Atomicity</code>） ： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；</li>
<li><strong>一致性</strong>（<code>Consistency</code>）： 执行事务前后，数据保持一致，例如转账业务中，无论事务是否成功，转账者和收款人的总额应该是不变的；</li>
<li><strong>隔离性</strong>（<code>Isolation</code>）： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；</li>
<li><strong>持久性</strong>（<code>Durability</code>）： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。</li>
</ol>
<p><strong>数据事务的实现原理呢？</strong></p>
<p>我们这里以 MySQL 的 InnoDB 引擎为例来简单说一下。</p>
<p>MySQL InnoDB 引擎使用 <strong>redo log(重做日志)</strong> 保证事务的<strong>持久性</strong>，使用 <strong>undo log(回滚日志)</strong> 来保证事务的<strong>原子性</strong>。</p>
<p>MySQL InnoDB 引擎通过 <strong>锁机制</strong>、<strong>MVCC</strong> 等手段来保证事务的隔离性（ 默认支持的隔离级别是 <strong><code>REPEATABLE-READ</code></strong> ）。</p>
<p>保证了事务的持久性、原子性、隔离性之后，一致性才能得到保障。</p>
<h3 id="并发事务带来哪些问题"><a href="#并发事务带来哪些问题" class="headerlink" title="并发事务带来哪些问题?"></a>并发事务带来哪些问题?</h3><p>在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。</p>
<ul>
<li><strong>脏读（Dirty read）:</strong> 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。</li>
<li><strong>丢失修改（Lost to modify）:</strong> 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务 1 读取某表中的数据 A=20，事务 2 也读取 A=20，事务 1 修改 A=A-1，事务 2 也修改 A=A-1，最终结果 A=19，事务 1 的修改被丢失。</li>
<li><strong>不可重复读（Unrepeatable read）:</strong> 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。</li>
<li><strong>幻读（Phantom read）:</strong> 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。</li>
</ul>
<p><strong>不可重复读和幻读区别：</strong></p>
<p>不可重复读的重点是修改比如多次读取一条记录发现其中某些列的值被修改，幻读的重点在于新增或者删除比如多次查询同一条查询语句（DQL）时，记录发现记录增多或减少了。</p>
<h3 id="事务隔离级别有哪些"><a href="#事务隔离级别有哪些" class="headerlink" title="事务隔离级别有哪些?"></a>事务隔离级别有哪些?</h3><p>SQL 标准定义了四个隔离级别：</p>
<ul>
<li><strong>READ-UNCOMMITTED(读取未提交)：</strong> 最低的隔离级别，允许读取尚未提交的数据变更，<strong>可能会导致脏读、幻读或不可重复读</strong>。</li>
<li><strong>READ-COMMITTED(读取已提交)：</strong> 允许读取并发事务已经提交的数据，<strong>可以阻止脏读，但是幻读或不可重复读仍有可能发生</strong>。</li>
<li><strong>REPEATABLE-READ(可重复读)：</strong> 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，<strong>可以阻止脏读和不可重复读，但幻读仍有可能发生</strong>。</li>
<li><strong>SERIALIZABLE(可串行化)：</strong> 最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，<strong>该级别可以防止脏读、不可重复读以及幻读</strong>。</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th align="center">隔离级别</th>
<th align="center">脏读</th>
<th align="center">不可重复读</th>
<th align="center">幻读</th>
</tr>
</thead>
<tbody><tr>
<td align="center">READ-UNCOMMITTED</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">√</td>
</tr>
<tr>
<td align="center">READ-COMMITTED</td>
<td align="center">×</td>
<td align="center">√</td>
<td align="center">√</td>
</tr>
<tr>
<td align="center">REPEATABLE-READ</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">√</td>
</tr>
<tr>
<td align="center">SERIALIZABLE</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
</tbody></table>
<h3 id="MySQL-的默认隔离级别是什么"><a href="#MySQL-的默认隔离级别是什么" class="headerlink" title="MySQL 的默认隔离级别是什么?"></a>MySQL 的默认隔离级别是什么?</h3><p>MySQL InnoDB 存储引擎的默认支持的隔离级别是 <strong>REPEATABLE-READ（可重读）</strong>。我们可以通过<code>SELECT @@tx_isolation;</code>命令来查看，MySQL 8.0 该命令改为<code>SELECT @@transaction_isolation;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql">mysql<span class="hljs-operator">&gt;</span> <span class="hljs-keyword">SELECT</span> @<span class="hljs-variable">@tx</span>_isolation;<br><span class="hljs-operator">+</span><span class="hljs-comment">-----------------+</span><br><span class="hljs-operator">|</span> @<span class="hljs-variable">@tx</span>_isolation  <span class="hljs-operator">|</span><br><span class="hljs-operator">+</span><span class="hljs-comment">-----------------+</span><br><span class="hljs-operator">|</span> REPEATABLE<span class="hljs-operator">-</span>READ <span class="hljs-operator">|</span><br><span class="hljs-operator">+</span><span class="hljs-comment">-----------------+</span><br></code></pre></td></tr></table></figure>

<p><del>这里需要注意的是：与 SQL 标准不同的地方在于 InnoDB 存储引擎在 <strong>REPEATABLE-READ（可重读）</strong> 事务隔离级别下使用的是 Next-Key Lock 锁算法，因此可以避免幻读的产生，这与其他数据库系统(如 SQL Server)是不同的。所以说 InnoDB 存储引擎的默认支持的隔离级别是 <strong>REPEATABLE-READ（可重读）</strong> 已经可以完全保证事务的隔离性要求，即达到了 SQL 标准的 <strong>SERIALIZABLE(可串行化)</strong> 隔离级别。</del></p>
<p>🐛 问题更正：<strong>MySQL InnoDB 的 REPEATABLE-READ（可重读）并不保证避免幻读，需要应用使用加锁读来保证。而这个加锁读使用到的机制就是 Next-Key Locks。</strong></p>
<p>因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是 <strong>READ-COMMITTED(读取提交内容)</strong> ，但是你要知道的是 InnoDB 存储引擎默认使用 <strong>REPEATABLE-READ（可重读）</strong> 并不会有任何性能损失。</p>
<p>InnoDB 存储引擎在 <strong>分布式事务</strong> 的情况下一般会用到 <strong>SERIALIZABLE(可串行化)</strong> 隔离级别。</p>
<p>🌈 拓展一下(以下内容摘自《MySQL 技术内幕：InnoDB 存储引擎(第 2 版)》7.7 章)：</p>
<blockquote>
<p>InnoDB 存储引擎提供了对 XA 事务的支持，并通过 XA 事务来支持分布式事务的实现。分布式事务指的是允许多个独立的事务资源（transactional resources）参与到一个全局的事务中。事务资源通常是关系型数据库系统，但也可以是其他类型的资源。全局事务要求在其中的所有参与的事务要么都提交，要么都回滚，这对于事务原有的 ACID 要求又有了提高。另外，在使用分布式事务时，InnoDB 存储引擎的事务隔离级别必须设置为 SERIALIZABLE。</p>
</blockquote>
<blockquote>
<p>转载至javaGuide</p>
</blockquote>
]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>操作系统</title>
    <url>/2022/03/25/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<h2 id="一-操作系统基础"><a href="#一-操作系统基础" class="headerlink" title="一 操作系统基础"></a>一 操作系统基础</h2><h3 id="1-1-什么是操作系统？"><a href="#1-1-什么是操作系统？" class="headerlink" title="1.1 什么是操作系统？"></a>1.1 什么是操作系统？</h3><span id="more"></span>

<p>👨‍💻<strong>面试官</strong> ： 先来个简单问题吧！<strong>什么是操作系统？</strong></p>
<p>🙋 <strong>我</strong> ：我通过以下四点向您介绍一下什么是操作系统吧！</p>
<ol>
<li><strong>操作系统（Operating System，简称 OS）是管理计算机硬件与软件资源的程序，是计算机的基石。</strong></li>
<li><strong>操作系统本质上是一个运行在计算机上的软件程序 ，用于管理计算机硬件和软件资源。</strong> 举例：运行在你电脑上的所有应用程序都通过操作系统来调用系统内存以及磁盘等等硬件。</li>
<li><strong>操作系统存在屏蔽了硬件层的复杂性。</strong> 操作系统就像是硬件使用的负责人，统筹着各种相关事项。</li>
<li><strong>操作系统的内核（Kernel）是操作系统的核心部分，它负责系统的内存管理，硬件设备的管理，文件系统的管理以及应用程序的管理</strong>。 内核是连接应用程序和硬件的桥梁，决定着系统的性能和稳定性。</li>
</ol>
<p><img src="https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/2020-8/Kernel_Layout.png" alt="Kernel_Layout"></p>
<h3 id="1-2-系统调用"><a href="#1-2-系统调用" class="headerlink" title="1.2 系统调用"></a>1.2 系统调用</h3><p>👨‍💻<strong>面试官</strong> ：<strong>什么是系统调用呢？</strong> 能不能详细介绍一下。</p>
<p>🙋 <strong>我</strong> ：介绍系统调用之前，我们先来了解一下用户态和系统态。</p>
<p>根据进程访问资源的特点，我们可以把进程在系统上的运行分为两个级别：</p>
<ol>
<li>用户态(user mode) : 用户态运行的进程可以直接读取用户程序的数据。</li>
<li>系统态(kernel mode):可以简单的理解系统态运行的进程或程序几乎可以访问计算机的任何资源，不受限制。</li>
</ol>
<p>说了用户态和系统态之后，那么什么是系统调用呢？</p>
<p>我们运行的程序基本都是运行在用户态，如果我们调用操作系统提供的系统态级别的子功能咋办呢？那就需要系统调用了！</p>
<p>也就是说在我们运行的用户程序中，凡是与系统态级别的资源有关的操作（如文件管理、进程控制、内存管理等)，都必须通过系统调用方式向操作系统提出服务请求，并由操作系统代为完成。</p>
<p>这些系统调用按功能大致可分为如下几类：</p>
<ul>
<li>设备管理。完成设备的请求或释放，以及设备启动等功能。</li>
<li>文件管理。完成文件的读、写、创建及删除等功能。</li>
<li>进程控制。完成进程的创建、撤销、阻塞及唤醒等功能。</li>
<li>进程通信。完成进程之间的消息传递或信号传递等功能。</li>
<li>内存管理。完成内存的分配、回收以及获取作业占用内存区大小及地址等功能。</li>
</ul>
<h2 id="二-进程和线程"><a href="#二-进程和线程" class="headerlink" title="二 进程和线程"></a>二 进程和线程</h2><h3 id="2-1-进程和线程的区别"><a href="#2-1-进程和线程的区别" class="headerlink" title="2.1 进程和线程的区别"></a>2.1 进程和线程的区别</h3><p>👨‍💻<strong>面试官</strong>: 好的！我明白了！那你再说一下： <strong>进程和线程的区别</strong>。</p>
<p>🙋 <strong>我：</strong> 好的！ 下图是 Java 内存区域，我们从 JVM 的角度来说一下线程和进程之间的关系吧！</p>
<p><img src="https://oscimg.oschina.net/oscnet/up-cd8ac705f6f004c01e0a1312f1599430ba5.png"></p>
<p>从上图可以看出：一个进程中可以有多个线程，多个线程共享进程的<strong>堆</strong>和<strong>方法区 (JDK1.8 之后的元空间)<strong>资源，但是每个线程有自己的</strong>程序计数器</strong>、<strong>虚拟机栈</strong> 和 <strong>本地方法栈</strong>。</p>
<p><strong>总结：</strong> 线程是进程划分成的更小的运行单位,一个进程在其执行的过程中可以产生多个线程。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。线程执行开销小，但不利于资源的管理和保护；而进程正相反。</p>
<h3 id="2-2-进程有哪几种状态"><a href="#2-2-进程有哪几种状态" class="headerlink" title="2.2 进程有哪几种状态?"></a>2.2 进程有哪几种状态?</h3><p>👨‍💻<strong>面试官</strong> ： 那你再说说<strong>进程有哪几种状态?</strong></p>
<p>🙋 <strong>我</strong> ：我们一般把进程大致分为 5 种状态，这一点和<a href="https://github.com/Snailclimb/JavaGuide/blob/master/docs/java/Multithread/JavaConcurrencyBasicsCommonInterviewQuestionsSummary.md#6-%E8%AF%B4%E8%AF%B4%E7%BA%BF%E7%A8%8B%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E5%92%8C%E7%8A%B6%E6%80%81">线程</a>很像！</p>
<ul>
<li><strong>创建状态(new)</strong> ：进程正在被创建，尚未到就绪状态。</li>
<li><strong>就绪状态(ready)</strong> ：进程已处于准备运行状态，即进程获得了除了处理器之外的一切所需资源，一旦得到处理器资源(处理器分配的时间片)即可运行。</li>
<li><strong>运行状态(running)</strong> ：进程正在处理器上上运行(单核 CPU 下任意时刻只有一个进程处于运行状态)。</li>
<li><strong>阻塞状态(waiting)</strong> ：又称为等待状态，进程正在等待某一事件而暂停运行如等待某资源为可用或等待 IO 操作完成。即使处理器空闲，该进程也不能运行。</li>
<li><strong>结束状态(terminated)</strong> ：进程正在从系统中消失。可能是进程正常结束或其他原因中断退出运行。</li>
</ul>
<blockquote>
<p>订正：下图中 running 状态被 interrupt 向 ready 状态转换的箭头方向反了。</p>
</blockquote>
<p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/d38202593012b457debbcd74994c6292.png" alt="process-state"></p>
<h3 id="2-3-进程间的通信方式"><a href="#2-3-进程间的通信方式" class="headerlink" title="2.3 进程间的通信方式"></a>2.3 进程间的通信方式</h3><p>👨‍💻<strong>面试官</strong> ：<strong>进程间的通信常见的的有哪几种方式呢?</strong></p>
<p>🙋 <strong>我</strong> ：大概有 7 种常见的进程间的通信方式。</p>
<blockquote>
<p>下面这部分总结参考了:<a href="https://www.jianshu.com/p/c1015f5ffa74">《进程间通信 IPC (InterProcess Communication)》</a> 这篇文章，推荐阅读，总结的非常不错。</p>
</blockquote>
<ol>
<li><strong>管道/匿名管道(Pipes)</strong> ：用于具有亲缘关系的父子进程间或者兄弟进程之间的通信。</li>
<li><strong>有名管道(Names Pipes)</strong> : 匿名管道由于没有名字，只能用于亲缘关系的进程间通信。为了克服这个缺点，提出了有名管道。有名管道严格遵循**先进先出(first in first out)**。有名管道以磁盘文件的方式存在，可以实现本机任意两个进程通信。</li>
<li><strong>信号(Signal)</strong> ：信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生；</li>
<li><strong>消息队列(Message Queuing)</strong> ：消息队列是消息的链表,具有特定的格式,存放在内存中并由消息队列标识符标识。管道和消息队列的通信数据都是先进先出的原则。与管道（无名管道：只存在于内存中的文件；命名管道：存在于实际的磁盘介质或者文件系统）不同的是消息队列存放在内核中，只有在内核重启(即，操作系统重启)或者显式地删除一个消息队列时，该消息队列才会被真正的删除。消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取.比 FIFO 更有优势。<strong>消息队列克服了信号承载信息量少，管道只能承载无格式字 节流以及缓冲区大小受限等缺点。</strong></li>
<li><strong>信号量(Semaphores)</strong> ：信号量是一个计数器，用于多进程对共享数据的访问，信号量的意图在于进程间同步。这种通信方式主要用于解决与同步相关的问题并避免竞争条件。</li>
<li><strong>共享内存(Shared memory)</strong> ：使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据的更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等。可以说这是最有用的进程间通信方式。</li>
<li><strong>套接字(Sockets)</strong> : 此方法主要用于在客户端和服务器之间通过网络进行通信。套接字是支持 TCP/IP 的网络通信的基本操作单元，可以看做是不同主机之间的进程进行双向通信的端点，简单的说就是通信的两方的一种约定，用套接字中的相关函数来完成通信过程。</li>
</ol>
<h3 id="2-4-线程间的同步的方式"><a href="#2-4-线程间的同步的方式" class="headerlink" title="2.4 线程间的同步的方式"></a>2.4 线程间的同步的方式</h3><p>👨‍💻<strong>面试官</strong> ：<strong>那线程间的同步的方式有哪些呢?</strong></p>
<p>🙋 <strong>我</strong> ：线程同步是两个或多个共享关键资源的线程的并发执行。应该同步线程以避免关键的资源使用冲突。操作系统一般有下面三种线程同步的方式：</p>
<ol>
<li>**互斥量(Mutex)**：采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问。比如 Java 中的 synchronized 关键词和各种 Lock 都是这种机制。</li>
<li><strong>信号量(Semphares)</strong> ：它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量。</li>
<li><strong>事件(Event)</strong> :Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操作。</li>
</ol>
<h3 id="2-5-进程的调度算法"><a href="#2-5-进程的调度算法" class="headerlink" title="2.5 进程的调度算法"></a>2.5 进程的调度算法</h3><p>👨‍💻<strong>面试官</strong> ：<strong>你知道操作系统中进程的调度算法有哪些吗?</strong></p>
<p>🙋 <strong>我</strong> ：嗯嗯！这个我们大学的时候学过，是一个很重要的知识点！</p>
<p>为了确定首先执行哪个进程以及最后执行哪个进程以实现最大 CPU 利用率，计算机科学家已经定义了一些算法，它们是：</p>
<ul>
<li><strong>先到先服务(FCFS)调度算法</strong> : 从就绪队列中选择一个最先进入该队列的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度。</li>
<li><strong>短作业优先(SJF)的调度算法</strong> : 从就绪队列中选出一个估计运行时间最短的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度。</li>
<li><strong>时间片轮转调度算法</strong> : 时间片轮转调度是一种最古老，最简单，最公平且使用最广的算法，又称 RR(Round robin)调度。每个进程被分配一个时间段，称作它的时间片，即该进程允许运行的时间。</li>
<li><strong>多级反馈队列调度算法</strong> ：前面介绍的几种进程调度的算法都有一定的局限性。如<strong>短进程优先的调度算法，仅照顾了短进程而忽略了长进程</strong> 。多级反馈队列调度算法既能使高优先级的作业得到响应又能使短作业（进程）迅速完成。，因而它是目前<strong>被公认的一种较好的进程调度算法</strong>，UNIX 操作系统采取的便是这种调度算法。</li>
<li><strong>优先级调度</strong> ： 为每个流程分配优先级，首先执行具有最高优先级的进程，依此类推。具有相同优先级的进程以 FCFS 方式执行。可以根据内存要求，时间要求或任何其他资源要求来确定优先级。</li>
</ul>
<h3 id="2-6-什么是死锁"><a href="#2-6-什么是死锁" class="headerlink" title="2.6 什么是死锁"></a>2.6 什么是死锁</h3><p>👨‍💻<strong>面试官</strong> ：<strong>你知道什么是死锁吗?</strong></p>
<p>🙋 <strong>我</strong> ：死锁描述的是这样一种情况：多个进程/线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于进程/线程被无限期地阻塞，因此程序不可能正常终止。</p>
<h3 id="2-7-死锁的四个条件"><a href="#2-7-死锁的四个条件" class="headerlink" title="2.7 死锁的四个条件"></a>2.7 死锁的四个条件</h3><p>👨‍💻<strong>面试官</strong> ：<strong>产生死锁的四个必要条件是什么?</strong></p>
<p>🙋 <strong>我</strong> ：如果系统中以下四个条件同时成立，那么就能引起死锁：</p>
<ul>
<li><strong>互斥</strong>：资源必须处于非共享模式，即一次只有一个进程可以使用。如果另一进程申请该资源，那么必须等待直到该资源被释放为止。</li>
<li><strong>占有并等待</strong>：一个进程至少应该占有一个资源，并等待另一资源，而该资源被其他进程所占有。</li>
<li><strong>非抢占</strong>：资源不能被抢占。只能在持有资源的进程完成任务后，该资源才会被释放。</li>
<li><strong>循环等待</strong>：有一组等待进程 <code>&#123;P0, P1,..., Pn&#125;</code>， <code>P0</code> 等待的资源被 <code>P1</code> 占有，<code>P1</code> 等待的资源被 <code>P2</code> 占有，……，<code>Pn-1</code> 等待的资源被 <code>Pn</code> 占有，<code>Pn</code> 等待的资源被 <code>P0</code> 占有。</li>
</ul>
<p>注意，只有四个条件同时成立时，死锁才会出现。</p>
<h3 id="2-8-解决死锁的方法"><a href="#2-8-解决死锁的方法" class="headerlink" title="2.8 解决死锁的方法"></a>2.8 解决死锁的方法</h3><p>解决死锁的方法可以从多个角度去分析，一般的情况下，有<strong>预防，避免，检测和解除四种</strong>。</p>
<ul>
<li><p><strong>预防</strong> 是采用某种策略，<strong>限制并发进程对资源的请求</strong>，从而使得死锁的必要条件在系统执行的任何时间上都不满足。</p>
</li>
<li><p><strong>避免</strong>则是系统在分配资源时，根据资源的使用情况<strong>提前做出预测</strong>，从而<strong>避免死锁的发生</strong></p>
</li>
<li><p><strong>检测</strong>是指系统设有<strong>专门的机构</strong>，当死锁发生时，该机构能够检测死锁的发生，并精确地确定与死锁有关的进程和资源。</p>
</li>
<li><p><strong>解除</strong> 是与检测相配套的一种措施，用于<strong>将进程从死锁状态下解脱出来</strong>。</p>
</li>
</ul>
<h4 id="死锁的预防"><a href="#死锁的预防" class="headerlink" title="死锁的预防"></a>死锁的预防</h4><p>死锁四大必要条件上面都已经列出来了，很显然，只要破坏四个必要条件中的任何一个就能够预防死锁的发生。</p>
<p>破坏第一个条件 <strong>互斥条件</strong>：使得资源是可以同时访问的，这是种简单的方法，磁盘就可以用这种方法管理，但是我们要知道，有很多资源 <strong>往往是不能同时访问的</strong> ，所以这种做法在大多数的场合是行不通的。</p>
<p>破坏第三个条件 <strong>非抢占</strong> ：也就是说可以采用 <strong>剥夺式调度算法</strong>，但剥夺式调度方法目前一般仅适用于 <strong>主存资源</strong> 和 <strong>处理器资源</strong> 的分配，并不适用于所以的资源，会导致 <strong>资源利用率下降</strong>。</p>
<p>所以一般比较实用的 <strong>预防死锁的方法</strong>，是通过考虑破坏第二个条件和第四个条件。</p>
<p><strong>1、静态分配策略</strong></p>
<p>静态分配策略可以破坏死锁产生的第二个条件（占有并等待）。所谓静态分配策略，就是指一个进程必须在执行前就申请到它所需要的全部资源，并且知道它所要的资源都得到满足之后才开始执行。进程要么占有所有的资源然后开始执行，要么不占有资源，不会出现占有一些资源等待一些资源的情况。</p>
<p>静态分配策略逻辑简单，实现也很容易，但这种策略 <strong>严重地降低了资源利用率</strong>，因为在每个进程所占有的资源中，有些资源是在比较靠后的执行时间里采用的，甚至有些资源是在额外的情况下才是用的，这样就可能造成了一个进程占有了一些 <strong>几乎不用的资源而使其他需要该资源的进程产生等待</strong> 的情况。</p>
<p><strong>2、层次分配策略</strong></p>
<p>层次分配策略破坏了产生死锁的第四个条件(循环等待)。在层次分配策略下，所有的资源被分成了多个层次，一个进程得到某一次的一个资源后，它只能再申请较高一层的资源；当一个进程要释放某层的一个资源时，必须先释放所占用的较高层的资源，按这种策略，是不可能出现循环等待链的，因为那样的话，就出现了已经申请了较高层的资源，反而去申请了较低层的资源，不符合层次分配策略，证明略。</p>
<h4 id="死锁的避免"><a href="#死锁的避免" class="headerlink" title="死锁的避免"></a>死锁的避免</h4><p>上面提到的 <strong>破坏</strong> 死锁产生的四个必要条件之一就可以成功 <strong>预防系统发生死锁</strong> ，但是会导致 <strong>低效的进程运行</strong> 和 <strong>资源使用率</strong> 。而死锁的避免相反，它的角度是允许系统中<strong>同时存在四个必要条件</strong> ，只要掌握并发进程中与每个进程有关的资源动态申请情况，做出 <strong>明智和合理的选择</strong> ，仍然可以避免死锁，因为四大条件仅仅是产生死锁的必要条件。</p>
<p>我们将系统的状态分为 <strong>安全状态</strong> 和 <strong>不安全状态</strong> ，每当在未申请者分配资源前先测试系统状态，若把系统资源分配给申请者会产生死锁，则拒绝分配，否则接受申请，并为它分配资源。</p>
<blockquote>
<p>如果操作系统能够保证所有的进程在有限的时间内得到需要的全部资源，则称系统处于安全状态，否则说系统是不安全的。很显然，系统处于安全状态则不会发生死锁，系统若处于不安全状态则可能发生死锁。</p>
</blockquote>
<p>那么如何保证系统保持在安全状态呢？通过算法，其中最具有代表性的 <strong>避免死锁算法</strong> 就是 Dijkstra 的银行家算法，银行家算法用一句话表达就是：当一个进程申请使用资源的时候，<strong>银行家算法</strong> 通过先 <strong>试探</strong> 分配给该进程资源，然后通过 <strong>安全性算法</strong> 判断分配后系统是否处于安全状态，若不安全则试探分配作废，让该进程继续等待，若能够进入到安全的状态，则就 <strong>真的分配资源给该进程</strong>。</p>
<p>银行家算法详情可见：<a href="https://blog.csdn.net/qq_33414271/article/details/80245715">《一句话+一张图说清楚——银行家算法》</a> 。</p>
<p>操作系统教程树中讲述的银行家算法也比较清晰，可以一看.</p>
<p>死锁的避免(银行家算法)改善解决了 <strong>资源使用率低的问题</strong> ，但是它要不断地检测每个进程对各类资源的占用和申请情况，以及做 <strong>安全性检查</strong> ，需要花费较多的时间。</p>
<h4 id="死锁的检测"><a href="#死锁的检测" class="headerlink" title="死锁的检测"></a>死锁的检测</h4><p>对资源的分配加以限制可以 <strong>预防和避免</strong> 死锁的发生，但是都不利于各进程对系统资源的<strong>充分共享</strong>。解决死锁问题的另一条途径是 <strong>死锁检测和解除</strong> (这里突然联想到了乐观锁和悲观锁，感觉死锁的检测和解除就像是 <strong>乐观锁</strong> ，分配资源时不去提前管会不会发生死锁了，等到真的死锁出现了再来解决嘛，而 <strong>死锁的预防和避免</strong> 更像是悲观锁，总是觉得死锁会出现，所以在分配资源的时候就很谨慎)。</p>
<p>这种方法对资源的分配不加以任何限制，也不采取死锁避免措施，但系统 <strong>定时地运行一个 “死锁检测”</strong> 的程序，判断系统内是否出现死锁，如果检测到系统发生了死锁，再采取措施去解除它。</p>
<h5 id="进程-资源分配图"><a href="#进程-资源分配图" class="headerlink" title="进程-资源分配图"></a>进程-资源分配图</h5><p>操作系统中的每一刻时刻的<strong>系统状态</strong>都可以用<strong>进程-资源分配图</strong>来表示，进程-资源分配图是描述进程和资源申请及分配关系的一种有向图，可用于<strong>检测系统是否处于死锁状态</strong>。</p>
<p>用一个方框表示每一个资源类，方框中的黑点表示该资源类中的各个资源，每个键进程用一个圆圈表示，用 <strong>有向边</strong> 来表示<strong>进程申请资源和资源被分配的情况</strong>。</p>
<p>图中 2-21 是<strong>进程-资源分配图</strong>的一个例子，其中共有三个资源类，每个进程的资源占有和申请情况已清楚地表示在图中。在这个例子中，由于存在 <strong>占有和等待资源的环路</strong> ，导致一组进程永远处于等待资源的状态，发生了 <strong>死锁</strong>。</p>
<p><img src="/./images/%E8%BF%9B%E7%A8%8B-%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E5%9B%BE.jpg" alt="进程-资源分配图"></p>
<p>进程-资源分配图中存在环路并不一定是发生了死锁。因为循环等待资源仅仅是死锁发生的必要条件，而不是充分条件。图 2-22 便是一个有环路而无死锁的例子。虽然进程 P1 和进程 P3 分别占用了一个资源 R1 和一个资源 R2，并且因为等待另一个资源 R2 和另一个资源 R1 形成了环路，但进程 P2 和进程 P4 分别占有了一个资源 R1 和一个资源 R2，它们申请的资源得到了满足，在有限的时间里会归还资源，于是进程 P1 或 P3 都能获得另一个所需的资源，环路自动解除，系统也就不存在死锁状态了。</p>
<h5 id="死锁检测步骤"><a href="#死锁检测步骤" class="headerlink" title="死锁检测步骤"></a>死锁检测步骤</h5><p>知道了死锁检测的原理，我们可以利用下列步骤编写一个 <strong>死锁检测</strong> 程序，检测系统是否产生了死锁。</p>
<ol>
<li>如果进程-资源分配图中无环路，则此时系统没有发生死锁</li>
<li>如果进程-资源分配图中有环路，且每个资源类仅有一个资源，则系统中已经发生了死锁。</li>
<li>如果进程-资源分配图中有环路，且涉及到的资源类有多个资源，此时系统未必会发生死锁。如果能在进程-资源分配图中找出一个 <strong>既不阻塞又非独立的进程</strong> ，该进程能够在有限的时间内归还占有的资源，也就是把边给消除掉了，重复此过程，直到能在有限的时间内 <strong>消除所有的边</strong> ，则不会发生死锁，否则会发生死锁。(消除边的过程类似于 <strong>拓扑排序</strong>)</li>
</ol>
<h4 id="死锁的解除"><a href="#死锁的解除" class="headerlink" title="死锁的解除"></a>死锁的解除</h4><p>当死锁检测程序检测到存在死锁发生时，应设法让其解除，让系统从死锁状态中恢复过来，常用的解除死锁的方法有以下四种：</p>
<ol>
<li><strong>立即结束所有进程的执行，重新启动操作系统</strong> ：这种方法简单，但以前所在的工作全部作废，损失很大。</li>
<li><strong>撤销涉及死锁的所有进程，解除死锁后继续运行</strong> ：这种方法能彻底打破<strong>死锁的循环等待</strong>条件，但将付出很大代价，例如有些进程可能已经计算了很长时间，由于被撤销而使产生的部分结果也被消除了，再重新执行时还要再次进行计算。</li>
<li><strong>逐个撤销涉及死锁的进程，回收其资源直至死锁解除。</strong></li>
<li><strong>抢占资源</strong> ：从涉及死锁的一个或几个进程中抢占资源，把夺得的资源再分配给涉及死锁的进程直至死锁解除。</li>
</ol>
<h2 id="三-操作系统内存管理基础"><a href="#三-操作系统内存管理基础" class="headerlink" title="三 操作系统内存管理基础"></a>三 操作系统内存管理基础</h2><h3 id="3-1-内存管理介绍"><a href="#3-1-内存管理介绍" class="headerlink" title="3.1 内存管理介绍"></a>3.1 内存管理介绍</h3><p>👨‍💻 <strong>面试官</strong>: <strong>操作系统的内存管理主要是做什么？</strong></p>
<p>🙋 <strong>我：</strong> 操作系统的内存管理主要负责内存的分配与回收（malloc 函数：申请内存，free 函数：释放内存），另外地址转换也就是将逻辑地址转换成相应的物理地址等功能也是操作系统内存管理做的事情。</p>
<h3 id="3-2-常见的几种内存管理机制"><a href="#3-2-常见的几种内存管理机制" class="headerlink" title="3.2 常见的几种内存管理机制"></a>3.2 常见的几种内存管理机制</h3><p>👨‍💻 <strong>面试官</strong>: <strong>操作系统的内存管理机制了解吗？内存管理有哪几种方式?</strong></p>
<p>🙋 <strong>我：</strong> 这个在学习操作系统的时候有了解过。</p>
<p>简单分为<strong>连续分配管理方式</strong>和<strong>非连续分配管理方式</strong>这两种。连续分配管理方式是指为一个用户程序分配一个连续的内存空间，常见的如 <strong>块式管理</strong> 。同样地，非连续分配管理方式允许一个程序使用的内存分布在离散或者说不相邻的内存中，常见的如<strong>页式管理</strong> 和 <strong>段式管理</strong>。</p>
<ol>
<li><strong>块式管理</strong> ： 远古时代的计算机操系统的内存管理方式。将内存分为几个固定大小的块，每个块中只包含一个进程。如果程序运行需要内存的话，操作系统就分配给它一块，如果程序运行只需要很小的空间的话，分配的这块内存很大一部分几乎被浪费了。这些在每个块中未被利用的空间，我们称之为碎片。</li>
<li><strong>页式管理</strong> ：把主存分为大小相等且固定的一页一页的形式，页较小，相对相比于块式管理的划分力度更大，提高了内存利用率，减少了碎片。页式管理通过页表对应逻辑地址和物理地址。</li>
<li><strong>段式管理</strong> ： 页式管理虽然提高了内存利用率，但是页式管理其中的页实际并无任何实际意义。 段式管理把主存分为一段段的，段是有实际意义的，每个段定义了一组逻辑信息，例如,有主程序段 MAIN、子程序段 X、数据段 D 及栈段 S 等。 段式管理通过段表对应逻辑地址和物理地址。</li>
</ol>
<p>简单来说：页是物理单位，段是逻辑单位。分页可以有效提高内存利用率，分段可以更好满足用户需求。</p>
<p>👨‍💻<strong>面试官</strong> ： 回答的还不错！不过漏掉了一个很重要的 <strong>段页式管理机制</strong> 。段页式管理机制结合了段式管理和页式管理的优点。简单来说段页式管理机制就是把主存先分成若干段，每个段又分成若干页，也就是说 <strong>段页式管理机制</strong> 中段与段之间以及段的内部的都是离散的。</p>
<p>🙋 <strong>我</strong> ：谢谢面试官！刚刚把这个给忘记了～</p>
<h3 id="3-3-快表和多级页表"><a href="#3-3-快表和多级页表" class="headerlink" title="3.3 快表和多级页表"></a>3.3 快表和多级页表</h3><p>👨‍💻<strong>面试官</strong> ： 页表管理机制中有两个很重要的概念：快表和多级页表，这两个东西分别解决了页表管理中很重要的两个问题。你给我简单介绍一下吧！</p>
<p>🙋 <strong>我</strong> ：在分页内存管理中，很重要的两点是：</p>
<ol>
<li>虚拟地址到物理地址的转换要快。</li>
<li>解决虚拟地址空间大，页表也会很大的问题。</li>
</ol>
<h4 id="快表"><a href="#快表" class="headerlink" title="快表"></a>快表</h4><p>为了解决虚拟地址到物理地址的转换速度，操作系统在 <strong>页表方案</strong> 基础之上引入了 <strong>快表</strong> 来加速虚拟地址到物理地址的转换。我们可以把快表理解为一种特殊的高速缓冲存储器（Cache），其中的内容是页表的一部分或者全部内容。作为页表的 Cache，它的作用与页表相似，但是提高了访问速率。由于采用页表做地址转换，读写内存数据时 CPU 要访问两次主存。有了快表，有时只要访问一次高速缓冲存储器，一次主存，这样可加速查找并提高指令执行速度。</p>
<p>使用快表之后的地址转换流程是这样的：</p>
<ol>
<li>根据虚拟地址中的页号查快表；</li>
<li>如果该页在快表中，直接从快表中读取相应的物理地址；</li>
<li>如果该页不在快表中，就访问内存中的页表，再从页表中得到物理地址，同时将页表中的该映射表项添加到快表中；</li>
<li>当快表填满后，又要登记新页时，就按照一定的淘汰策略淘汰掉快表中的一个页。</li>
</ol>
<p>看完了之后你会发现快表和我们平时经常在我们开发的系统使用的缓存（比如 Redis）很像，的确是这样的，操作系统中的很多思想、很多经典的算法，你都可以在我们日常开发使用的各种工具或者框架中找到它们的影子。</p>
<h4 id="多级页表"><a href="#多级页表" class="headerlink" title="多级页表"></a>多级页表</h4><p>引入多级页表的主要目的是为了避免把全部页表一直放在内存中占用过多空间，特别是那些根本就不需要的页表就不需要保留在内存中。多级页表属于时间换空间的典型场景，具体可以查看下面这篇文章</p>
<ul>
<li>多级页表如何节约内存：<a href="https://www.polarxiong.com/archives/%E5%A4%9A%E7%BA%A7%E9%A1%B5%E8%A1%A8%E5%A6%82%E4%BD%95%E8%8A%82%E7%BA%A6%E5%86%85%E5%AD%98.html">https://www.polarxiong.com/archives/多级页表如何节约内存.html</a></li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>为了提高内存的空间性能，提出了多级页表的概念；但是提到空间性能是以浪费时间性能为基础的，因此为了补充损失的时间性能，提出了快表（即 TLB）的概念。 不论是快表还是多级页表实际上都利用到了程序的局部性原理，局部性原理在后面的虚拟内存这部分会介绍到。</p>
<h3 id="3-4-分页机制和分段机制的共同点和区别"><a href="#3-4-分页机制和分段机制的共同点和区别" class="headerlink" title="3.4 分页机制和分段机制的共同点和区别"></a>3.4 分页机制和分段机制的共同点和区别</h3><p>👨‍💻<strong>面试官</strong> ： <strong>分页机制和分段机制有哪些共同点和区别呢？</strong></p>
<p>🙋 <strong>我</strong> ：</p>
<ol>
<li><strong>共同点</strong> ：<ul>
<li>分页机制和分段机制都是为了提高内存利用率，减少内存碎片。</li>
<li>页和段都是离散存储的，所以两者都是离散分配内存的方式。但是，每个页和段中的内存是连续的。</li>
</ul>
</li>
<li><strong>区别</strong> ：<ul>
<li>页的大小是固定的，由操作系统决定；而段的大小不固定，取决于我们当前运行的程序。</li>
<li>分页仅仅是为了满足操作系统内存管理的需求，而段是逻辑信息的单位，在程序中可以体现为代码段，数据段，能够更好满足用户的需要。</li>
</ul>
</li>
</ol>
<h3 id="3-5-逻辑-虚拟-地址和物理地址"><a href="#3-5-逻辑-虚拟-地址和物理地址" class="headerlink" title="3.5 逻辑(虚拟)地址和物理地址"></a>3.5 逻辑(虚拟)地址和物理地址</h3><p>👨‍💻<strong>面试官</strong> ：你刚刚还提到了<strong>逻辑地址和物理地址</strong>这两个概念，我不太清楚，你能为我解释一下不？</p>
<p>🙋 <strong>我：</strong> em…好的嘛！我们编程一般只有可能和逻辑地址打交道，比如在 C 语言中，指针里面存储的数值就可以理解成为内存里的一个地址，这个地址也就是我们说的逻辑地址，逻辑地址由操作系统决定。物理地址指的是真实物理内存中地址，更具体一点来说就是内存地址寄存器中的地址。物理地址是内存单元真正的地址。</p>
<h3 id="3-6-CPU-寻址了解吗-为什么需要虚拟地址空间"><a href="#3-6-CPU-寻址了解吗-为什么需要虚拟地址空间" class="headerlink" title="3.6 CPU 寻址了解吗?为什么需要虚拟地址空间?"></a>3.6 CPU 寻址了解吗?为什么需要虚拟地址空间?</h3><p>👨‍💻<strong>面试官</strong> ：<strong>CPU 寻址了解吗?为什么需要虚拟地址空间?</strong></p>
<p>🙋 <strong>我</strong> ：这部分我真不清楚！</p>
<p>于是面试完之后我默默去查阅了相关文档！留下了没有技术的泪水。。。</p>
<blockquote>
<p>这部分内容参考了 Microsoft 官网的介绍，地址：<a href="https://docs.microsoft.com/zh-cn/windows-hardware/drivers/gettingstarted/virtual-address-spaces?redirectedfrom=MSDN">https://docs.microsoft.com/zh-cn/windows-hardware/drivers/gettingstarted/virtual-address-spaces?redirectedfrom=MSDN</a></p>
</blockquote>
<p>现代处理器使用的是一种称为 <strong>虚拟寻址(Virtual Addressing)</strong> 的寻址方式。<strong>使用虚拟寻址，CPU 需要将虚拟地址翻译成物理地址，这样才能访问到真实的物理内存。</strong> 实际上完成虚拟地址转换为物理地址转换的硬件是 CPU 中含有一个被称为 <strong>内存管理单元（Memory Management Unit, MMU）</strong> 的硬件。如下图所示：</p>
<p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/2b27dac8cc647f8aac989da2d1166db2.png" alt="MMU_principle_updated"></p>
<p><strong>为什么要有虚拟地址空间呢？</strong></p>
<p>先从没有虚拟地址空间的时候说起吧！没有虚拟地址空间的时候，<strong>程序都是直接访问和操作的都是物理内存</strong> 。但是这样有什么问题呢？</p>
<ol>
<li>用户程序可以访问任意内存，寻址内存的每个字节，这样就很容易（有意或者无意）破坏操作系统，造成操作系统崩溃。</li>
<li>想要同时运行多个程序特别困难，比如你想同时运行一个微信和一个 QQ 音乐都不行。为什么呢？举个简单的例子：微信在运行的时候给内存地址 1xxx 赋值后，QQ 音乐也同样给内存地址 1xxx 赋值，那么 QQ 音乐对内存的赋值就会覆盖微信之前所赋的值，这就造成了微信这个程序就会崩溃。</li>
</ol>
<p><strong>总结来说：如果直接把物理地址暴露出来的话会带来严重问题，比如可能对操作系统造成伤害以及给同时运行多个程序造成困难。</strong></p>
<p>通过虚拟地址访问内存有以下优势：</p>
<ul>
<li>程序可以使用一系列相邻的虚拟地址来访问物理内存中不相邻的大内存缓冲区。</li>
<li>程序可以使用一系列虚拟地址来访问大于可用物理内存的内存缓冲区。当物理内存的供应量变小时，内存管理器会将物理内存页（通常大小为 4 KB）保存到磁盘文件。数据或代码页会根据需要在物理内存与磁盘之间移动。</li>
<li>不同进程使用的虚拟地址彼此隔离。一个进程中的代码无法更改正在由另一进程或操作系统使用的物理内存。</li>
</ul>
<h2 id="四-虚拟内存"><a href="#四-虚拟内存" class="headerlink" title="四 虚拟内存"></a>四 虚拟内存</h2><h3 id="4-1-什么是虚拟内存-Virtual-Memory"><a href="#4-1-什么是虚拟内存-Virtual-Memory" class="headerlink" title="4.1 什么是虚拟内存(Virtual Memory)?"></a>4.1 什么是虚拟内存(Virtual Memory)?</h3><p>👨‍💻<strong>面试官</strong> ：再问你一个常识性的问题！<strong>什么是虚拟内存(Virtual Memory)?</strong></p>
<p>🙋 <strong>我</strong> ：这个在我们平时使用电脑特别是 Windows 系统的时候太常见了。很多时候我们使用点开了很多占内存的软件，这些软件占用的内存可能已经远远超出了我们电脑本身具有的物理内存。<strong>为什么可以这样呢？</strong> 正是因为 <strong>虚拟内存</strong> 的存在，通过 <strong>虚拟内存</strong> 可以让程序可以拥有超过系统物理内存大小的可用内存空间。另外，<strong>虚拟内存为每个进程提供了一个一致的、私有的地址空间，它让每个进程产生了一种自己在独享主存的错觉（每个进程拥有一片连续完整的内存空间）</strong>。这样会更加有效地管理内存并减少出错。</p>
<p><strong>虚拟内存</strong>是计算机系统内存管理的一种技术，我们可以手动设置自己电脑的虚拟内存。不要单纯认为虚拟内存只是“使用硬盘空间来扩展内存“的技术。<strong>虚拟内存的重要意义是它定义了一个连续的虚拟地址空间</strong>，并且 <strong>把内存扩展到硬盘空间</strong>。推荐阅读：<a href="https://juejin.im/post/59f8691b51882534af254317">《虚拟内存的那点事儿》</a></p>
<p>维基百科中有几句话是这样介绍虚拟内存的。</p>
<blockquote>
<p><strong>虚拟内存</strong> 使得应用程序认为它拥有连续的可用的内存（一个连续完整的地址空间），而实际上，它通常是被分隔成多个物理内存碎片，还有部分暂时存储在外部磁盘存储器上，在需要时进行数据交换。与没有使用虚拟内存技术的系统相比，使用这种技术的系统使得大型程序的编写变得更容易，对真正的物理内存（例如 RAM）的使用也更有效率。目前，大多数操作系统都使用了虚拟内存，如 Windows 家族的“虚拟内存”；Linux 的“交换空间”等。From:<a href="https://zh.wikipedia.org/wiki/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98">https://zh.wikipedia.org/wiki/虚拟内存</a></p>
</blockquote>
<h3 id="4-2-局部性原理"><a href="#4-2-局部性原理" class="headerlink" title="4.2 局部性原理"></a>4.2 局部性原理</h3><p>👨‍💻<strong>面试官</strong> ：要想更好地理解虚拟内存技术，必须要知道计算机中著名的<strong>局部性原理</strong>。另外，局部性原理既适用于程序结构，也适用于数据结构，是非常重要的一个概念。</p>
<p>🙋 <strong>我</strong> ：局部性原理是虚拟内存技术的基础，正是因为程序运行具有局部性原理，才可以只装入部分程序到内存就开始运行。</p>
<blockquote>
<p>以下内容摘自《计算机操作系统教程》 第 4 章存储器管理。</p>
</blockquote>
<p>早在 1968 年的时候，就有人指出我们的程序在执行的时候往往呈现局部性规律，也就是说在某个较短的时间段内，程序执行局限于某一小部分，程序访问的存储空间也局限于某个区域。</p>
<p>局部性原理表现在以下两个方面：</p>
<ol>
<li><strong>时间局部性</strong> ：如果程序中的某条指令一旦执行，不久以后该指令可能再次执行；如果某数据被访问过，不久以后该数据可能再次被访问。产生时间局部性的典型原因，是由于在程序中存在着大量的循环操作。</li>
<li><strong>空间局部性</strong> ：一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问，即程序在一段时间内所访问的地址，可能集中在一定的范围之内，这是因为指令通常是顺序存放、顺序执行的，数据也一般是以向量、数组、表等形式簇聚存储的。</li>
</ol>
<p>时间局部性是通过将近来使用的指令和数据保存到高速缓存存储器中，并使用高速缓存的层次结构实现。空间局部性通常是使用较大的高速缓存，并将预取机制集成到高速缓存控制逻辑中实现。虚拟内存技术实际上就是建立了 “内存一外存”的两级存储器的结构，利用局部性原理实现髙速缓存。</p>
<h3 id="4-3-虚拟存储器"><a href="#4-3-虚拟存储器" class="headerlink" title="4.3 虚拟存储器"></a>4.3 虚拟存储器</h3><blockquote>
<p><strong>勘误：虚拟存储器又叫做虚拟内存，都是 Virtual Memory 的翻译，属于同一个概念。</strong></p>
</blockquote>
<p>👨‍💻<strong>面试官</strong> ：<del>都说了虚拟内存了。你再讲讲<strong>虚拟存储器</strong>把！</del></p>
<p>🙋 <strong>我</strong> ：</p>
<blockquote>
<p>这部分内容来自：<a href="https://wizardforcel.gitbooks.io/wangdaokaoyan-os/content/13.html">王道考研操作系统知识点整理</a>。</p>
</blockquote>
<p>基于局部性原理，在程序装入时，可以将程序的一部分装入内存，而将其他部分留在外存，就可以启动程序执行。由于外存往往比内存大很多，所以我们运行的软件的内存大小实际上是可以比计算机系统实际的内存大小大的。在程序执行过程中，当所访问的信息不在内存时，由操作系统将所需要的部分调入内存，然后继续执行程序。另一方面，操作系统将内存中暂时不使用的内容换到外存上，从而腾出空间存放将要调入内存的信息。这样，计算机好像为用户提供了一个比实际内存大的多的存储器——<strong>虚拟存储器</strong>。</p>
<p>实际上，我觉得虚拟内存同样是一种时间换空间的策略，你用 CPU 的计算时间，页的调入调出花费的时间，换来了一个虚拟的更大的空间来支持程序的运行。不得不感叹，程序世界几乎不是时间换空间就是空间换时间。</p>
<h3 id="4-4-虚拟内存的技术实现"><a href="#4-4-虚拟内存的技术实现" class="headerlink" title="4.4 虚拟内存的技术实现"></a>4.4 虚拟内存的技术实现</h3><p>👨‍💻<strong>面试官</strong> ：<strong>虚拟内存技术的实现呢？</strong></p>
<p>🙋 <strong>我</strong> ：<strong>虚拟内存的实现需要建立在离散分配的内存管理方式的基础上。</strong> 虚拟内存的实现有以下三种方式：</p>
<ol>
<li><strong>请求分页存储管理</strong> ：建立在分页管理之上，为了支持虚拟存储器功能而增加了请求调页功能和页面置换功能。请求分页是目前最常用的一种实现虚拟存储器的方法。请求分页存储管理系统中，在作业开始运行之前，仅装入当前要执行的部分段即可运行。假如在作业运行的过程中发现要访问的页面不在内存，则由处理器通知操作系统按照对应的页面置换算法将相应的页面调入到主存，同时操作系统也可以将暂时不用的页面置换到外存中。</li>
<li><strong>请求分段存储管理</strong> ：建立在分段存储管理之上，增加了请求调段功能、分段置换功能。请求分段储存管理方式就如同请求分页储存管理方式一样，在作业开始运行之前，仅装入当前要执行的部分段即可运行；在执行过程中，可使用请求调入中断动态装入要访问但又不在内存的程序段；当内存空间已满，而又需要装入新的段时，根据置换功能适当调出某个段，以便腾出空间而装入新的段。</li>
<li><strong>请求段页式存储管理</strong></li>
</ol>
<p><strong>这里多说一下？很多人容易搞混请求分页与分页存储管理，两者有何不同呢？</strong></p>
<p>请求分页存储管理建立在分页管理之上。他们的根本区别是是否将程序全部所需的全部地址空间都装入主存，这也是请求分页存储管理可以提供虚拟内存的原因，我们在上面已经分析过了。</p>
<p>它们之间的根本区别在于是否将一作业的全部地址空间同时装入主存。请求分页存储管理不要求将作业全部地址空间同时装入主存。基于这一点，请求分页存储管理可以提供虚存，而分页存储管理却不能提供虚存。</p>
<p>不管是上面那种实现方式，我们一般都需要：</p>
<ol>
<li>一定容量的内存和外存：在载入程序的时候，只需要将程序的一部分装入内存，而将其他部分留在外存，然后程序就可以执行了；</li>
<li><strong>缺页中断</strong>：如果<strong>需执行的指令或访问的数据尚未在内存</strong>（称为缺页或缺段），则由处理器通知操作系统将相应的页面或段<strong>调入到内存</strong>，然后继续执行程序；</li>
<li><strong>虚拟地址空间</strong> ：逻辑地址到物理地址的变换。</li>
</ol>
<h3 id="4-5-页面置换算法"><a href="#4-5-页面置换算法" class="headerlink" title="4.5 页面置换算法"></a>4.5 页面置换算法</h3><p>👨‍💻<strong>面试官</strong> ：虚拟内存管理很重要的一个概念就是页面置换算法。那你说一下 <strong>页面置换算法的作用?常见的页面置换算法有哪些?</strong></p>
<p>🙋 <strong>我</strong> ：</p>
<blockquote>
<p>这个题目经常作为笔试题出现，网上已经给出了很不错的回答，我这里只是总结整理了一下。</p>
</blockquote>
<p>地址映射过程中，若在页面中发现所要访问的页面不在内存中，则发生缺页中断 。</p>
<blockquote>
<p><strong>缺页中断</strong> 就是要访问的<strong>页</strong>不在主存，需要操作系统将其调入主存后再进行访问。 在这个时候，被内存映射的文件实际上成了一个分页交换文件。</p>
</blockquote>
<p>当发生缺页中断时，如果当前内存中并没有空闲的页面，操作系统就必须在内存选择一个页面将其移出内存，以便为即将调入的页面让出空间。用来选择淘汰哪一页的规则叫做页面置换算法，我们可以把页面置换算法看成是淘汰页面的规则。</p>
<ul>
<li><strong>OPT 页面置换算法（最佳页面置换算法）</strong> ：最佳(Optimal, OPT)置换算法所选择的被淘汰页面将是以后永不使用的，或者是在最长时间内不再被访问的页面,这样可以保证获得最低的缺页率。但由于人们目前无法预知进程在内存下的若千页面中哪个是未来最长时间内不再被访问的，因而该算法无法实现。一般作为衡量其他置换算法的方法。</li>
<li><strong>FIFO（First In First Out） 页面置换算法（先进先出页面置换算法）</strong> : 总是淘汰最先进入内存的页面，即选择在内存中驻留时间最久的页面进行淘汰。</li>
<li><strong>LRU （Least Recently Used）页面置换算法（最近最久未使用页面置换算法）</strong> ：LRU 算法赋予每个页面一个访问字段，用来记录一个页面自上次被访问以来所经历的时间 T，当须淘汰一个页面时，选择现有页面中其 T 值最大的，即最近最久未使用的页面予以淘汰。</li>
<li><strong>LFU （Least Frequently Used）页面置换算法（最少使用页面置换算法）</strong> : 该置换算法选择在之前时期使用最少的页面作为淘汰页。</li>
</ul>
<blockquote>
<p> 转至javaguide，用来学习操作系统</p>
</blockquote>
]]></content>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机网络</title>
    <url>/2022/03/25/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h2 id="应用层有哪些常见的协议？"><a href="#应用层有哪些常见的协议？" class="headerlink" title="应用层有哪些常见的协议？"></a>应用层有哪些常见的协议？</h2><h3 id="HTTP-超文本传输协议"><a href="#HTTP-超文本传输协议" class="headerlink" title="HTTP:超文本传输协议"></a>HTTP:超文本传输协议</h3><span id="more"></span>

<p><strong>超文本传输协议（HTTP，HyperText Transfer Protocol)</strong> 主要是为 Web 浏览器与 Web 服务器之间的通信而设计的。当我们使用浏览器浏览网页的时候，我们网页就是通过 HTTP 请求进行加载的，整个过程如下图所示。</p>
<p><img src="https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/450px-HTTP-Header.png" alt="img"></p>
<p>HTTP 协是基于 TCP协议，发送 HTTP 请求之前首先要建立 TCP 连接也就是要经历 3 次握手。目前使用的 HTTP 协议大部分都是 1.1。在 1.1 的协议里面，默认是开启了 Keep-Alive 的，这样的话建立的连接就可以在多次请求中被复用了。</p>
<p>另外， HTTP 协议是”无状态”的协议，它无法记录客户端用户的状态，一般我们都是通过 Session 来记录客户端用户的状态。</p>
<h3 id="SMTP-简单邮件传输-发送-协议"><a href="#SMTP-简单邮件传输-发送-协议" class="headerlink" title="SMTP:简单邮件传输(发送)协议"></a>SMTP:简单邮件传输(发送)协议</h3><p><strong>简单邮件传输(发送)协议（SMTP，Simple Mail Transfer Protocol）</strong> 基于 TCP 协议，用来发送电子邮件。</p>
<p>注意⚠️：<strong>接受邮件的协议不是 SMTP 而是 POP3 协议。</strong></p>
<p>SMTP 协议这块涉及的内容比较多，下面这两个问题比较重要：</p>
<ol>
<li>电子邮件的发送过程</li>
<li>如何判断邮箱是真正存在的？</li>
</ol>
<p><strong>电子邮件的发送过程？</strong></p>
<p>比如我的邮箱是“<a href="mailto:&#100;&#97;&#98;&#97;&#x69;&#64;&#x63;&#115;&#122;&#104;&#x69;&#110;&#x61;&#x6e;&#46;&#x63;&#111;&#x6d;">&#100;&#97;&#98;&#97;&#x69;&#64;&#x63;&#115;&#122;&#104;&#x69;&#110;&#x61;&#x6e;&#46;&#x63;&#111;&#x6d;</a>”，我要向“<a href="mailto:&#x78;&#x69;&#97;&#111;&#109;&#97;&#64;&#113;&#x71;&#x2e;&#99;&#111;&#x6d;">&#x78;&#x69;&#97;&#111;&#109;&#97;&#64;&#113;&#x71;&#x2e;&#99;&#111;&#x6d;</a>”发送邮件，整个过程可以简单分为下面几步：</p>
<ol>
<li>通过 <strong>SMTP</strong> 协议，我将我写好的邮件交给163邮箱服务器（邮局）。</li>
<li>163邮箱服务器发现我发送的邮箱是qq邮箱，然后它使用 SMTP协议将我的邮件转发到 qq邮箱服务器。</li>
<li>qq邮箱服务器接收邮件之后就通知邮箱为“<a href="mailto:&#120;&#x69;&#97;&#111;&#x6d;&#x61;&#x40;&#x71;&#x71;&#46;&#x63;&#x6f;&#x6d;">&#120;&#x69;&#97;&#111;&#x6d;&#x61;&#x40;&#x71;&#x71;&#46;&#x63;&#x6f;&#x6d;</a>”的用户来收邮件，然后用户就通过 <strong>POP3/IMAP</strong> 协议将邮件取出。</li>
</ol>
<p><strong>如何判断邮箱是真正存在的？</strong></p>
<p>很多场景(比如邮件营销)下面我们需要判断我们要发送的邮箱地址是否真的存在，这个时候我们可以利用 SMTP 协议来检测：</p>
<ol>
<li>查找邮箱域名对应的 SMTP 服务器地址</li>
<li>尝试与服务器建立连接</li>
<li>连接成功后尝试向需要验证的邮箱发送邮件</li>
<li>根据返回结果判定邮箱地址的真实性</li>
</ol>
<p>推荐几个在线邮箱是否有效检测工具：</p>
<ol>
<li><a href="https://verify-email.org/">https://verify-email.org/</a></li>
<li><a href="http://tool.chacuo.net/mailverify">http://tool.chacuo.net/mailverify</a></li>
<li><a href="https://www.emailcamel.com/">https://www.emailcamel.com/</a></li>
</ol>
<h3 id="POP3-IMAP-邮件接收的协议"><a href="#POP3-IMAP-邮件接收的协议" class="headerlink" title="POP3/IMAP:邮件接收的协议"></a>POP3/IMAP:邮件接收的协议</h3><p>这两个协议没必要多做阐述，只需要了解 <strong>POP3 和 IMAP 两者都是负责邮件接收的协议</strong>即可。另外，需要注意不要将这两者和 SMTP 协议搞混淆了。<strong>SMTP 协议只负责邮件的发送，真正负责接收的协议是POP3/IMAP。</strong></p>
<p>IMAP 协议相比于POP3更新一点，为用户提供的可选功能也更多一点,几乎所有现代电子邮件客户端和服务器都支持IMAP。大部分网络邮件服务提供商都支持POP3和IMAP。</p>
<h3 id="FTP-文件传输协议"><a href="#FTP-文件传输协议" class="headerlink" title="FTP:文件传输协议"></a>FTP:文件传输协议</h3><p><strong>FTP 协议</strong> 主要提供文件传输服务，基于 TCP 实现可靠的传输。使用 FTP 传输文件的好处是可以屏蔽操作系统和文件存储方式。</p>
<p>FTP 是基于客户—服务器（C/S）模型而设计的，在客户端与 FTP 服务器之间建立两个连接。如果我们要基于 FTP 协议开发一个文件传输的软件的话，首先需要搞清楚 FTP 的原理。关于 FTP 的原理，很多书籍上已经描述的非常详细了：</p>
<blockquote>
<p>FTP 的独特的优势同时也是与其它客户服务器程序最大的不同点就在于它在两台通信的主机之间使用了两条 TCP 连接（其它客户服务器应用程序一般只有一条 TCP 连接）：</p>
<ol>
<li>控制连接：用于传送控制信息（命令和响应）</li>
<li>数据连接：用于数据传送；</li>
</ol>
<p>这种将命令和数据分开传送的思想大大提高了 FTP 的效率。</p>
</blockquote>
<p><img src="https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/ftp.png" alt="FTP工作过程"></p>
<h3 id="Telnet-远程登陆协议"><a href="#Telnet-远程登陆协议" class="headerlink" title="Telnet:远程登陆协议"></a>Telnet:远程登陆协议</h3><p><strong>Telnet 协议</strong> 通过一个终端登陆到其他服务器，建立在可靠的传输协议 TCP 之上。Telnet 协议的最大缺点之一是所有数据（包括用户名和密码）均以明文形式发送，这有潜在的安全风险。这就是为什么如今很少使用Telnet并被一种称为SSH的非常安全的协议所取代的主要原因。</p>
<h3 id="SSH-安全的网络传输协议"><a href="#SSH-安全的网络传输协议" class="headerlink" title="SSH:安全的网络传输协议"></a>SSH:安全的网络传输协议</h3><p><strong>SSH（ Secure Shell）</strong> 是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。利用 SSH 协议可以有效防止远程管理过程中的信息泄露问题。SSH 建立在可靠的传输协议 TCP 之上。</p>
<p><strong>Telnet 和 SSH 之间的主要区别在于 SSH 协议会对传输的数据进行加密保证数据安全性。</strong></p>
<h2 id="TCP-三次握手和四次挥手-面试常客"><a href="#TCP-三次握手和四次挥手-面试常客" class="headerlink" title="TCP 三次握手和四次挥手(面试常客)"></a>TCP 三次握手和四次挥手(面试常客)</h2><p>为了准确无误地把数据送达目标处，TCP 协议采用了三次握手策略。</p>
<h3 id="TCP-三次握手漫画图解"><a href="#TCP-三次握手漫画图解" class="headerlink" title="TCP 三次握手漫画图解"></a>TCP 三次握手漫画图解</h3><p>如下图所示，下面的两个机器人通过 3 次握手确定了对方能正确接收和发送消息(图片来源：《图解 HTTP》)。</p>
<p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019/7/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.png" alt="TCP三次握手"></p>
<p><strong>简单示意图：</strong></p>
<p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019/7/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B2.png" alt="TCP三次握手"></p>
<ul>
<li>客户端–发送带有 SYN 标志的数据包–一次握手–服务端</li>
<li>服务端–发送带有 SYN/ACK 标志的数据包–二次握手–客户端</li>
<li>客户端–发送带有带有 ACK 标志的数据包–三次握手–服务端</li>
</ul>
<p><strong>详细示意图（图片来源不详）</strong></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/0c9f470819684156cfdc27c682db4def.png" alt="img"></p>
<h3 id="为什么要三次握手"><a href="#为什么要三次握手" class="headerlink" title="为什么要三次握手"></a>为什么要三次握手</h3><p><strong>三次握手的目的是建立可靠的通信信道，说到通讯，简单来说就是数据的发送与接收，而三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的。</strong></p>
<p>第一次握手：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常</p>
<p>第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：对方发送正常，自己接收正常</p>
<p>第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常</p>
<p>所以三次握手就能确认双方收发功能都正常，缺一不可。</p>
<h3 id="第-2-次握手传回了-ACK，为什么还要传回-SYN？"><a href="#第-2-次握手传回了-ACK，为什么还要传回-SYN？" class="headerlink" title="第 2 次握手传回了 ACK，为什么还要传回 SYN？"></a>第 2 次握手传回了 ACK，为什么还要传回 SYN？</h3><p>接收端传回发送端所发送的 ACK 是为了告诉客户端，我接收到的信息确实就是你所发送的信号了，这表明从客户端到服务端的通信是正常的。而回传 SYN 则是为了建立并确认从服务端到客户端的通信。”</p>
<blockquote>
<p>SYN 同步序列编号(Synchronize Sequence Numbers) 是 TCP/IP 建立连接时使用的握手信号。在客户机和服务器之间建立正常的 TCP 网络连接时，客户机首先发出一个 SYN 消息，服务器使用 SYN-ACK 应答表示接收到了这个消息，最后客户机再以 ACK(Acknowledgement）消息响应。这样在客户机和服务器之间才能建立起可靠的 TCP 连接，数据才可以在客户机和服务器之间传递。</p>
</blockquote>
<h3 id="为什么要四次挥手"><a href="#为什么要四次挥手" class="headerlink" title="为什么要四次挥手"></a>为什么要四次挥手</h3><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019/7/TCP%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B.png" alt="TCP四次挥手"></p>
<p>断开一个 TCP 连接则需要“四次挥手”：</p>
<ul>
<li>客户端-发送一个 FIN，用来关闭客户端到服务器的数据传送</li>
<li>服务器-收到这个 FIN，它发回一 个 ACK，确认序号为收到的序号加 1 。和 SYN 一样，一个 FIN 将占用一个序号</li>
<li>服务器-关闭与客户端的连接，发送一个 FIN 给客户端</li>
<li>客户端-发回 ACK 报文确认，并将确认序号设置为收到序号加 1</li>
</ul>
<p>任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送的时候，则发出连接释放通知，对方确认后就完全关闭了 TCP 连接。</p>
<p>举个例子：A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B 回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话，于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了”，A 回答“知道了”，这样通话才算结束。</p>
<p>上面讲的比较概括，推荐一篇讲的比较细致的文章：<a href="https://blog.csdn.net/qzcsu/article/details/72861891">https://blog.csdn.net/qzcsu/article/details/72861891open in new window</a></p>
<h2 id="TCP-UDP-协议的区别"><a href="#TCP-UDP-协议的区别" class="headerlink" title="TCP, UDP 协议的区别"></a>TCP, UDP 协议的区别</h2><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/tcp-vs-udp.jpg" alt="TCP、UDP协议的区别"></p>
<p>UDP 在传送数据之前不需要先建立连接，远地主机在收到 UDP 报文后，不需要给出任何确认。虽然 UDP 不提供可靠交付，但在某些情况下 UDP 却是一种最有效的工作方式（一般用于即时通信），比如： QQ 语音、 QQ 视频 、直播等等</p>
<p>TCP 提供面向连接的服务。在传送数据之前必须先建立连接，数据传送结束后要释放连接。 TCP 不提供广播或多播服务。由于 TCP 要提供可靠的，面向连接的传输服务（TCP 的可靠体现在 TCP 在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资源），这难以避免增加了许多开销，如确认，流量控制，计时器以及连接管理等。这不仅使协议数据单元的首部增大很多，还要占用许多处理机资源。TCP 一般用于文件传输、发送和接收邮件、远程登录等场景。</p>
<h2 id="TCP-协议如何保证可靠传输"><a href="#TCP-协议如何保证可靠传输" class="headerlink" title="TCP 协议如何保证可靠传输"></a>TCP 协议如何保证可靠传输</h2><ol>
<li>应用数据被分割成 TCP 认为最适合发送的数据块。</li>
<li>TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。</li>
<li><strong>校验和：</strong> TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。</li>
<li>TCP 的接收端会丢弃重复的数据。</li>
<li><strong>流量控制：</strong> TCP 连接的每一方都有固定大小的缓冲空间，TCP 的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 （TCP 利用滑动窗口实现流量控制）</li>
<li><strong>拥塞控制：</strong> 当网络拥塞时，减少数据的发送。</li>
<li><strong>ARQ 协议：</strong> 也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。</li>
<li><strong>超时重传：</strong> 当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。</li>
</ol>
<h3 id="ARQ-协议"><a href="#ARQ-协议" class="headerlink" title="ARQ 协议"></a>ARQ 协议</h3><p><strong>自动重传请求</strong>（Automatic Repeat-reQuest，ARQ）是 OSI 模型中数据链路层和传输层的错误纠正协议之一。它通过使用确认和超时这两个机制，在不可靠服务的基础上实现可靠的信息传输。如果发送方在发送后一段时间之内没有收到确认帧，它通常会重新发送。ARQ 包括停止等待 ARQ 协议和连续 ARQ 协议。</p>
<h4 id="停止等待-ARQ-协议"><a href="#停止等待-ARQ-协议" class="headerlink" title="停止等待 ARQ 协议"></a>停止等待 ARQ 协议</h4><p>停止等待协议是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认（回复 ACK）。如果过了一段时间（超时时间后），还是没有收到 ACK 确认，说明没有发送成功，需要重新发送，直到收到确认后再发下一个分组。</p>
<p>在停止等待协议中，若接收方收到重复分组，就丢弃该分组，但同时还要发送确认。</p>
<p><strong>优缺点：</strong></p>
<ul>
<li><strong>优点：</strong> 简单</li>
<li><strong>缺点：</strong> 信道利用率低，等待时间长</li>
</ul>
<p><strong>1) 无差错情况:</strong></p>
<p>发送方发送分组, 接收方在规定时间内收到, 并且回复确认. 发送方再次发送。</p>
<p><strong>2) 出现差错情况（超时重传）:</strong></p>
<p>停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重传时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为 <strong>自动重传请求 ARQ</strong> 。另外在停止等待协议中若收到重复分组，就丢弃该分组，但同时还要发送确认。<strong>连续 ARQ 协议</strong> 可提高信道利用率。发送维持一个发送窗口，凡位于发送窗口内的分组可连续发送出去，而不需要等待对方确认。接收方一般采用累积确认，对按序到达的最后一个分组发送确认，表明到这个分组位置的所有分组都已经正确收到了。</p>
<p><strong>3) 确认丢失和确认迟到</strong></p>
<ul>
<li><strong>确认丢失</strong> ：确认消息在传输过程丢失。当 A 发送 M1 消息，B 收到后，B 向 A 发送了一个 M1 确认消息，但却在传输过程中丢失。而 A 并不知道，在超时计时过后，A 重传 M1 消息，B 再次收到该消息后采取以下两点措施：1. 丢弃这个重复的 M1 消息，不向上层交付。 2. 向 A 发送确认消息。（不会认为已经发送过了，就不再发送。A 能重传，就证明 B 的确认消息丢失）。</li>
<li><strong>确认迟到</strong> ：确认消息在传输过程中迟到。A 发送 M1 消息，B 收到并发送确认。在超时时间内没有收到确认消息，A 重传 M1 消息，B 仍然收到并继续发送确认消息（B 收到了 2 份 M1）。此时 A 收到了 B 第二次发送的确认消息。接着发送其他数据。过了一会，A 收到了 B 第一次发送的对 M1 的确认消息（A 也收到了 2 份确认消息）。处理如下：1. A 收到重复的确认后，直接丢弃。2. B 收到重复的 M1 后，也直接丢弃重复的 M1。</li>
</ul>
<h4 id="连续-ARQ-协议"><a href="#连续-ARQ-协议" class="headerlink" title="连续 ARQ 协议"></a>连续 ARQ 协议</h4><p>连续 ARQ 协议可提高信道利用率。发送方维持一个发送窗口，凡位于发送窗口内的分组可以连续发送出去，而不需要等待对方确认。接收方一般采用累积确认，对按序到达的最后一个分组发送确认，表明到这个分组为止的所有分组都已经正确收到了。</p>
<p><strong>优缺点：</strong></p>
<ul>
<li><strong>优点：</strong> 信道利用率高，容易实现，即使确认丢失，也不必重传。</li>
<li><strong>缺点：</strong> 不能向发送方反映出接收方已经正确收到的所有分组的信息。 比如：发送方发送了 5 条 消息，中间第三条丢失（3 号），这时接收方只能对前两个发送确认。发送方无法知道后三个分组的下落，而只好把后三个全部重传一次。这也叫 Go-Back-N（回退 N），表示需要退回来重传已经发送过的 N 个消息。</li>
</ul>
<h3 id="滑动窗口和流量控制"><a href="#滑动窗口和流量控制" class="headerlink" title="滑动窗口和流量控制"></a>滑动窗口和流量控制</h3><p><strong>TCP 利用滑动窗口实现流量控制。流量控制是为了控制发送方发送速率，保证接收方来得及接收。</strong> 接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。</p>
<h3 id="拥塞控制"><a href="#拥塞控制" class="headerlink" title="拥塞控制"></a>拥塞控制</h3><p>在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种情况就叫拥塞。拥塞控制就是为了防止过多的数据注入到网络中，这样就可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机，所有的路由器，以及与降低网络传输性能有关的所有因素。相反，流量控制往往是点对点通信量的控制，是个端到端的问题。流量控制所要做到的就是抑制发送端发送数据的速率，以便使接收端来得及接收。</p>
<p>为了进行拥塞控制，TCP 发送方要维持一个 <strong>拥塞窗口(cwnd)</strong> 的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。</p>
<p>TCP 的拥塞控制采用了四种算法，即 <strong>慢开始</strong> 、 <strong>拥塞避免</strong> 、<strong>快重传</strong> 和 <strong>快恢复</strong>。在网络层也可以使路由器采用适当的分组丢弃策略（如主动队列管理 AQM），以减少网络拥塞的发生。</p>
<ul>
<li><strong>慢开始：</strong> 慢开始算法的思路是当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么可能会引起网络阻塞，因为现在还不知道网络的符合情况。经验表明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是由小到大逐渐增大拥塞窗口数值。cwnd 初始值为 1，每经过一个传播轮次，cwnd 加倍。</li>
<li><strong>拥塞避免：</strong> 拥塞避免算法的思路是让拥塞窗口 cwnd 缓慢增大，即每经过一个往返时间 RTT 就把发送放的 cwnd 加 1.</li>
<li><strong>快重传与快恢复：</strong> 在 TCP/IP 中，快速重传和恢复（fast retransmit and recovery，FRR）是一种拥塞控制算法，它能快速恢复丢失的数据包。没有 FRR，如果数据包丢失了，TCP 将会使用定时器来要求传输暂停。在暂停的这段时间内，没有新的或复制的数据包被发送。有了 FRR，如果接收机接收到一个不按顺序的数据段，它会立即给发送机发送一个重复确认。如果发送机接收到三个重复确认，它会假定确认件指出的数据段丢失了，并立即重传这些丢失的数据段。有了 FRR，就不会因为重传时要求的暂停被耽误。 　当有单独的数据包丢失时，快速重传和恢复（FRR）能最有效地工作。当有多个数据信息包在某一段很短的时间内丢失时，它则不能很有效地工作。</li>
</ul>
<h2 id="在浏览器中输入-url-地址-gt-gt-显示主页的过程-面试常客"><a href="#在浏览器中输入-url-地址-gt-gt-显示主页的过程-面试常客" class="headerlink" title="在浏览器中输入 url 地址 -&gt;&gt; 显示主页的过程(面试常客)"></a>在浏览器中输入 url 地址 -&gt;&gt; 显示主页的过程(面试常客)</h2><p>百度好像最喜欢问这个问题。</p>
<blockquote>
<p>打开一个网页，整个过程会使用哪些协议？</p>
</blockquote>
<p>图解（图片来源：《图解 HTTP》）：</p>
<p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/url%E8%BE%93%E5%85%A5%E5%88%B0%E5%B1%95%E7%A4%BA%E5%87%BA%E6%9D%A5%E7%9A%84%E8%BF%87%E7%A8%8B.jpg" alt="img"></p>
<blockquote>
<p>上图有一个错误，请注意，是 OSPF 不是 OPSF。 OSPF（Open Shortest Path First，ospf）开放最短路径优先协议, 是由 Internet 工程任务组开发的路由选择协议</p>
</blockquote>
<p>总体来说分为以下几个过程:</p>
<ol>
<li>DNS 解析</li>
<li>TCP 连接</li>
<li>发送 HTTP 请求</li>
<li>服务器处理请求并返回 HTTP 报文</li>
<li>浏览器解析渲染页面</li>
<li>连接结束</li>
</ol>
<p>具体可以参考下面这篇文章：</p>
<ul>
<li><a href="https://segmentfault.com/a/1190000006879700">https://segmentfault.com/a/1190000006879700open in new window</a></li>
</ul>
<h2 id="状态码"><a href="#状态码" class="headerlink" title="状态码"></a>状态码</h2><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019/7/%E7%8A%B6%E6%80%81%E7%A0%81.png" alt="状态码"></p>
<h2 id="各种协议与-HTTP-协议之间的关系"><a href="#各种协议与-HTTP-协议之间的关系" class="headerlink" title="各种协议与 HTTP 协议之间的关系"></a>各种协议与 HTTP 协议之间的关系</h2><p>一般面试官会通过这样的问题来考察你对计算机网络知识体系的理解。</p>
<p>图片来源：《图解 HTTP》</p>
<p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019/7/%E5%90%84%E7%A7%8D%E5%8D%8F%E8%AE%AE%E4%B8%8EHTTP%E5%8D%8F%E8%AE%AE%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB.png" alt="各种协议与HTTP协议之间的关系"></p>
<h2 id="HTTP-是不保存状态的协议-如何保存用户状态"><a href="#HTTP-是不保存状态的协议-如何保存用户状态" class="headerlink" title="HTTP 是不保存状态的协议, 如何保存用户状态?"></a>HTTP 是不保存状态的协议, 如何保存用户状态?</h2><p>HTTP 是一种不保存状态，即无状态（stateless）协议。也就是说 HTTP 协议自身不对请求和响应之间的通信状态进行保存。那么我们保存用户状态呢？Session 机制的存在就是为了解决这个问题，Session 的主要作用就是通过服务端记录用户的状态。典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了（一般情况下，服务器会在一定时间内保存这个 Session，过了时间限制，就会销毁这个 Session）。</p>
<p>在服务端保存 Session 的方法很多，最常用的就是内存和数据库(比如是使用内存数据库 redis 保存)。既然 Session 存放在服务器端，那么我们如何实现 Session 跟踪呢？大部分情况下，我们都是通过在 Cookie 中附加一个 Session ID 来方式来跟踪。</p>
<p><strong>Cookie 被禁用怎么办?</strong></p>
<p>最常用的就是利用 URL 重写把 Session ID 直接附加在 URL 路径的后面。</p>
<p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/HTTP%E6%98%AF%E6%97%A0%E7%8A%B6%E6%80%81%E7%9A%84.png" alt="HTTP是无状态协议"></p>
<h2 id="Cookie-的作用是什么-和-Session-有什么区别？"><a href="#Cookie-的作用是什么-和-Session-有什么区别？" class="headerlink" title="Cookie 的作用是什么? 和 Session 有什么区别？"></a>Cookie 的作用是什么? 和 Session 有什么区别？</h2><p>Cookie 和 Session 都是用来跟踪浏览器用户身份的会话方式，但是两者的应用场景不太一样。</p>
<p><strong>Cookie 一般用来保存用户信息</strong> 比如 ① 我们在 Cookie 中保存已经登录过的用户信息，下次访问网站的时候页面可以自动帮你把登录的一些基本信息给填了；② 一般的网站都会有保持登录，也就是说下次你再访问网站的时候就不需要重新登录了，这是因为用户登录的时候我们可以存放了一个 Token 在 Cookie 中，下次登录的时候只需要根据 Token 值来查找用户即可(为了安全考虑，重新登录一般要将 Token 重写)；③ 登录一次网站后访问网站其他页面不需要重新登录。<strong>Session 的主要作用就是通过服务端记录用户的状态。</strong> 典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了。</p>
<p>Cookie 数据保存在客户端(浏览器端)，Session 数据保存在服务器端。</p>
<p>Cookie 存储在客户端中，而 Session 存储在服务器上，相对来说 Session 安全性更高。如果要在 Cookie 中存储一些敏感信息，不要直接写入 Cookie 中，最好能将 Cookie 信息加密，然后使用到的时候再去服务器端解密。</p>
<h2 id="URI-和-URL-的区别是什么"><a href="#URI-和-URL-的区别是什么" class="headerlink" title="URI 和 URL 的区别是什么?"></a>URI 和 URL 的区别是什么?</h2><ul>
<li>URI(Uniform Resource Identifier) 是统一资源标志符，可以唯一标识一个资源。</li>
<li>URL(Uniform Resource Locator) 是统一资源定位符，可以提供该资源的路径。它是一种具体的 URI，即 URL 可以用来标识一个资源，而且还指明了如何 locate 这个资源。</li>
</ul>
<p>URI 的作用像身份证号一样，URL 的作用更像家庭住址一样。URL 是一种具体的 URI，它不仅唯一标识资源，而且还提供了定位该资源的信息。</p>
<blockquote>
<p>转载至Javaguide，用来了解网络知识</p>
</blockquote>
]]></content>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title>sql</title>
    <url>/2022/03/24/sql/</url>
    <content><![CDATA[<p><a href="https://www.nowcoder.com/practice/83f84aa5c32b4cf5a75558d02dd7743c?tpId=82&tqId=37924&rp=1&ru=/ta/sql&qru=/ta/sql&difficulty=&judgeStatus=3&tags=/question-ranking">实习广场投递简历分析(三)_牛客题霸_牛客网 (nowcoder.com)</a></p>
<p>2025年 join 2026年 条件是job=job，用substr选择月份=月份，最后从里面选数据就行。</p>
<span id="more"></span>

<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> t1.job,<br>mon1 <span class="hljs-keyword">as</span> first_year_mon,<br>sum1 <span class="hljs-keyword">as</span> first_year_cnt,<br>mon2 <span class="hljs-keyword">as</span> second_year_mon,<br>sum2 <span class="hljs-keyword">as</span> second_year_cnt<br><span class="hljs-keyword">from</span> (<br>    <span class="hljs-keyword">select</span> job,substr(<span class="hljs-type">date</span>,<span class="hljs-number">1</span>,<span class="hljs-number">7</span>)<span class="hljs-keyword">as</span> mon1,<span class="hljs-built_in">sum</span>(num) <span class="hljs-keyword">as</span> sum1<br>    <span class="hljs-keyword">from</span> resume_info<br>    <span class="hljs-keyword">where</span> <span class="hljs-keyword">year</span>(<span class="hljs-type">date</span>)<span class="hljs-operator">=</span><span class="hljs-number">2025</span><br>    <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> job,mon1<br>) t1<br><span class="hljs-keyword">join</span> (<br>    <span class="hljs-keyword">select</span> job,substr(<span class="hljs-type">date</span>,<span class="hljs-number">1</span>,<span class="hljs-number">7</span>)<span class="hljs-keyword">as</span> mon2,<span class="hljs-built_in">sum</span>(num) <span class="hljs-keyword">as</span> sum2<br>    <span class="hljs-keyword">from</span> resume_info<br>    <span class="hljs-keyword">where</span> <span class="hljs-keyword">year</span>(<span class="hljs-type">date</span>)<span class="hljs-operator">=</span><span class="hljs-number">2026</span><br>    <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> job,mon2<br>) t2<br><span class="hljs-keyword">on</span> t1.job <span class="hljs-operator">=</span> t2.job<br><span class="hljs-keyword">and</span> substr(mon1,<span class="hljs-number">5</span>,<span class="hljs-number">7</span>) <span class="hljs-operator">=</span> substr(mon2,<span class="hljs-number">5</span>,<span class="hljs-number">7</span>)<br><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> first_year_mon <span class="hljs-keyword">desc</span>,job <span class="hljs-keyword">desc</span> <br></code></pre></td></tr></table></figure>



<p><a href="https://www.nowcoder.com/practice/b626ff9e2ad04789954c2132c74c0512?tpId=82&tqId=35496&rp=1&ru=/ta/sql&qru=/ta/sql&difficulty=&judgeStatus=3&tags=/question-ranking">考试分数(五)_牛客题霸_牛客网 (nowcoder.com)</a></p>
<p>思路是：使用子查询和窗口函数，只有正序和反序都为中位数即为中位数。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span>,<br><span class="hljs-built_in">count</span>(score) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> job) <span class="hljs-keyword">as</span> total,<br><span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(partiton <span class="hljs-keyword">by</span> job <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> score) <span class="hljs-keyword">as</span> a,<br><span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(partion <span class="hljs-keyword">by</span> job <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> score) <span class="hljs-keyword">as</span> b<br><span class="hljs-keyword">from</span> grade<br></code></pre></td></tr></table></figure>

<p>=&gt;</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">with</span> base <span class="hljs-keyword">as</span>(<br>    <span class="hljs-keyword">select</span> <span class="hljs-operator">*</span>,<br>    <span class="hljs-built_in">count</span>(score) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> job) <span class="hljs-keyword">as</span> total,<br>    <span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> job <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> score) <span class="hljs-keyword">as</span> a,<br>    <span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> job <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> score <span class="hljs-keyword">desc</span>) <span class="hljs-keyword">as</span> b<br>    <span class="hljs-keyword">from</span> grade<br>)<br><span class="hljs-keyword">select</span> id,job,score,b <span class="hljs-keyword">from</span> base <br><span class="hljs-keyword">where</span> a<span class="hljs-operator">&gt;=</span>total<span class="hljs-operator">/</span><span class="hljs-number">2</span> <span class="hljs-keyword">and</span> b<span class="hljs-operator">&gt;=</span>total<span class="hljs-operator">/</span><span class="hljs-number">2</span><br><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> id<br></code></pre></td></tr></table></figure>

<p><a href="https://www.nowcoder.com/practice/7cc3c814329546e89e71bb45c805c9ad?tpId=82&tqId=35085&rp=1&ru=/ta/sql&qru=/ta/sql&difficulty=&judgeStatus=2&tags=/question-ranking">牛客每个人最近的登录日期(二)_牛客题霸_牛客网 (nowcoder.com)</a></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> u.name,c.name,l.date<br><span class="hljs-keyword">from</span> login l<br><span class="hljs-keyword">join</span> <span class="hljs-keyword">user</span> u <span class="hljs-keyword">on</span> u.id<span class="hljs-operator">=</span>l.user_id<br><span class="hljs-keyword">join</span> client c <span class="hljs-keyword">on</span> c.id<span class="hljs-operator">=</span>l.client_id<br><br><span class="hljs-keyword">SQL</span><br><span class="hljs-keyword">where</span> (l.user_id,<span class="hljs-type">date</span>) <br><span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> user_id,<span class="hljs-built_in">max</span>(<span class="hljs-type">date</span>) <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id)<br><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> u.name<br></code></pre></td></tr></table></figure>

<p>=&gt;</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> u.name,c.name,l.date<br><span class="hljs-keyword">from</span> login l<br><span class="hljs-keyword">join</span> <span class="hljs-keyword">user</span> u <span class="hljs-keyword">on</span> u.id<span class="hljs-operator">=</span>l.user_id<br><span class="hljs-keyword">join</span> client c <span class="hljs-keyword">on</span> c.id<span class="hljs-operator">=</span>l.client_id<br><span class="hljs-keyword">where</span> (l.user_id,<span class="hljs-type">date</span>) <br><span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> user_id,<span class="hljs-built_in">max</span>(<span class="hljs-type">date</span>) <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id)<br><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> u.name<br></code></pre></td></tr></table></figure>

<p><a href="https://www.nowcoder.com/practice/16d41af206cd4066a06a3a0aa585ad3d?tpId=82&tqId=35086&rp=1&ru=/ta/sql&qru=/ta/sql&difficulty=&judgeStatus=3&tags=/question-ranking">牛客每个人最近的登录日期(三)_牛客题霸_牛客网 (nowcoder.com)</a></p>
<p>选出来符合要求的user_id做count/所有的user_id</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> round(<span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> user_id)<span class="hljs-operator">/</span><br>             (<span class="hljs-keyword">select</span> <span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> user_id) <span class="hljs-keyword">from</span> login),<span class="hljs-number">3</span>)<br><span class="hljs-keyword">from</span> login<br></code></pre></td></tr></table></figure>

<p>user_id,date,date处理使用DATE_ADD(date,INTERVAL 1 DAY)</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">where</span> (user_id,<span class="hljs-type">date</span>)<br><span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> user_id,DATE_ADD(<span class="hljs-built_in">min</span>(<span class="hljs-type">date</span>),<span class="hljs-type">INTERVAL</span> <span class="hljs-number">1</span> <span class="hljs-keyword">DAY</span>)<br>   <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span>  <span class="hljs-keyword">by</span> user_id)<br></code></pre></td></tr></table></figure>

<p>=&gt;</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> round(<span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> user_id)<span class="hljs-operator">/</span><br>             (<span class="hljs-keyword">select</span> <span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> user_id) <span class="hljs-keyword">from</span> login),<span class="hljs-number">3</span>)<br><span class="hljs-keyword">from</span> login<br><span class="hljs-keyword">where</span> (user_id,<span class="hljs-type">date</span>)<br><span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> user_id,DATE_ADD(<span class="hljs-built_in">min</span>(<span class="hljs-type">date</span>),<span class="hljs-type">INTERVAL</span> <span class="hljs-number">1</span> <span class="hljs-keyword">DAY</span>)<br>   <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span>  <span class="hljs-keyword">by</span> user_id)<br></code></pre></td></tr></table></figure>

<p><a href="https://www.nowcoder.com/practice/e524dc7450234395aa21c75303a42b0a?tpId=82&tqId=35087&rp=1&ru=/ta/sql&qru=/ta/sql&difficulty=&judgeStatus=3&tags=/question-ranking">牛客每个人最近的登录日期(四)_牛客题霸_牛客网 (nowcoder.com)</a></p>
<p>思路是窗口函数获取登录的次数，最早就是首次登录，然后分组对t_rank=1的求和即可</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> a.date,<span class="hljs-built_in">sum</span>(<span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> t_rank<span class="hljs-operator">=</span><span class="hljs-number">1</span> <span class="hljs-keyword">then</span> <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">end</span>) <span class="hljs-keyword">new</span> <br><span class="hljs-keyword">from</span> <br>(<span class="hljs-keyword">select</span> <span class="hljs-type">date</span>, <span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> user_id <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> <span class="hljs-type">date</span>) t_rank<br><span class="hljs-keyword">from</span> login) a<br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-type">date</span>;<br></code></pre></td></tr></table></figure>

<p><a href="https://www.nowcoder.com/practice/348afda488554ceb922efd2f3effc427?tpId=82&tqId=37919&rp=1&ru=/ta/sql&qru=/ta/sql&difficulty=&judgeStatus=3&tags=/question-ranking">牛客的课程订单分析(五)_牛客题霸_牛客网 (nowcoder.com)</a></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> user_id,<br>       <span class="hljs-built_in">min</span>(<span class="hljs-type">date</span>) first_buy_date,<br>       <span class="hljs-built_in">min</span>(date2) second_buy_date,<br>       <span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>) cnt<br><span class="hljs-keyword">from</span><br>(<span class="hljs-keyword">select</span> <span class="hljs-operator">*</span>,<br>        <span class="hljs-built_in">lead</span>(<span class="hljs-type">date</span>) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> user_id <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> <span class="hljs-type">date</span>) date2<br>        <span class="hljs-keyword">from</span> order_info<br>        <span class="hljs-keyword">where</span> <span class="hljs-type">date</span><span class="hljs-operator">&gt;</span><span class="hljs-string">&#x27;2025-10-15&#x27;</span><br>        <span class="hljs-keyword">and</span> status<span class="hljs-operator">=</span><span class="hljs-string">&#x27;completed&#x27;</span><br>        <span class="hljs-keyword">and</span> product_name <span class="hljs-keyword">in</span> (<span class="hljs-string">&#x27;C++&#x27;</span>,<span class="hljs-string">&#x27;Java&#x27;</span>,<span class="hljs-string">&#x27;Python&#x27;</span>)<br>)a<br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id<br><span class="hljs-keyword">having</span> <span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>)<span class="hljs-operator">&gt;=</span><span class="hljs-number">2</span><br><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> user_id<br></code></pre></td></tr></table></figure>

<p><a href="https://www.nowcoder.com/practice/048ed413ac0e4cf4a774b906fc87e0e7?tpId=82&tqId=38864&rp=1&ru=/ta/sql&qru=/ta/sql&difficulty=&judgeStatus=3&tags=/question-ranking">网易云音乐推荐(网易校招笔试真题)_牛客题霸_牛客网 (nowcoder.com)</a></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> music_name<br><span class="hljs-keyword">from</span> music<br><span class="hljs-keyword">where</span> id <span class="hljs-keyword">IN</span> (<span class="hljs-keyword">select</span> <span class="hljs-keyword">distinct</span> music_id<br><span class="hljs-keyword">from</span> music_likes<br><span class="hljs-keyword">where</span> user_id <span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> follower_id<br><span class="hljs-keyword">from</span> follow<br><span class="hljs-keyword">where</span> user_id <span class="hljs-operator">=</span> <span class="hljs-number">1</span>))<br><span class="hljs-keyword">and</span> id <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> music_id<br><span class="hljs-keyword">from</span> music_likes<br><span class="hljs-keyword">where</span> user_id <span class="hljs-operator">=</span> <span class="hljs-number">1</span>)<br><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> id<br></code></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper</title>
    <url>/2022/03/12/Zookeeper/</url>
    <content><![CDATA[<p>本文部分内容摘录于《从Paxos到Zookeeper分布式一致性原理与实践》</p>
<span id="more"></span>

<h3 id="分布式架构"><a href="#分布式架构" class="headerlink" title="分布式架构"></a>分布式架构</h3><h4 id="集中式到分布式的演变"><a href="#集中式到分布式的演变" class="headerlink" title="集中式到分布式的演变"></a><strong>集中式到分布式的演变</strong></h4><p>集中式向分布式演变是必然的</p>
<h4 id="ACID到CAP-BASE"><a href="#ACID到CAP-BASE" class="headerlink" title="ACID到CAP/BASE"></a><strong>ACID到CAP/BASE</strong></h4><p><strong>ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸。</strong></p>
<p>事务具有四个特征：原子性(Atomicity)，一致性(Consistency),隔离性(Isolation)和持久性(Durability)，简称ACID</p>
<p><strong>CAP</strong> 是 <strong>Consistency（一致性）</strong>、<strong>Availability（可用性）</strong>、<strong>Partition Tolerance（分区容错性）</strong> </p>
<p>CAP 定理（CAP theorem）指出对于一个分布式系统来说，当设计读写操作时，只能同时满足以下三点中的两个：</p>
<ul>
<li><strong>一致性（Consistency）</strong> : 所有节点访问同一份最新的数据副本</li>
<li><strong>可用性（Availability）</strong>: 非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。</li>
<li><strong>分区容错性（Partition tolerance）</strong> : 分布式系统出现网络分区的时候，仍然能够对外提供服务。</li>
</ul>
<p>不是所谓的“3 选 2”,简单的表述为：“一致性、可用性、分区容忍性三者你只能同时达到其中两个，不可能同时达到”。</p>
<blockquote>
<p><strong>当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能 2 选 1。也就是说当网络分区之后 P 是前提，决定了 P 之后才有 C 和 A 的选择。也就是说分区容错性（Partition tolerance）我们是必须要实现的。</strong></p>
<p>简而言之就是：CAP 理论中分区容错性 P 是一定要满足的，在此基础上，只能满足可用性 A 或者一致性 C。</p>
</blockquote>
<p><strong>分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构。</strong> 比如 ZooKeeper、HBase 就是 CP 架构，Cassandra、Eureka 就是 AP 架构，Nacos 不仅支持 CP 架构也支持 AP 架构。</p>
<p><strong>为啥不可能选择 CA 架构呢？</strong> 举个例子：若系统出现“分区”，系统中的某个节点在进行写操作。为了保证 C， 必须要禁止其他节点的读写操作，这就和 A 发生冲突了。如果为了保证 A，其他节点的读写操作正常的话，那就和 C 发生冲突了</p>
<p><strong>选择 CP 还是 AP 的关键在于当前的业务场景，没有定论，比如对于需要确保强一致性的场景如银行一般会选择保证 CP 。如果网络分区正常的话（系统在绝大部分时候所处的状态），也就说不需要保证 P 的时候，C 和 A 能够同时保证。</strong></p>
<p><strong>BASE</strong> 是 <strong>Basically Available（基本可用）</strong> 、<strong>Soft-state（软状态）</strong> 和 <strong>Eventually Consistent（最终一致性）</strong> 三个短语的缩写，BASE 理论是对 CAP 中一致性 C 和可用性 A 权衡的结果。</p>
<p>即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。</p>
<blockquote>
<p>也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。</p>
</blockquote>
<p><strong>BASE 理论本质上是对 CAP 的延伸和补充，更具体地说，是对 CAP 中 AP 方案的一个补充。</strong></p>
<p>AP 方案只是在系统发生分区的时候放弃一致性，而不是永远放弃一致性。在分区故障恢复后，系统应该达到最终一致性。这一点其实就是 BASE 理论延伸的地方。</p>
<p><strong>1. 基本可用</strong></p>
<p>基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。</p>
<p><strong>什么叫允许损失部分可用性呢？</strong></p>
<ul>
<li><strong>响应时间上的损失</strong>: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为 3 s。</li>
<li><strong>系统功能上的损失</strong>：正常情况下，用户可以使用系统的全部功能，但是由于系统访问量突然剧增，系统的部分非核心功能无法使用。</li>
</ul>
<p><strong>2. 软状态</strong></p>
<p>软状态指允许系统中的数据存在中间状态（<strong>CAP 理论中的数据不一致</strong>），并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。</p>
<p><strong>3. 最终一致性</strong></p>
<p>最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。</p>
<blockquote>
<p>分布式一致性的 3 种级别：</p>
<ol>
<li><strong>强一致性</strong> ：系统写入了什么，读出来的就是什么。</li>
<li><strong>弱一致性</strong> ：不一定可以读取到最新写入的值，也不保证多少时间之后读取到的数据是最新的，只是会尽量保证某个时刻达到数据一致的状态。</li>
<li><strong>最终一致性</strong> ：弱一致性的升级版，系统会保证在一定时间内达到数据一致的状态。</li>
</ol>
<p><strong>业界比较推崇是最终一致性级别，但是某些对数据一致要求十分严格的场景比如银行转账还是要保证强一致性。</strong></p>
</blockquote>
<p>那实现最终一致性的具体方式是什么呢?</p>
<blockquote>
<ul>
<li><strong>读时修复</strong> : 在读取数据时，检测数据的不一致，进行修复。比如 Cassandra 的 Read Repair 实现，具体来说，在向 Cassandra 系统查询数据的时候，如果检测到不同节点 的副本数据不一致，系统就自动修复数据。</li>
<li><strong>写时修复</strong> : 在写入数据，检测数据的不一致时，进行修复。比如 Cassandra 的 Hinted Handoff 实现。具体来说，Cassandra 集群的节点之间远程写数据的时候，如果写失败 就将数据缓存下来，然后定时重传，修复数据的不一致性。</li>
<li><strong>异步修复</strong> : 这个是最常用的方式，通过定时对账检测副本数据的一致性，并修复。</li>
</ul>
</blockquote>
<p>比较推荐 <strong>写时修复</strong>，这种方式对性能消耗比较低。</p>
<h3 id="一致性协议"><a href="#一致性协议" class="headerlink" title="一致性协议"></a>一致性协议</h3><p><strong>两阶段提交和三阶段提交</strong></p>
<h4 id="两阶段提交"><a href="#两阶段提交" class="headerlink" title="两阶段提交"></a>两阶段提交</h4><blockquote>
<p>两阶段提交用来保持在分布式系统架构里的原子性和一致性，关于事务特性见前文，本文后面的PPT里也有简单补充。<br>其中包含的角色为：协调者，参与者</p>
</blockquote>
<p><strong>第一阶段：</strong></p>
<figure class="highlight"><table><tr><td class="code"><pre><code class="hljs">提交事务请求<br></code></pre></td></tr></table></figure>

<ol>
<li><p>询问<br> 协调者向所有参与者发送事务内容，询问是否提交，开始等待响应。</p>
</li>
<li><p>执行<br> 参与者执行事务操作，写undo 和 redo信息到事务日志，</p>
</li>
<li><p>参与者向协调者返回询问的响应<br> 如果参与者成功执行了事务，反馈给协调者Yes，表示可以执行；如果参与者执行失败，返回给协调者No响应，表示不可以执行。</p>
</li>
</ol>
<p><strong>第二阶段：</strong></p>
<figure class="highlight"><table><tr><td class="code"><pre><code class="hljs">执行事务提交<br></code></pre></td></tr></table></figure>

<p>结果一：成功执行提交</p>
<ol>
<li>发送提交请求<br> 协调者收到所有的Yes反馈，向所有参与者发送commit请求</li>
<li>事务提交<br> 参与者收到协调者的commit请求，开始正式提交事务，完成提交后释放占用的事务资源</li>
<li>反馈提交结果<br> 参与者完成正式提交之后，给协调者发送ACK响应</li>
<li>完成事务<br> 协调者接收到所有参与者的ACK反馈后，完成事务</li>
</ol>
<p>结果二：执行事务中断</p>
<ol>
<li>发送回滚请求<br> 协调者收到参与者No反馈，向所有参与者发送rollback请求</li>
<li>事务回滚<br> 参与者接收到rollback请求，按照undo日志执行回滚，完成后释放占用的事务资源。</li>
<li>反馈回滚结果<br> 参与者完成回滚后，给协调者发送ACK消息 </li>
<li>中断事务<br> 协调者接收到所有参与者的ACK消息后，完成整体事务的中断</li>
</ol>
<blockquote>
<p>两阶段产生的问题：</p>
<ul>
<li>同步阻塞：一次事务请求中，参与者在等待其他参与者响应时无法做任何其他操作</li>
<li>单点问题： 协调者处于单点位置，尤其在第二阶段（提交或者中断）会导致资源锁定。如发生故障，参与者占有的资源无法释放。</li>
<li>数据不一致：在最终commit的过程中，如果发生网络故障或者其他原因导致只发送了部分commit，会造成只有部分执行了commit，产生数据的不一致，也有称作“脑裂”</li>
<li>设计太过保守：如果协调者无法获取参与者的信息，则只能通过自身的超时机制来确认是否中断事务，容错机制不是特别完善，任何一个节点失败都会导致整个事务的失败，失败影响的范围较大。</li>
</ul>
</blockquote>
<h4 id="三阶段提交"><a href="#三阶段提交" class="headerlink" title="三阶段提交"></a>三阶段提交</h4><blockquote>
<p>三阶段针对两阶段的缺点，做出改进，将2PC的第一阶段拆分，并且在协调者和参与者中都引入超时机制。</p>
</blockquote>
<p><strong>第一阶段：</strong></p>
<figure class="highlight"><table><tr><td class="code"><pre><code class="hljs">请求提交canCommit<br></code></pre></td></tr></table></figure>

<ol>
<li><p>事务询问<br> 协调者向所有参与者发送一个canCommit请求，询问是否可以执行事务提交操作。</p>
</li>
<li><p>各个参与者向协调者反馈询问的响应<br> 参与者在收到canCommit询问请求后，确认是否可以执行事务，可以回复Yes，并进入预备状态；否则回复No响应。</p>
</li>
</ol>
<p><strong>第二阶段：</strong></p>
<figure class="highlight"><table><tr><td class="code"><pre><code class="hljs">预备提交preCommit<br></code></pre></td></tr></table></figure>

<p>结果1：在所有参与者回复Yes之后，执行事务预提交</p>
<ol>
<li>协调者发送预提交请求<br> 协调者向所有参与者发送PreCommit请求，协调者进入Prepare状态。</li>
<li>事务预提交<br> 参与者收到preCommit之后，执行事务操作，写redo和undo信息到事务日志中。</li>
<li>反馈预提交结果<br> 参与者如果成功执行了事务操作，反馈给协调者ACK响应，并等待协调者最终执行，commit或者abort（注意此处的commit或者abort是对preCommit的处理，不是最终提交）</li>
</ol>
<p>结果2：中断preCommit，任何一个参与者向协调者发送No，或者协调者等待响应超时，则开始中断。</p>
<ol>
<li>发送预中断请求<br> 协调者向所有参与者发送abort请求。</li>
<li>中断事务<br> 参与者收到协调者的abort请求，或者参与者等待请求超时，开始执行中断。</li>
</ol>
<p><strong>第三阶段</strong></p>
<figure class="highlight"><table><tr><td class="code"><pre><code class="hljs">正式提交doCommit<br></code></pre></td></tr></table></figure>

<p>结果1：执行正式提交</p>
<ol>
<li>发送正式提交请求<br> 协调者正常工作，收到所有参与者的ACK响应，将从prepare转到commit状态，并向所有参与者发送doCommit请求。</li>
<li>事务正式提交<br> 参与者接收到doCommit请求后，正式提交事务，并释放锁定的事务资源。</li>
<li>反馈正式提交结果<br> 参与者如果成功执行了事务操作，反馈给协调者ACK响应，并等待协调者最终执行，commit或者abort（此处是正式提交阶段的commit或者abort）</li>
<li>完成事务<br> 协调者接收到所有参与者的ACK消息之后，完成事务。</li>
</ol>
<p>结果2：中断事务 只要协调者收到任意一个参与者的No响应，则开始中断事务</p>
<ol>
<li>发送中断请求<br> 协调者正常工作，向所有参与者发送abort请求。</li>
<li>事务回滚<br> 参与者收到abort请求之后，利用undo信息执行事务回滚操作，完成后释放锁定的事务资源。</li>
<li>反馈回滚结果<br> 参与者完成事务回滚之后，向协调者发送ACK消息</li>
<li>中断事务<br> 协调者接收到所有参与者的ACK消息后，完成中断事务</li>
</ol>
<blockquote>
<p>三阶段的优劣：</p>
<ul>
<li>减小了参与者阻塞的范围，在出现单点故障后仍然能达成一致。</li>
<li>和2PC类似，如果参与者收到preCommit之后，发送网络异常，参与者仍旧会执行事务提交，导致数据的不一致。</li>
<li>实现较为复杂</li>
<li>网络通信量大，会增高延迟</li>
</ul>
</blockquote>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h08kkbe7coj30e808674k.jpg"></p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h08kjw381cj30hy0ctt9k.jpg"></p>
<p>补充：</p>
<blockquote>
<p>redo和undo日志：<br> redo日志用来重放，找到所有已经commit的事务，根据redo log重做，记&gt;录在MySQL存储页的物理修改；undo用来回滚，找到所有uncommitted的事务，利用undo来回滚，记录逻辑的修改（insert-&gt;delete)。另外undo也有自己的redo日志。对应的过程参考下图，过程A先于过程B：</p>
</blockquote>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h08kjijtdoj30hu09uaa5.jpg"></p>
<h3 id="Zookeeper初识"><a href="#Zookeeper初识" class="headerlink" title="Zookeeper初识"></a>Zookeeper初识</h3><h4 id="什么是Zookeeper"><a href="#什么是Zookeeper" class="headerlink" title="什么是Zookeeper"></a>什么是Zookeeper</h4><p>Zookeeper 是一个开源的分布式协调服务，。Zookeeper 可以用于实现分布式系统中常见的发布/订阅、负载均衡、命令服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能，特性如下：</p>
<blockquote>
<p><strong>顺序一致性</strong>：从一个客户端发起的事务请求，最终都会严格按照其发起顺序被应用到 Zookeeper 中；</p>
<p><strong>原子性</strong>：所有事务请求的处理结果在整个集群中所有机器上都是一致的；不存在部分机器应用了该事务，而另一部分没有应用的情况；</p>
<p><strong>单一视图</strong>：所有客户端看到的服务端数据模型都是一致的；</p>
<p><strong>可靠性</strong>：一旦服务端成功应用了一个事务，则其引起的改变会一直保留，直到被另外一个事务所更改；</p>
<p><strong>实时性</strong>：一旦一个事务被成功应用后，Zookeeper 可以保证客户端立即可以读取到这个事务变更后的最新状态的数据。</p>
</blockquote>
<p>Zookeeper 致力于为那些高吞吐的大型分布式系统提供一个高性能、高可用、且具有严格顺序访问控制能力的分布式协调服务,它具有以下四个目标：</p>
<ol>
<li>简单的数据模型</li>
</ol>
<p>Zookeeper 通过树形结构来存储数据，它由一系列被称为 ZNode 的数据节点组成，类似于常见的文件系统。不过和常见的文件系统不同，Zookeeper 将数据全量存储在内存中，以此来实现高吞吐，减少访问延迟。</p>
<ol start="2">
<li>构建集群</li>
</ol>
<p>可以由一组 Zookeeper 服务构成 Zookeeper 集群，集群中每台机器都会单独在内存中维护自身的状态，并且每台机器之间都保持着通讯，只要集群中有半数机器能够正常工作，那么整个集群就可以正常提供服务。</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h08ksnfdk3j30go055aaj.jpg"></p>
<ol start="3">
<li>顺序访问</li>
</ol>
<p>对于来自客户端的每个更新请求，Zookeeper 都会分配一个全局唯一的递增 ID，这个 ID 反映了所有事务请求的先后顺序。</p>
<ol start="4">
<li>高性能高可用</li>
</ol>
<p>ZooKeeper 将数据存全量储在内存中以保持高性能，并通过服务集群来实现高可用，由于 Zookeeper 的所有更新和删除都是基于事务的，所以其在读多写少的应用场景中有着很高的性能表现。</p>
<h4 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h4><p>集群角色</p>
<p>Zookeeper 集群中的机器分为以下三种角色：</p>
<ul>
<li><strong>Leader</strong> ：为客户端提供读写服务，并维护集群状态，它是由集群选举所产生的；</li>
<li><strong>Follower</strong> ：为客户端提供读写服务，并定期向 Leader 汇报自己的节点状态。同时也参与写操作“过半写成功”的策略和 Leader 的选举；</li>
<li><strong>Observer</strong> ：为客户端提供读写服务，并定期向 Leader 汇报自己的节点状态，但不参与写操作“过半写成功”的策略和 Leader 的选举，因此 Observer 可以在不影响写性能的情况下提升集群的读性能。</li>
</ul>
<p>会话</p>
<p>Zookeeper 客户端通过 TCP 长连接连接到服务集群，会话 (Session) 从第一次连接开始就已经建立，之后通过心跳检测机制来保持有效的会话状态。通过这个连接，客户端可以发送请求并接收响应，同时也可以接收到 Watch 事件的通知。</p>
<p>关于会话中另外一个核心的概念是 sessionTimeOut(会话超时时间)，当由于网络故障或者客户端主动断开等原因，导致连接断开，此时只要在会话超时时间之内重新建立连接，则之前创建的会话依然有效。</p>
<p>数据节点</p>
<p>Zookeeper 数据模型是由一系列基本数据单元 <code>Znode</code>(数据节点) 组成的节点树，其中根节点为 <code>/</code>。每个节点上都会保存自己的数据和节点信息。Zookeeper 中节点可以分为两大类：</p>
<ul>
<li><strong>持久节点</strong> ：节点一旦创建，除非被主动删除，否则一直存在；</li>
<li><strong>临时节点</strong> ：一旦创建该节点的客户端会话失效，则所有该客户端创建的临时节点都会被删除。</li>
</ul>
<p>临时节点和持久节点都可以添加一个特殊的属性：<code>SEQUENTIAL</code>，代表该节点是否具有递增属性。如果指定该属性，那么在这个节点创建时，Zookeeper 会自动在其节点名称后面追加一个由父节点维护的递增数字。</p>
<p>节点信息</p>
<p>每个 ZNode 节点在存储数据的同时，都会维护一个叫做 <code>Stat</code> 的数据结构，里面存储了关于该节点的全部状态信息。</p>
<p>Watcher</p>
<p>Zookeeper 中一个常用的功能是 Watcher(事件监听器)，它允许用户在指定节点上针对感兴趣的事件注册监听，当事件发生时，监听器会被触发，并将事件信息推送到客户端。该机制是 Zookeeper 实现分布式协调服务的重要特性。</p>
<p>ACL</p>
<p>Zookeeper 采用 ACL(Access Control Lists) 策略来进行权限控制，类似于 UNIX 文件系统的权限控制。它定义了如下五种权限：</p>
<blockquote>
<ul>
<li>CREATE：允许创建子节点；</li>
<li>READ：允许从节点获取数据并列出其子节点；</li>
<li>WRITE：允许为节点设置数据；</li>
<li>DELETE：允许删除子节点；</li>
<li>ADMIN：允许为节点设置权限。</li>
</ul>
</blockquote>
<h4 id="Zookeeper的ZAB协议"><a href="#Zookeeper的ZAB协议" class="headerlink" title="Zookeeper的ZAB协议"></a>Zookeeper的ZAB协议</h4><p><strong>ZAB协议与数据一致性</strong></p>
<p>ZAB 协议是 Zookeeper 专门设计的一种支持崩溃恢复的原子广播协议。通过该协议，Zookeeper 基于主从模式的系统架构来保持集群中各个副本之间数据的一致性。具体如下：</p>
<p>Zookeeper 使用一个单一的主进程来接收并处理客户端的所有事务请求，并采用原子广播协议将数据状态的变更以事务 Proposal 的形式广播到所有的副本进程上去。如下图：</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h08kyekp6nj30gz06wdg1.jpg"></p>
<p>具体流程如下：</p>
<p>所有的事务请求必须由唯一的 Leader 服务来处理，Leader 服务将事务请求转换为事务 Proposal，并将该 Proposal 分发给集群中所有的 Follower 服务。如果有半数的 Follower 服务进行了正确的反馈，那么 Leader 就会再次向所有的 Follower 发出 Commit 消息，要求将前一个 Proposal 进行提交。</p>
<p><strong>ZAB协议的内容</strong></p>
<p>ZAB 协议包括两种基本的模式，分别是崩溃恢复和消息广播：</p>
<p>崩溃恢复:</p>
<p>当整个服务框架在启动过程中，或者当 Leader 服务器出现异常时，ZAB 协议就会进入恢复模式，通过过半选举机制产生新的 Leader，之后其他机器将从新的 Leader 上同步状态，当有过半机器完成状态同步后，就退出恢复模式，进入消息广播模式。</p>
<p>消息广播:</p>
<p>ZAB 协议的消息广播过程使用的是原子广播协议。在整个消息的广播过程中，Leader 服务器会每个事物请求生成对应的 Proposal，并为其分配一个全局唯一的递增的事务 ID(ZXID)，之后再对其进行广播。具体过程如下：</p>
<p>Leader 服务会为每一个 Follower 服务器分配一个单独的队列，然后将事务 Proposal 依次放入队列中，并根据 FIFO(先进先出) 的策略进行消息发送。Follower 服务在接收到 Proposal 后，会将其以事务日志的形式写入本地磁盘中，并在写入成功后反馈给 Leader 一个 Ack 响应。当 Leader 接收到超过半数 Follower 的 Ack 响应后，就会广播一个 Commit 消息给所有的 Follower 以通知其进行事务提交，之后 Leader 自身也会完成对事务的提交。而每一个 Follower 则在接收到 Commit 消息后，完成事务的提交。</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h08kyv4bebj30f606adfx.jpg"></p>
<h3 id="Zookeeper典型场景"><a href="#Zookeeper典型场景" class="headerlink" title="Zookeeper典型场景"></a>Zookeeper典型场景</h3><h4 id="数据的发布-订阅"><a href="#数据的发布-订阅" class="headerlink" title="数据的发布/订阅"></a>数据的发布/订阅</h4><p>数据的发布/订阅系统，通常也用作配置中心。在分布式系统中，你可能有成千上万个服务节点，如果想要对所有服务的某项配置进行更改，由于数据节点过多，你不可逐台进行修改，而应该在设计时采用统一的配置中心。之后发布者只需要将新的配置发送到配置中心，所有服务节点即可自动下载并进行更新，从而实现配置的集中管理和动态更新。</p>
<p>Zookeeper 通过 Watcher 机制可以实现数据的发布和订阅。分布式系统的所有的服务节点可以对某个 ZNode 注册监听，之后只需要将新的配置写入该 ZNode，所有服务节点都会收到该事件。</p>
<h4 id="命名服务"><a href="#命名服务" class="headerlink" title="命名服务"></a>命名服务</h4><p>在分布式系统中，通常需要一个全局唯一的名字，如生成全局唯一的订单号等，Zookeeper 可以通过顺序节点的特性来生成全局唯一 ID，从而可以对分布式系统提供命名服务。</p>
<h4 id="Master选举"><a href="#Master选举" class="headerlink" title="Master选举"></a>Master选举</h4><p>分布式系统一个重要的模式就是主从模式 (Master/Salves)，Zookeeper 可以用于该模式下的 Matser 选举。可以让所有服务节点去竞争性地创建同一个 ZNode，由于 Zookeeper 不能有路径相同的 ZNode，必然只有一个服务节点能够创建成功，这样该服务节点就可以成为 Master 节点。</p>
<h4 id="分布式锁"><a href="#分布式锁" class="headerlink" title="分布式锁"></a>分布式锁</h4><p>可以通过 Zookeeper 的临时节点和 Watcher 机制来实现分布式锁，这里以排它锁为例进行说明：</p>
<p>分布式系统的所有服务节点可以竞争性地去创建同一个临时 ZNode，由于 Zookeeper 不能有路径相同的 ZNode，必然只有一个服务节点能够创建成功，此时可以认为该节点获得了锁。其他没有获得锁的服务节点通过在该 ZNode 上注册监听，从而当锁释放时再去竞争获得锁。锁的释放情况有以下两种：</p>
<blockquote>
<ul>
<li><p>当正常执行完业务逻辑后，客户端主动将临时 ZNode 删除，此时锁被释放；</p>
</li>
<li><p>当获得锁的客户端发生宕机时，临时 ZNode 会被自动删除，此时认为锁已经释放。</p>
</li>
</ul>
</blockquote>
<p>当锁被释放后，其他服务节点则再次去竞争性地进行创建，但每次都只有一个服务节点能够获取到锁，这就是排他锁。</p>
<h4 id="集群管理"><a href="#集群管理" class="headerlink" title="集群管理"></a>集群管理</h4><p>Zookeeper 还能解决大多数分布式系统中的问题：</p>
<blockquote>
<ul>
<li>如可以通过创建临时节点来建立心跳检测机制。如果分布式系统的某个服务节点宕机了，则其持有的会话会超时，此时该临时节点会被删除，相应的监听事件就会被触发。</li>
<li>分布式系统的每个服务节点还可以将自己的节点状态写入临时节点，从而完成状态报告或节点工作进度汇报。</li>
<li>通过数据的订阅和发布功能，Zookeeper 还能对分布式系统进行模块的解耦和任务的调度。</li>
<li>通过监听机制，还能对分布式系统的服务节点进行动态上下线，从而实现服务的动态扩容。</li>
</ul>
</blockquote>
<h3 id="Zookeeper技术内幕"><a href="#Zookeeper技术内幕" class="headerlink" title="Zookeeper技术内幕"></a>Zookeeper技术内幕</h3><p><strong>系统模型</strong></p>
<p><strong>会话</strong></p>
<p><strong>Leader选举</strong></p>
<p><strong>服务器角色介绍</strong></p>
<p><strong>请求处理</strong></p>
<p><strong>数据与存储</strong></p>
]]></content>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据项目</title>
    <url>/2022/03/12/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A1%B9%E7%9B%AE/</url>
    <content><![CDATA[<blockquote>
<p>大数据新闻热点项目</p>
</blockquote>
<span id="more"></span>

<h3 id="项目需求设计与分析"><a href="#项目需求设计与分析" class="headerlink" title="项目需求设计与分析"></a>项目需求设计与分析</h3><p><strong>目标</strong></p>
<p>1、完成大数据项目的架构设计，安装部署，架构继承与开发、用户可视化交互设计</p>
<p>2、完成实时在线数据分析</p>
<p>3、完成离线数据分析</p>
<p><strong>具体功能</strong></p>
<p>1、捕获用户浏览日志信息</p>
<p>2、实时分析前20名流量最高的新闻话题</p>
<p>3、实时统计当前线上已曝光的新闻话题</p>
<p>4、统计哪个时段用户浏览量最高</p>
<p>5、报表</p>
<p><strong>项目技术栈</strong></p>
<p>Hadoop2.x、Zookeeper、Flume、Hive、Hbase、Kafka、Spark2.x、SparkStreaming、MySQL、Hue、J2EE、websoket、Echarts</p>
<p><strong>项目架构</strong></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h06thvxvapj30op0ee40r.jpg"></p>
<p><strong>集群资源规划</strong></p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h06tiyis0nj30m20ck75k.jpg"></p>
<h3 id="环境准备与设置"><a href="#环境准备与设置" class="headerlink" title="环境准备与设置"></a>环境准备与设置</h3><p><strong>Linux重要配置</strong></p>
<p><strong>1）设置ip地址</strong> 项目视频里面直接使用界面修改ip比较方便，如果Linux没有安装操作界面，需要使用命令：vi /etc/sysconfig/network-scripts/ifcfg-eth0 来修改ip地址，然后重启网络服务service network restart即可。 参考链接：<a href="https://www.willxu.xyz/2018/08/23/hadoop/1%E3%80%81vmware%E4%B8%8A%E7%BD%91%E9%85%8D%E7%BD%AE/">请点击。</a></p>
<p><strong>2）创建用户</strong> 大数据项目开发中，一般不直接使用root用户，需要我们创建新的用户来操作，比如kfk。 a）创建用户命令：adduser kfk b）设置用户密码命令：passwd kfk</p>
<p><strong>3）文件中设置主机名</strong> Linux系统的主机名默认是localhost，显然不方便后面集群的操作，我们需要手动修改Linux系统的主机名。 a）查看主机名命令：hostname b）修改主机名称 vi /etc/sysconfig/network NETWORKING=yes HOSTNAME=bigdata-pro01.kfk.com</p>
<p><strong>4）主机名映射</strong> 如果想通过主机名访问Linux系统，还需要配置主机名跟ip地址之间的映射关系。 vi /etc/hosts 192.168.31.151 bigdata-pro01.kfk.com 配置完成之后，reboot重启Linux系统即可。 如果需要在windows也能通过hostname访问Linux系统，也需要在windows下的hosts文件中配置主机名称与ip之间的映射关系。在windows系统下找到C:\WINDOWS\system32\drivers\etc\路径，打开HOSTS文件添加如下内容： 192.168.31.151 bigdata-pro01.kfk.com</p>
<p><strong>5）root用户下设置无密码用户切换</strong> 在Linux系统中操作是，kfk用户经常需要操作root用户权限下的文件，但是访问权限受限或者需要输入密码。修改/etc/sudoers这个文件添加如下代码，即可实现无密码用户切换操作。 vi /etc/sudoers 。。。添加如下内容即可 kfk ALL=(root)NOPASSWD:ALL</p>
<p><strong>6）关闭防火墙</strong> 我们都知道防火墙对我们的服务器是进行一种保护，但是有时候防火墙也会给我们带来很大的麻烦。 比如它会妨碍hadoop集群间的相互通信，所以我们需要关闭防火墙。 那么我们永久关闭防火墙的方法如下: vi /etc/sysconfig/selinux SELINUX=disabled 保存、重启后，验证机器的防火墙是否已经关闭。 a）查看防火墙状态：service iptables status b）打开防火墙：service iptables start c）关闭防火墙：service iptables stop</p>
<p><strong>7）卸载Linux本身自带的jdk</strong> 一般情况下jdk需要我们手动安装兼容的版本，此时Linux自带的jdk需要手动删除掉，具体操作如下所示： a）查看Linux自带的jdk rpm -qa|grep java b）删除Linux自带的jdk rpm -e –nodeps [jdk进程名称1 jdk进程名称2 …]</p>
<p><strong>克隆虚拟机并进行相关的配置</strong></p>
<p>前面我们已经做好了Linux的系统常规设置，接下来需要克隆虚拟机并进行相关的配置。 <strong>1）kfk用户下创建我们将要使用的各个目录</strong></p>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk">软件目录<br>mkdir <span class="hljs-regexp">/opt/</span>softwares<br>模块目录<br>mkdir <span class="hljs-regexp">/opt/m</span>odules<br>工具目录<br>mkdir <span class="hljs-regexp">/opt/</span>tools<br>数据目录<br>mkdir <span class="hljs-regexp">/opt/</span>datas<br></code></pre></td></tr></table></figure>

<p><strong>2）jdk安装(1.7以上，1.9以下)</strong> 大数据平台运行环境依赖JVM，所以我们需要提前安装和配置好jdk。 前面我们已经安装了64位的centos系统，所以我们的jdk也需要安装64位的，与之相匹配 下面步骤给的是1.7的。我自己用的是jdk1.8.0_191 a）将jdk安装包通过工具上传到/opt/softwares目录下 b）解压jdk安装包</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-comment">#解压命令</span><br><span class="hljs-attribute">tar</span> -zxf jdk-<span class="hljs-number">7</span>u67-linux-x64.tar.gz /opt/modules/<br><span class="hljs-comment">#查看解压结果</span><br><span class="hljs-attribute">ls</span><br><span class="hljs-attribute">jdk1</span>.<span class="hljs-number">7</span>.<span class="hljs-number">0</span>_67<br></code></pre></td></tr></table></figure>

<p>c）配置Java 环境变量</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><code class="hljs routeros">vi /etc<span class="hljs-built_in">/profile</span><br><span class="hljs-built_in"></span><span class="hljs-built_in">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/opt/modules/jdk1.7.0_67<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-variable">$PATH</span>:$JAVA_HOME/bin<br></code></pre></td></tr></table></figure>

<p>d）查看Java是否安装成功</p>
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">java </span>-version<br><span class="hljs-keyword">java </span>version <span class="hljs-string">&quot;1.7.0_67&quot;</span><br><span class="hljs-keyword">Java(TM) </span>SE Runtime Environment (<span class="hljs-keyword">build </span><span class="hljs-number">1</span>.<span class="hljs-number">7</span>.<span class="hljs-number">0</span>_67-<span class="hljs-keyword">b15)</span><br><span class="hljs-keyword"></span><span class="hljs-keyword">Java </span>HotSpot(TM) <span class="hljs-number">64</span>-<span class="hljs-keyword">Bit </span>Server VM (<span class="hljs-keyword">build </span><span class="hljs-number">24</span>.<span class="hljs-number">79</span>-<span class="hljs-keyword">b02, </span>mixed mode)<br></code></pre></td></tr></table></figure>

<p><strong>3）克隆虚拟机</strong></p>
<p>在克隆虚拟机之前，需要关闭虚拟机，然后右键选中虚拟机——》选择管理——》选择克隆——》选择下一步——》选择下一步——》选择创建完整克隆，下一步——》选择克隆虚拟机位置（提前创建好），修改虚拟机名称为Hadoop-Linux-pro-2，然后选择完成即可。 然后使用同样的方式创建第三个虚拟机Hadoop-Linux-pro-3。</p>
<p><strong>4）修改克隆虚拟机配置</strong> 克隆完虚拟机Hadoop-Linux-pro-2和Hadoop-Linux-pro-3之后，可以按照Hadoop-Linux-pro-1的方式配置好ip地址、hostname，以及ip地址与hostname之间的关系</p>
<h3 id="Hadoop2-X分布式集群部署"><a href="#Hadoop2-X分布式集群部署" class="headerlink" title="Hadoop2.X分布式集群部署"></a>Hadoop2.X分布式集群部署</h3><p><strong>配置要点</strong></p>
<p><strong>1）hadoop2.x版本下载及安装</strong> 官网下载2.x版本就好</p>
<p><strong>2）hadoop配置要点</strong> 参考官网给的例子：<a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html</a> 网站左下角有全部配置信息 <strong>1）hadoop2.x分布式集群配置-HDFS</strong><br>安装hdfs需要修改4个配置文件：hadoop-env.sh、core-site.xml、hdfs-site.xml和slaves <strong>2）hadoop2.x分布式集群配置-YARN</strong> 安装yarn需要修改4个配置文件：yarn-env.sh、mapred-env.sh、yarn-site.xml和mapred-site.xml</p>
<p><strong>3）分发配置到节点</strong> 最好先SCP设置成无密码访问，需要生成秘钥，自己百度吧 hadoop相关配置在第一个节点配置好之后，可以通过脚本命令分发给另外两个节点即可，具体操作如下所示。 将安装包分发给第二个节点 scp -r hadoop-2.5.0 <a href="mailto:kaf@bigdata-pro02.kfk.com">kaf@bigdata-pro02.kfk.com</a>:/opt/modules/ 将安装包分发给第三个节点 scp -r hadoop-2.5.0 <a href="mailto:kaf@bigdata-pro02.kfk.com">kaf@bigdata-pro02.kfk.com</a>:/opt/modules/</p>
<p><strong>4）HDFS启动集群运行测试</strong> hdfs相关配置好之后，可以启动hdfs集群。 1.格式化NameNode 通过命令：bin/hdfs namenode -format 格式化NameNode。 2.启动各个节点机器服务 1）启动NameNode命令：sbin/hadoop-daemon.sh start namenode 2) 启动DataNode命令：sbin/hadoop-daemon.sh start datanode 3）启动ResourceManager命令：sbin/yarn-daemon.sh start resourcemanager 4）启动NodeManager命令：sbin/yarn-daemon.sh start resourcemanager 5）启动log日志命令：sbin/mr-jobhistory-daemon.sh start historyserver</p>
<p><strong>5）YARN集群运行MapReduce程序测试</strong> 前面hdfs和yarn都启动起来之后，可以通过运行WordCount程序检测一下集群是否能run起来。 集群自带的WordCount程序执行命令：bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0.jar wordcount input output</p>
<p><strong>6）ssh无秘钥登录</strong> （可以提前设置好） 在集群搭建的过程中，需要不同节点分发文件，那么节点间分发文件每次都需要输入密码，比较麻烦。另外在hadoop 集群启动过程中，也需要使用批量脚本统一启动各个节点服务，此时也需要节点之间实现无秘钥登录。具体操作步骤如下所示： 1.主节点上创建 .ssh 目录，然后生成公钥文件id_rsa.pub和私钥文件id_rsa mkdir .ssh ssh-keygen -t rsa 2.拷贝公钥到各个机器 ssh-copy-id bigdata-pro1.kfk.com ssh-copy-id bigdata-pro2.kfk.com ssh-copy-id bigdata-pro3.kfk.com 3.测试ssh连接 ssh bigdata-pro1.kfk.com ssh bigdata-pro2.kfk.com ssh bigdata-pro3.kfk.com 4.测试hdfs ssh无秘钥登录做好之后，可以在主节点通过一键启动命令，启动hdfs各个节点的服务，具体操作如下所示： sbin/start-dfs.sh 如果yarn和hdfs主节点共用，配置一个节点即可。否则，yarn也需要单独配置ssh无秘钥登录。</p>
<p><strong>7）配置集群内机器时间同步（使用Linux ntp进行）</strong> 选择一台机器作为时间服务器，比如bigdata-pro1.kfk.com节点。 1.查看ntp服务是否已经存在 sudo rpm -qa|grep ntp 2.ntp服务相关操作 1）查看ntp状态 sudo service ntpd status 2）启动ntp sudo service ntpd start 3）关闭ntp sudo service ntpd stop 3.设置ntp随机器启动 sudo chkconfig ntpd on 4.修改ntp配置文件 vi /etc/ntp.conf 释放注释并将ip地址修改为 restrict 192.168.31.151 mask 255.255.255.0 nomodify notrap 注释掉以下命令行 server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst 释放以下命令行 server 127.127.1.0 #local clock fudge 127.127.1.0 stratum 10 重启ntp服务 sudo service ntpd restart 5.修改服务器时间</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-comment">#设置当前日期</span><br><span class="hljs-attribute">sudo</span> date -s <span class="hljs-number">2017</span>-<span class="hljs-number">06</span>-<span class="hljs-number">16</span><br><span class="hljs-comment">#设置当前时间</span><br><span class="hljs-attribute">sudo</span> date -s <span class="hljs-number">22</span>:<span class="hljs-number">06</span>:<span class="hljs-number">00</span><br></code></pre></td></tr></table></figure>

<p>6.其他节点手动同步主服务器时间</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk"><span class="hljs-comment">#查看ntp位置</span><br>which ntpdate<br><span class="hljs-regexp">/usr/</span>sbin/ntpdate<br><span class="hljs-number">1</span>）手动同步bigdata-pro2.kfk.com节点时间<br>sudo <span class="hljs-regexp">/usr/</span>sbin/ntpdate bigdata-pro2.kfk.com<br><span class="hljs-number">2</span>）手动同步bigdata-pro3.kfk.com节点时间<br>sudo <span class="hljs-regexp">/usr/</span>sbin/ntpdate bigdata-pro3.kfk.com<br><span class="hljs-number">7</span>.其他节点定时同步主服务器时间<br>bigdata-pro2.kfk.com和bigdata-pro3.kfk.com节点分别切换到root用户， 通过crontab -e 命令，每<span class="hljs-number">10</span>分钟同步一次主服务器节点的时间。<br>crontab -e<br><span class="hljs-comment">#定时，每隔10分钟同步bigdata-pro1.kfk.com服务器时间</span><br><span class="hljs-number">0</span>-<span class="hljs-number">59</span><span class="hljs-regexp">/10 * * * *  /u</span>sr<span class="hljs-regexp">/sbin/</span>ntpdate bigdata-pro1.kfk.com<br></code></pre></td></tr></table></figure>

<h3 id="Zookeeper分布式集群部署"><a href="#Zookeeper分布式集群部署" class="headerlink" title="Zookeeper分布式集群部署"></a>Zookeeper分布式集群部署</h3><p><strong>Zookeeper部署步骤</strong></p>
<p><strong>1）下载Zookeeper</strong> 这里选择cdh版本的zookeeper-3.4.5-cdh5.10.0.tar.gz，将下载好的安装包上传至bigdata-pro01.kfk.com节点的/opt/softwares目录下。</p>
<p><strong>2）解压Zookeeper</strong> tar -zxf zookeeper-3.4.5-cdh5.10.0.tar.gz -C /opt/modules/ </p>
<p><strong>3）修改配置</strong> </p>
<ol>
<li>复制配置文件 cp conf/zoo_sample.cfg zoo.cfg</li>
<li>修改配置文件zoo.cfg</li>
</ol>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">vi</span> zoo.cfg<br><span class="hljs-comment">#这个时间是作为Zookeeper服务器之间或客户端与服务器之间维持心跳的时间间隔</span><br><span class="hljs-attribute">tickTime</span>=<span class="hljs-number">2000</span><br><span class="hljs-comment">#配置 Zookeeper 接受客户端初始化连接时最长能忍受多少个心跳时间间隔数。</span><br><span class="hljs-attribute">initLimit</span>=<span class="hljs-number">10</span><br><span class="hljs-comment">#Leader 与 Follower 之间发送消息，请求和应答时间长度</span><br><span class="hljs-attribute">syncLimit</span>=<span class="hljs-number">5</span><br><span class="hljs-comment">#数据目录需要提前创建</span><br><span class="hljs-attribute">dataDir</span>=/opt/modules/zookeeper-<span class="hljs-number">3</span>.<span class="hljs-number">4</span>.<span class="hljs-number">5</span>-cdh5.<span class="hljs-number">10</span>.<span class="hljs-number">0</span>/zkData<br><span class="hljs-comment">#访问端口号</span><br><span class="hljs-attribute">clientPort</span>=<span class="hljs-number">2181</span><br><span class="hljs-comment">#server.每个节点服务编号=服务器ip地址：集群通信端口：选举端口</span><br><span class="hljs-attribute">server</span>.<span class="hljs-number">1</span>=bigdata-pro01.kfk.com:<span class="hljs-number">2888</span>:<span class="hljs-number">3888</span><br><span class="hljs-attribute">server</span>.<span class="hljs-number">2</span>=bigdata-pro02.kfk.com:<span class="hljs-number">2888</span>:<span class="hljs-number">3888</span><br><span class="hljs-attribute">server</span>.<span class="hljs-number">3</span>=bigdata-pro03.kfk.com:<span class="hljs-number">2888</span>:<span class="hljs-number">3888</span><br></code></pre></td></tr></table></figure>

<p><strong>4）分发各个节点</strong> 将Zookeeper安装配置分发到其他两个节点，具体操作如下所示： scp -r zookeeper-3.4.5-cdh5.10.0/ bigdata-pro02.kfk.com:/opt/modules/ scp -r zookeeper-3.4.5-cdh5.10.0/ bigdata-pro03.kfk.com:/opt/modules/ </p>
<p><strong>5）创建相关目录和文件</strong> </p>
<ol>
<li>在3个节点上分别创建数据目录 mkdir /opt/modules/zookeeper-3.4.5-cdh5.10.0/zkData </li>
<li>在各个节点的数据存储目录下创建myid文件，并且编辑每个机器的myid内容为</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#切换到数据目录</span><br><span class="hljs-built_in">cd</span> /opt/modules/zookeeper-3.4.5-cdh5.10.0/zkData<br><span class="hljs-comment">#bigdata-pro01.kfk.com节点</span><br><span class="hljs-built_in">touch</span> myid<br>vi myid<br>1<br><span class="hljs-comment">#bigdata-pro02.kfk.com节点</span><br><span class="hljs-built_in">touch</span> myid<br>vi myid<br>2<br><span class="hljs-comment">#bigdata-pro03.kfk.com节点</span><br><span class="hljs-built_in">touch</span> myid<br>vi myid<br>3<br></code></pre></td></tr></table></figure>

<p><strong>6）启动Zookeeper服务</strong> </p>
<ol>
<li>各个节点使用如下命令启动Zookeeper服务 bin/zkServer.sh start </li>
<li>查看各个节点服务状态 bin/zkServer.sh status 不是follower </li>
<li>关闭各个节点服务 bin/zkServer.sh stop </li>
<li>查看Zookeeper目录树结构 bin/zkCli.sh</li>
</ol>
<h3 id="Hadoop高可用配置-HA"><a href="#Hadoop高可用配置-HA" class="headerlink" title="Hadoop高可用配置(HA)"></a>Hadoop高可用配置(HA)</h3><p><strong>HA原理</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h06tt0ow1oj30dp0bo3zb.jpg"></p>
<p>当发生故障时，Active的 NN 挂掉后，Standby NN 会在它成为Active NN 前，读取所有的JN里面的修改日志，这样就能高可靠的保证与挂掉的NN的目录镜像树一致，然后无缝的接替它的职责，维护来自客户端请求，从而达到一个高可用的目的。</p>
<p><strong>HDFS-HA配置</strong></p>
<p>1）修改hdfs-site.xml配置文件 </p>
<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.nameservices<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>ns<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.namenodes.ns<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>nn1,nn2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.rpc-address.ns.nn1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com:8020<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.rpc-address.ns.nn2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro02.kfk.com:8020<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.http-address.ns.nn1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com:50070<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>       <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.http-address.ns.nn2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>       <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro02.kfk.com:50070<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>qjournal://bigdata-pro01.kfk.com:8485;bigdata-pro02.kfk.com:8485;bigdata-pro03.kfk.com:8485/ns<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>       <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>       <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/opt/modules/hadoop-2.6.0/data/jn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled.ns<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>sshfence<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/home/kfk/.ssh/id_rsa<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>3<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.permissions.enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>false<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure>

<p>2）修改core-site.xml配置文件</p>
<figure class="highlight dust"><table><tr><td class="code"><pre><code class="hljs dust"><span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://ns<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>kfk<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>	</span><br><span class="language-xml">	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/opt/modules/hadoop-2.6.0/data/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.name.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file://$</span><span class="hljs-template-variable">&#123;hadoop.tmp.dir&#125;</span><span class="language-xml">/dfs/name<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>ha.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com:2181,bigdata-pro02.kfk.com:2181,bigdata-pro03.kfk.com:2181<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span></span><br></code></pre></td></tr></table></figure>

<p>3）将修改的配置分发到其他节点</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk">scp hdfs-site.xml bigdata-pro02.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc<span class="hljs-regexp">/hadoop/</span><br>scp hdfs-site.xml bigdata-pro03.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc<span class="hljs-regexp">/hadoop/</span><br>scp core-site.xml bigdata-pro02.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc<span class="hljs-regexp">/hadoop/</span><br>scp core-site.xml bigdata-pro03.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc<span class="hljs-regexp">/hadoop/</span><br></code></pre></td></tr></table></figure>

<p><strong>HDFS-HA自动故障转移测试</strong></p>
<p>1）在所有节点启动zookeeper cd /opt/modules/zookeeper-3.4.5-cdh5.10.0/ sbin/zkServer.sh start bin/hdfs zkfc -formatZK （第一次使用zkfc需要格式化） </p>
<p>2）启动hdfs bin/hdfs namenode -format （第一次使用hdfs需要格式化，在namenode） sbin/start-dfs.sh （会在各个节点上启动namenode/datanode/journalnode） </p>
<p>3）在HA的namenode节点上启动zkfc线程（两个namenode都要启动） sbin/hadoop-daemon.sh start zkfc 查看两个namenode状态一个是active(先启动zkfc的)，一个是standy，查看网页。 <a href="http://bigdata-pro01.kfk.com:50070/">http://bigdata-pro01.kfk.com:50070</a> <a href="http://bigdata-pro02.kfk.com:50070/">http://bigdata-pro02.kfk.com:50070</a> </p>
<p>4）上传文件到hdfs bin/hdfs dfs -mkdir /usr bin/hdfs dfs -put /opt/modules/hadoop-2.6.0/etc/hadoop/hdfs-site.xml /usr 在网页中可以看到 </p>
<p>5）杀死active的namenode </p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h06twirurij30au032t91.jpg"></p>
<p>6）再次查看namenode状态 应该完成了主备切换。原来的standy变成了active.</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h06txd90zdj30py096dgy.jpg"></p>
<h3 id="HDFS-HA所遇到的问题（看输出日志和查看日志）"><a href="#HDFS-HA所遇到的问题（看输出日志和查看日志）" class="headerlink" title="HDFS-HA所遇到的问题（看输出日志和查看日志）"></a>HDFS-HA所遇到的问题（看输出日志和查看日志）</h3><p><strong>1）输出提示：无法解析bigdata-pro03.kfk.com:2181</strong> 原因：因为我的core-site.xml配置文件写错了,参数一栏不能有换行，要不然读的不对的。</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-section">&lt;property&gt;</span><br>	<span class="hljs-section">&lt;name&gt;</span><span class="hljs-attribute">ha</span>.zookeeper.quorum&lt;/name&gt;<br>	<span class="hljs-section">&lt;value&gt;</span><span class="hljs-attribute">bigdata</span>-pro01.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro02.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro03.kfk.com:<span class="hljs-number">2181</span>&lt;/value&gt;<br><span class="hljs-section">&lt;/property&gt;</span><br></code></pre></td></tr></table></figure>

<p><strong>2） sbin/start-dfs.sh 启动不成功</strong> 因为这个启动需要配置ssh，所以 （1）在节点1上 ssh-keygen ssh-copy-id bigdata-pro1.kfk.com (包括自己的也要ssh) ssh-copy-id bigdata-pro2.kfk.com ssh-copy-id bigdata-pro3.kfk.com （2）测试ssh连接 ssh bigdata-pro1.kfk.com ssh bigdata-pro2.kfk.com ssh bigdata-pro3.kfk.com </p>
<p><strong>3） namenode准备切换失败</strong> bigdata-pro1.kfk.com可以竞选成active，但是杀掉bigdata-pro1.kfk.com，而bigdata-pro2.kfk.com不会竞选成active，仍然是standby。 查看bigdata-pro2.kfk.com日志： tail -10f hadoop-kfk-zkfc-bigdata-pro02.kfk.com.log </p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h06txpr4o5j30q10ftqds.jpg"></p>
<p><strong>红线部分说明，在bigdata-pro2.kfk.com准备选举时，需要对pro1进行fence，但是失败了，原因是ssh失败，说明在节点2上没法ssh到节点1上，所以需要在节点2上进行ssh-keygen,然后拷贝到节点1，这样就解决了</strong></p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h06tyjius2j30qd07043b.jpg"></p>
<p><strong>YARN-HA原理</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h06tzghav5j30hp0azt99.jpg"></p>
<p>ResourceManager HA 由一对Active，Standby结点构成，通过RMStateStore存储内部数据和主要应用的数据及标记。 目前支持的可替代的RMStateStore实现有：基于内存的MemoryRMStateStore，基于文件系统的FileSystemRMStateStore，及基于zookeeper的ZKRMStateStore。 ResourceManager HA的架构模式同NameNode HA的架构模式基本一致，数据共享由RMStateStore，而ZKFC成为 ResourceManager进程的一个服务，非独立存在。</p>
<p><strong>YARN-HA配置</strong></p>
<p><strong>1）修改yarn-site.xml配置文件</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>10000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>rs<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>rm1,rm2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro02.kfk.com<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com:2181,bigdata-pro02.kfk.com:2181,bigdata-pro03.kfk.com:2181<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>	<br><br>	<br><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure>

<p><strong>2）分发至其他节点</strong> </p>
<p>​    <code>scp yarn-site.xml bigdata-pro02.kfk.com:/opt/modules/hadoop-2.6.0/etc/hadoop/</code></p>
<p><code> scp yarn-site.xml bigdata-pro03.kfk.com:/opt/modules/hadoop-2.6.0/etc/hadoop/</code></p>
<p><strong>YARN-HA故障转移测试</strong></p>
<ol>
<li>在rm1节点上启动yarn服务 sbin/start-yarn.sh </li>
<li>在rm2节点上启动ResourceManager服务 sbin/yarn-daemon.sh start resourcemanager </li>
<li>查看yarn的web界面 <a href="http://bigdata-pro01.kfk.com:8088/">http://bigdata-pro01.kfk.com:8088</a> <a href="http://bigdata-pro02.kfk.com:8088/">http://bigdata-pro02.kfk.com:8088</a> </li>
<li>上传wordcount所需的文件到hdfs并执行MapReduce例子 bin/hdfs dfs -put data/wc /usr/kfk/data<br>bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /usr/kfk/data/wc /usr/kfk/data/wc.out </li>
<li>执行到一半的时候，kill掉rm1上的resourcemanager 任务会转移到rm2继续处理 这是bigdata-pro01.kfk.com输出的日志（额外打开一个bigdata-pro01.kfk.com进行kill）</li>
</ol>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h06u1030pij30mb01xgmb.jpg"></p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h06u1grjbnj31cb0b2jut.jpg"></p>
<h3 id="HBase分布式部署"><a href="#HBase分布式部署" class="headerlink" title="HBase分布式部署"></a>HBase分布式部署</h3><p><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fynxzha2acj30hr053weh.jpg"><br>1、解压安装到/opt/modules/<br>2、修改配置文件<br><strong>a.hbase-env.sh</strong><br>配置jdk<br>export JAVA_HOME=/opt/modules/jdk1.8.0_191<br>使用外部的Zookeeper<br>export HBASE_MANAGES_ZK=false<br><strong>b.hbase-site.xml</strong><br>这里采用hadoop高可用下的配置</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.rootdir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://ns/hbase<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.cluster.distributed<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com,bigdata-pro02.kfk.com,bigdata-pro03.kfk.com<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure>
<p><strong>c.regionservers</strong><br>bigdata-pro01.kfk.com<br>bigdata-pro02.kfk.com<br>bigdata-pro03.kfk.com</p>
<p><strong>3、将hadoop中hdfs-site.xml和core-site.xml拷贝到hbase的conf下</strong><br>要不然会启动失败，具体日志如下：不认识ns，因为ns在hadoop中配置的<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fyny4rdsrfj30pu0cjdh8.jpg"><br>4、将hbase配置分发到各个节点<br>scp -r hbase-1.0.0-cdh5.4.0 bigdata-pro02.kfk.com:/opt/modules/<br>scp -r hbase-1.0.0-cdh5.4.0 bigdata-pro03.kfk.com:/opt/modules/</p>
<p><strong>HBase启动与测试</strong></p>
<ol>
<li>先启动zookeeper<pre><code class="hljs">zkServer.sh start
</code></pre>
</li>
<li>启动高可用下的hdfs<pre><code class="hljs">sbin/start-dfs.sh （会在各个节点上启动namenode/datanode/journalnode）
</code></pre>
在HA的namenode节点上启动zkfc线程（两个namenode都要启动）<br>sbin/hadoop-daemon.sh start zkfc</li>
<li>启动hbase<br>bin/start-hbase.sh</li>
<li>查看HBase Web界面<br>bigdata-pro01.kfk.com:60010/</li>
<li>HBase的master高可用测试</li>
</ol>
<figure class="highlight crmsh"><table><tr><td class="code"><pre><code class="hljs crmsh">在bigdata-pro02.kfk.com上启动<span class="hljs-literal">master</span>,<br>./hbase-daemon.sh <span class="hljs-literal">start</span> <span class="hljs-keyword">master</span><br><span class="hljs-title">然后杀死bigdata-pro01</span>.kfk.com的Hmaster<br>zookeeper会自动切换<span class="hljs-literal">master</span><br></code></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fynycepyczj30xp0cl3zb.jpg"></p>
<p><strong>HBase的shell测试</strong></p>
<p>1、启动shell<br>bin/hbase shell<br>2、创建表<br>create ‘weblogs’,’info’<br>3、列出表<br>list<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fyososol82j306q02o0sk.jpg"></p>
<h3 id="Kafka分布式部署"><a href="#Kafka分布式部署" class="headerlink" title="Kafka分布式部署"></a>Kafka分布式部署</h3><p>1）解压<br>tar -zxf kafka_2.10-0.9.0.0.tgz  -C /opt/modules/<br>2）配置server.properties文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs propertis">#节点唯一标识<br>broker.id=1<br><br>listeners=PLAINTEXT://bigdata-pro01.kfk.com:9092<br>#默认端口号<br>port=9092<br>#主机名绑定<br>host.name=bigdata-pro01.kfk.com<br>#Kafka数据目录<br>log.dirs=/opt/modules/kafka_2.10-0.9.0.0/kafka-logs<br>#配置Zookeeper<br>zookeeper.connect=bigdata-pro01.kfk.com:2181,bigdata-pro02.kfk.com:2181,bigdata-pro03.kfk.com:2181<br></code></pre></td></tr></table></figure>
<p>3）配置zookeeper.properties文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs propertis">#Zookeeper的数据存储路径与Zookeeper集群配置保持一致<br>dataDir=/opt/modules/zookeeper-3.4.5-cdh5.10.0/zkData<br></code></pre></td></tr></table></figure>

<p>4）配置consumer.properties文件</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">#配置Zookeeper地址<br>zookeeper.connect=bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro03<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span><br></code></pre></td></tr></table></figure>
<p>5）配置producer.properties文件</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">#配置Kafka集群地址  ,分布在三台机器上<br>metadata<span class="hljs-selector-class">.broker</span>.list=bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">9092</span>,bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">9092</span>,bigdata-pro03<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">9092</span><br></code></pre></td></tr></table></figure>
<p>6）拷贝<br>scp -r kafka_2.10-0.9.0.0 bigdata-pro02.kfk.com:/opt/modules/<br>scp -r kafka_2.10-0.9.0.0 bigdata-pro03.kfk.com:/opt/modules/<br>7）修改另外两个节点的server.properties</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-comment">#bigdata-pro02.kfk.com节点</span><br><span class="hljs-attr">broker.id</span>=<span class="hljs-number">2</span><br><span class="hljs-attr">listeners</span>=PLAINTEXT://bigdata-pro02.kfk.com:<span class="hljs-number">9092</span><br><span class="hljs-attr">host.name</span>=bigdata-pro02.kfk.com<br><span class="hljs-comment">#bigdata-pro03.kfk.com节点</span><br><span class="hljs-attr">broker.id</span>=<span class="hljs-number">3</span><br><span class="hljs-attr">listeners</span>=PLAINTEXT://bigdata-pro03.kfk.com:<span class="hljs-number">9092</span><br><span class="hljs-attr">host.name</span>=bigdata-pro03.kfk.com<br></code></pre></td></tr></table></figure>

<p><strong>kafka测试</strong></p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus"><span class="hljs-number">1</span>、所有节点启动zk<br>bin/zkServer<span class="hljs-selector-class">.sh</span> start<br><span class="hljs-number">2</span>、各个节点启动Kafka集群<br>bin/kafka-server-start<span class="hljs-selector-class">.sh</span> config/server<span class="hljs-selector-class">.properties</span> &amp;<br><span class="hljs-number">3</span>、创建topic<br>bin/kafka-topics<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--zookeeper</span> localhost:<span class="hljs-number">2181</span> <span class="hljs-attr">--create</span> <span class="hljs-attr">--topic</span> test <span class="hljs-attr">--replication-factor</span> <span class="hljs-number">1</span> <span class="hljs-attr">--partitions</span> <span class="hljs-number">1</span><br><span class="hljs-number">4</span>、查看topic<br>bin/kafka-topics<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--zookeeper</span> localhost:<span class="hljs-number">2181</span> –list<br><br>bin/kafka-topics<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--describe</span> <span class="hljs-attr">--zookeeper</span> localhost:<span class="hljs-number">2181</span> <span class="hljs-attr">--topic</span> test<br>结果：<br>        Topic:test      PartitionCount:<span class="hljs-number">1</span>        ReplicationFactor:<span class="hljs-number">1</span>     Configs:<br>        Topic: test     Partition: <span class="hljs-number">0</span>    Leader: <span class="hljs-number">2</span>       Replicas: <span class="hljs-number">2</span>     Isr: <span class="hljs-number">2</span><br><span class="hljs-number">5</span>、生产者生产数据（节点<span class="hljs-number">1</span>）<br>bin/kafka-console-producer<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--broker-list</span> bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">9092</span> <span class="hljs-attr">--topic</span> test<br><span class="hljs-number">6</span>、消费者消费数据（节点<span class="hljs-number">2</span>）<br>bin/kafka-console-consumer<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--zookeeper</span> bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span> <span class="hljs-attr">--topic</span> test <span class="hljs-attr">--from-beginning</span><br></code></pre></td></tr></table></figure>
<p>说下分区和消费关系：<br>一个主题可以有多个分区，具体分区方法有多种；关于消费，有消费组的概念。一种是指定消费组（每个消费者的组名一致），那么每个分区对应一个消费者；二是指定消费组（每个消费者的组名不一致），那么所有分区每个消息都会送至各个小组的消费者；三是不指定消费组，那么每条消息会发给消费组中一个消费者。</p>
<h3 id="Flume搭建部署"><a href="#Flume搭建部署" class="headerlink" title="Flume搭建部署"></a>Flume搭建部署</h3><p><strong>（先部署和设置了节点2和3采集部分，节点1的汇总分发后面继续）</strong><br>每一步都可以去查官方资料：官方地址：<a href="http://flume.apache.org/">http://flume.apache.org/</a></p>
<p>1、解压Flume<br>tar -zxf apache-flume-1.7.0-bin.tar.gz  -C /opt/modules/</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><code class="hljs routeros">vi flume-env.sh<br>配置下环境变量问题<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/opt/modules/jdk1.8.0_191<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">HADOOP_HOME</span>=/opt/modules/hadoop-2.6.0<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">HBASE_HOME</span>=/opt/modules/hbase-1.0.0-cdh5.4.0<br></code></pre></td></tr></table></figure>
<p>2、将flume分发到其他两个节点<br>scp -r flume-1.7.0-bin bigdata-pro02.kfk.com:/opt/modules/<br>scp -r flume-1.7.0-bin bigdata-pro03.kfk.com:/opt/modules/<br>3、flume agent-2采集节点服务配置（在bigdata-pro02.kfk.com）<br>三个部分：sources、channels、sinks<br>/opt/datas/weblogs.log是我们要采集的日志</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">vi flume-conf<span class="hljs-selector-class">.properties</span><br><br>agent2<span class="hljs-selector-class">.sources</span> = r1<br>agent2<span class="hljs-selector-class">.channels</span> = c1<br>agent2<span class="hljs-selector-class">.sinks</span> = k1<br><br>agent2<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.type</span> = exec<br>agent2<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.command</span> = tail -F /opt/datas/weblog-flume<span class="hljs-selector-class">.log</span><br>agent2<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.channels</span> = c1<br><br>agent2<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.type</span> = memory<br>agent2<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.capacity</span> = <span class="hljs-number">10000</span><br>agent2<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.transactionCapacity</span> = <span class="hljs-number">10000</span><br>agent2<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.keep-alive</span> = <span class="hljs-number">5</span><br><br>agent2<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.type</span> = avro<br>agent2<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.channel</span> = c1<br>agent2<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.hostname</span> = bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span><br>agent2<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.port</span> = <span class="hljs-number">5555</span><br></code></pre></td></tr></table></figure>
<p>4、flume agent-3采集节点服务配置（在bigdata-pro03.kfk.com）</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">vi flume-conf<span class="hljs-selector-class">.properties</span><br><br>agent3<span class="hljs-selector-class">.sources</span> = r1<br>agent3<span class="hljs-selector-class">.channels</span> = c1<br>agent3<span class="hljs-selector-class">.sinks</span> = k1<br><br>agent3<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.type</span> = exec<br>agent3<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.command</span> = tail -F /opt/datas/weblog-flume<span class="hljs-selector-class">.log</span><br>agent3<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.channels</span> = c1<br><br>agent3<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.type</span> = memory<br>agent3<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.capacity</span> = <span class="hljs-number">10000</span><br>agent3<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.transactionCapacity</span> = <span class="hljs-number">10000</span><br>agent3<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.keep-alive</span> = <span class="hljs-number">5</span><br><br>agent3<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.type</span> = avro<br>agent3<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.channel</span> = c1<br>agent3<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.hostname</span> = bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span><br>agent3<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.port</span> = <span class="hljs-number">5555</span><br></code></pre></td></tr></table></figure>

<h3 id="Flume源码修改与HBase-Kafka集成"><a href="#Flume源码修改与HBase-Kafka集成" class="headerlink" title="Flume源码修改与HBase+Kafka集成"></a>Flume源码修改与HBase+Kafka集成</h3><p><strong>如何修改flume源码？</strong></p>
<p>因为我们需要在节点1上将flume同时发送至Hbase以及kafka，但是hbase结构需要自定义，所以由flume发送至hbase代码需要进行修改。<br>步骤：<br>1.下载Flume源码并导入Idea开发工具<br>1）将apache-flume-1.7.0-src.tar.gz源码下载到本地解压<br>2）通过idea导入flume源码<br>打开idea开发工具，选择File——》Open，找到源码包，选中flume-ng-hbase-sink，点击ok加载相应模块的源码。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzd1atesvcj30dw0fmaae.jpg"><br>2、自己写个类完成类的修改。KfkAsyncHbaseEventSerializer这个是我自定义的。修改其中的下面这个方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> List&lt;PutRequest&gt; <span class="hljs-title function_">getActions</span><span class="hljs-params">()</span> &#123;<br>        List&lt;PutRequest&gt; actions = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br>        <span class="hljs-keyword">if</span> (payloadColumn != <span class="hljs-literal">null</span>) &#123;<br>            <span class="hljs-type">byte</span>[] rowKey;<br>            <span class="hljs-keyword">try</span> &#123;<br>                <span class="hljs-comment">/*---------------------------代码修改开始---------------------------------*/</span><br>                <span class="hljs-comment">//解析列字段</span><br>                String[] columns = <span class="hljs-keyword">new</span> <span class="hljs-title class_">String</span>(<span class="hljs-built_in">this</span>.payloadColumn).split(<span class="hljs-string">&quot;,&quot;</span>);<br>                <span class="hljs-comment">//解析flume采集过来的每行的值</span><br>                String[] values = <span class="hljs-keyword">new</span> <span class="hljs-title class_">String</span>(<span class="hljs-built_in">this</span>.payload).split(<span class="hljs-string">&quot;,&quot;</span>);<br>                <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i &lt; columns.length;i++) &#123;<br>                    <span class="hljs-type">byte</span>[] colColumn = columns[i].getBytes();<br>                    <span class="hljs-type">byte</span>[] colValue = values[i].getBytes(Charsets.UTF_8);<br><br>                    <span class="hljs-comment">//数据校验：字段和值是否对应</span><br>                    <span class="hljs-keyword">if</span> (colColumn.length != colValue.length) <span class="hljs-keyword">break</span>;<br><br>                    <span class="hljs-comment">//时间</span><br>                    <span class="hljs-type">String</span> <span class="hljs-variable">datetime</span> <span class="hljs-operator">=</span> values[<span class="hljs-number">0</span>].toString();<br>                    <span class="hljs-comment">//用户id</span><br>                    <span class="hljs-type">String</span> <span class="hljs-variable">userid</span> <span class="hljs-operator">=</span> values[<span class="hljs-number">1</span>].toString();<br>                    <span class="hljs-comment">//根据业务自定义Rowkey</span><br>                    rowKey = SimpleRowKeyGenerator.getKfkRowKey(userid, datetime);<br>                    <span class="hljs-comment">//插入数据</span><br>                    <span class="hljs-type">PutRequest</span> <span class="hljs-variable">putRequest</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">PutRequest</span>(table, rowKey, cf,<br>                            colColumn, colValue);<br>                    actions.add(putRequest);<br>                    <span class="hljs-comment">/*---------------------------代码修改结束---------------------------------*/</span><br>                &#125;<br>            &#125; <span class="hljs-keyword">catch</span> (Exception e) &#123;<br>                <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">FlumeException</span>(<span class="hljs-string">&quot;Could not get row key!&quot;</span>, e);<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> actions;<br>    &#125;<br></code></pre></td></tr></table></figure>
<p>修改这个类中自定义KEY生成方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleRowKeyGenerator</span> &#123;<br><br>  <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-type">byte</span>[] getKfkRowKey(String userid,String datetime)<span class="hljs-keyword">throws</span> UnsupportedEncodingException &#123;<br>    <span class="hljs-keyword">return</span> (userid + datetime + String.valueOf(System.currentTimeMillis())).getBytes(<span class="hljs-string">&quot;UTF8&quot;</span>);<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<p>3、应该进行测试，但是这边测试完成，目前不知如何搭建，就直接生成jar包放到虚拟机直接用了。<br>4、生成jar包，idea很好用<br>可参考：<a href="https://jingyan.baidu.com/article/c275f6ba0bbb65e33d7567cb.html">https://jingyan.baidu.com/article/c275f6ba0bbb65e33d7567cb.html</a><br>1）在idea工具中，选择File——》ProjectStructrue<br>2）左侧选中Artifacts，然后点击右侧的+号，最后选择JAR——》From modules with dependencies<br>3）一定要设置main class这一项选择自己要打包的类，然后直接点击ok<br>4）删除其他依赖包，只把flume-ng-hbase-sink打成jar包就可以了。<br>5）然后依次点击apply，ok<br>6）点击build进行编译，会自动打成jar包<br>7）到项目的apache-flume-1.7.0-src\flume-ng-sinks\flume-ng-hbase-sink\classes\artifacts\flume_ng_hbase_sink_jar目录下找到刚刚打的jar包<br>8）将打包名字替换为flume自带的包名flume-ng-hbase-sink-1.7.0.jar ，然后上传至虚拟机上flume/lib目录下，覆盖原有的jar包即可。</p>
<p><strong>修改flume配置</strong></p>
<p>这里在节点1上修改flume的配置，完成与hbase和kafka的集成。（flume自定义的jar已经上传覆盖）<br>修改flume-conf.properties</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">agent1.sources</span> = r1<br><span class="hljs-attr">agent1.channels</span> = kafkaC hbaseC <br><span class="hljs-attr">agent1.sinks</span> =  kafkaSink hbaseSink<br><br><span class="hljs-attr">agent1.sources.r1.type</span> = avro<br><span class="hljs-attr">agent1.sources.r1.channels</span> = hbaseC kafkaC<br><span class="hljs-attr">agent1.sources.r1.bind</span> = bigdata-pro01.kfk.com<br><span class="hljs-attr">agent1.sources.r1.port</span> = <span class="hljs-number">5555</span><br><span class="hljs-attr">agent1.sources.r1.threads</span> = <span class="hljs-number">5</span><br><span class="hljs-comment"># flume-hbase</span><br><span class="hljs-attr">agent1.channels.hbaseC.type</span> = memory<br><span class="hljs-attr">agent1.channels.hbaseC.capacity</span> = <span class="hljs-number">100000</span><br><span class="hljs-attr">agent1.channels.hbaseC.transactionCapacity</span> = <span class="hljs-number">100000</span><br><span class="hljs-attr">agent1.channels.hbaseC.keep-alive</span> = <span class="hljs-number">20</span><br><br><span class="hljs-attr">agent1.sinks.hbaseSink.type</span> = asynchbase<br><span class="hljs-attr">agent1.sinks.hbaseSink.table</span> = weblogs<br><span class="hljs-attr">agent1.sinks.hbaseSink.columnFamily</span> = info<br><span class="hljs-attr">agent1.sinks.hbaseSink.channel</span> = hbaseC<br><span class="hljs-attr">agent1.sinks.hbaseSink.serializer</span> = org.apache.flume.sink.hbase.KfkAsyncHbaseEventSerializer<br><span class="hljs-attr">agent1.sinks.hbaseSink.serializer.payloadColumn</span> = datatime,userid,searchname,retorder,cliorder,cliurl<br><span class="hljs-comment">#flume-kafka</span><br><span class="hljs-attr">agent1.channels.kafkaC.type</span> = memory<br><span class="hljs-attr">agent1.channels.kafkaC.capacity</span> = <span class="hljs-number">100000</span><br><span class="hljs-attr">agent1.channels.kafkaC.transactionCapacity</span> = <span class="hljs-number">100000</span><br><span class="hljs-attr">agent1.channels.kafkaC.keep-alive</span> = <span class="hljs-number">20</span><br><br><span class="hljs-attr">agent1.sinks.kafkaSink.channel</span> = kafkaC<br><span class="hljs-attr">agent1.sinks.kafkaSink.type</span> = org.apache.flume.sink.kafka.KafkaSink<br><span class="hljs-attr">agent1.sinks.kafkaSink.brokerList</span> = bigdata-pro01.kfk.com:<span class="hljs-number">9092</span>,bigdata-pro02.kfk.com:<span class="hljs-number">9092</span>,bigdata-pro03.kfk.com:<span class="hljs-number">9092</span><br><span class="hljs-attr">agent1.sinks.kafkaSink.topic</span> = weblogs<br><span class="hljs-attr">agent1.sinks.kafkaSink.zookeeperConnect</span> = bigdata-pro01.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro02.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro03.kfk.com:<span class="hljs-number">2181</span><br><span class="hljs-attr">agent1.sinks.kafkaSink.requiredAcks</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">agent1.sinks.kafkaSink.batchSize</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">agent1.sinks.kafkaSink.serializer.class</span> = kafka.serializer.StringEncoder<br></code></pre></td></tr></table></figure>

<p><strong>小结</strong></p>
<p>项目进行到这里，已经完成了节点2和节点3上flume采集配置、节点1上flume采集并发送至kafka和hbase配置。<br>如下图，这部分都已经完成，下一章进行联调。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzd2e99ywhj30go0gp43u.jpg"></p>
<h3 id="Flume-HBase-Kafka集成全流程测试"><a href="#Flume-HBase-Kafka集成全流程测试" class="headerlink" title="Flume+HBase+Kafka集成全流程测试"></a>Flume+HBase+Kafka集成全流程测试</h3><p><strong>全流程测试简介</strong></p>
<p>将完成对前面所有的设计进行测试，核心是进行flume日志的采集、汇总以及发送至kafka消费、hbase保存。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzd3g6rboxj30go0gp43u.jpg"></p>
<p><strong>原始日志数据简单处理</strong></p>
<p>1、下载搜狗实验室数据<br><a href="http://www.sogou.com/labs/resource/q.php">http://www.sogou.com/labs/resource/q.php</a><br>2、格式说明<br>数据格式为:访问时间\t用户ID\t[查询词]\t该URL在返回结果中的排名\t用户点击的顺序号\t用户点击的URL<br>其中，用户ID是根据用户使用浏览器访问搜索引擎时的Cookie信息自动赋值，即同一次使用浏览器输入的不同查询对应同一个用户ID<br>3、日志简单处理<br>1）将文件中的tab更换成逗号<br>cat weblog.log|tr “\t” “,” &gt; weblog2.log<br>2）将文件中的空格更换成逗号<br>cat weblog2.log|tr “ “ “,” &gt; weblog3.log<br>处理完：<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzd3eylp5zj30l008fab1.jpg"></p>
<p><strong>编写模拟日志生成过程</strong></p>
<p>1、代码实现<br>    实现功能是将原始日志，每次读取一行不断写入到另一个文件中（weblog-flume.log），所以这个文件就相等于服务器中日志不断增加的过程。编写完程序，将该项目打成weblogs.jar包，然后上传至bigdata-pro02.kfk.com节点和bigdata-pro03.kfk.com节点的/opt/jars目录下（目录需要提前创建）<br>2、编写运行模拟日志程序的shell脚本</p>
<figure class="highlight vim"><table><tr><td class="code"><pre><code class="hljs vim"><span class="hljs-number">1</span>）<br>在bigdata-pro02.kfk.<span class="hljs-keyword">com</span>节点的/<span class="hljs-keyword">opt</span>/datas目录下，创建weblog-<span class="hljs-keyword">shell</span>.<span class="hljs-keyword">sh</span>脚本。<br><span class="hljs-keyword">vi</span> weblog-<span class="hljs-keyword">shell</span>.<span class="hljs-keyword">sh</span><br>#/bin/bash<br><span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;start log......&quot;</span><br>#第一个参数是原日志文件，第二个参数是日志生成输出文件<br>java -jar /<span class="hljs-keyword">opt</span>/jars/weblogs.jar /<span class="hljs-keyword">opt</span>/datas/weblog.<span class="hljs-built_in">log</span> /<span class="hljs-keyword">opt</span>/datas/weblog-flume.<span class="hljs-built_in">log</span><br><br>修改weblog-<span class="hljs-keyword">shell</span>.<span class="hljs-keyword">sh</span>可执行权限<br>chmod <span class="hljs-number">777</span> weblog-<span class="hljs-keyword">shell</span>.<span class="hljs-keyword">sh</span><br><span class="hljs-number">2</span>）<br>将bigdata-pro02.kfk.<span class="hljs-keyword">com</span>节点上的/<span class="hljs-keyword">opt</span>/datas/目录拷贝到bigdata-pro03节点.kfk.<span class="hljs-keyword">com</span><br>scp -r /<span class="hljs-keyword">opt</span>/datas/ bigdata-pro03.kfk.<span class="hljs-keyword">com</span>:/<span class="hljs-keyword">opt</span>/datas/<br></code></pre></td></tr></table></figure>
<p>3、运行测试<br>/opt/datas/weblog-shell.sh<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdb284hefj30he0chn95.jpg"></p>
<p><strong>编写一些shell脚本便于执行</strong></p>
<p>1、编写启动flume服务程序的shell脚本</p>
<figure class="highlight vim"><table><tr><td class="code"><pre><code class="hljs vim"><span class="hljs-number">1</span>.在bigdata-pro02.kfk.<span class="hljs-keyword">com</span>节点的flume安装目录下编写flume启动脚本。<br><span class="hljs-keyword">vi</span> flume-kfk-start.<span class="hljs-keyword">sh</span><br>#/bin/bash<br><span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;flume-2 start ......&quot;</span><br>bin/flume-ng agent --<span class="hljs-keyword">conf</span> <span class="hljs-keyword">conf</span> -<span class="hljs-keyword">f</span> <span class="hljs-keyword">conf</span>/flume-<span class="hljs-keyword">conf</span>.properties -n agent2 -Dflume.root.logger=INFO,console<br><span class="hljs-number">2</span>.在bigdata-pro03.kfk.<span class="hljs-keyword">com</span>节点的flume安装目录下编写flume启动脚本。<br><span class="hljs-keyword">vi</span> flume-kfk-start.<span class="hljs-keyword">sh</span><br>#/bin/bash<br><span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;flume-3 start ......&quot;</span><br>bin/flume-ng agent --<span class="hljs-keyword">conf</span> <span class="hljs-keyword">conf</span> -<span class="hljs-keyword">f</span> <span class="hljs-keyword">conf</span>/flume-<span class="hljs-keyword">conf</span>.properties -n agent3 -Dflume.root.logger=INFO,console<br><span class="hljs-number">3</span>.在bigdata-pro01.kfk.<span class="hljs-keyword">com</span>节点的flume安装目录下编写flume启动脚本。<br><span class="hljs-keyword">vi</span> flume-kfk-start.<span class="hljs-keyword">sh</span><br>#/bin/bash<br><span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;flume-1 start ......&quot;</span><br>bin/flume-ng agent --<span class="hljs-keyword">conf</span> <span class="hljs-keyword">conf</span> -<span class="hljs-keyword">f</span> <span class="hljs-keyword">conf</span>/flume-<span class="hljs-keyword">conf</span>.properties -n agent1 -Dflume.root.logger=INFO,console<br><br></code></pre></td></tr></table></figure>
<p>2、编写Kafka Consumer执行脚本</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus"><span class="hljs-number">1</span>.在bigdata-pro01<span class="hljs-selector-class">.kfk</span>.com节点的Kafka安装目录下编写Kafka Consumer执行脚本<br>vi kfk-test-consumer<span class="hljs-selector-class">.sh</span><br>#/bin/bash<br>echo <span class="hljs-string">&quot;kfk-kafka-consumer.sh start ......&quot;</span><br>bin/kafka-console-consumer<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--zookeeper</span> bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro03<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span> <span class="hljs-attr">--from-beginning</span> <span class="hljs-attr">--topic</span> weblogs<br><span class="hljs-number">2</span>.将kfk-test-consumer.sh脚本分发另外两个节点<br>scp kfk-test-consumer<span class="hljs-selector-class">.sh</span> bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:/opt/modules/kakfa_2.<span class="hljs-number">11</span>-<span class="hljs-number">0.8</span>.<span class="hljs-number">2.1</span>/<br>scp kfk-test-consumer<span class="hljs-selector-class">.sh</span> bigdata-pro03<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:/opt/modules/kakfa_2.<span class="hljs-number">11</span>-<span class="hljs-number">0.8</span>.<span class="hljs-number">2.1</span>/<br><br></code></pre></td></tr></table></figure>
<p><strong>联调测试-数据采集分发</strong></p>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk"><span class="hljs-number">1</span>、在各个节点上启动zk<br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/zookeeper-3.4.5-cdh5.10.0/</span>sbin/zkServer.sh start  <br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/zookeeper-3.4.5-cdh5.10.0/</span>bin/zkCli.sh  登陆客户端进行测试是否启动成功<br><br><span class="hljs-number">2</span>、启动hdfs  --- http:<span class="hljs-regexp">//</span>bigdata-pro01.kfk.com:<span class="hljs-number">50070</span>/<br>在节点<span class="hljs-number">1</span>：<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>sbin/start-dfs.sh <br><span class="hljs-comment">#节点1 和 节点2  启动namenode高可用</span><br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>sbin/hadoop-daemon.sh start zkfc<br><br><span class="hljs-number">3</span>、启动hbase  ----http:<span class="hljs-regexp">//</span>bigdata-pro01.kfk.com:<span class="hljs-number">60010</span>/<br><span class="hljs-comment">#节点 1  启动hbase</span><br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hbase-1.0.0-cdh5.4.0/</span>bin/start-hbase.sh<br><span class="hljs-comment">#在节点2 启动备用master</span><br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hbase-1.0.0-cdh5.4.0/</span>bin/hbase-daemon.sh start  master<br><span class="hljs-comment">#启动hbase的shell用于操作</span><br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hbase-1.0.0-cdh5.4.0/</span>bin/hbase shell<br><span class="hljs-comment">#创建hbase业务表</span><br>bin/hbase shell<br>create <span class="hljs-string">&#x27;weblogs&#x27;</span>,<span class="hljs-string">&#x27;info&#x27;</span><br><br><span class="hljs-number">4</span>、启动kafka<br><span class="hljs-comment">#在各个个节点启动kafka</span><br>cd <span class="hljs-regexp">/opt/m</span>odules/kafka_2.<span class="hljs-number">10</span>-<span class="hljs-number">0.9</span>.<span class="hljs-number">0.0</span><br>bin<span class="hljs-regexp">/kafka-server-start.sh config/</span>server.properties &amp;<br><span class="hljs-comment">#创建业务</span><br>bin/kafka-topics.sh --zookeeper bigdata-pro01.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro02.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro03.kfk.com:<span class="hljs-number">2181</span> --create --topic weblogs --replication-factor <span class="hljs-number">2</span> --partitions <span class="hljs-number">1</span><br><span class="hljs-comment">#消费(之前编写的脚本可以用)</span><br>bin/kafka-console-consumer.sh --zookeeper bigdata-pro01.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro02.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro03.kfk.com:<span class="hljs-number">2181</span> --from-beginning --topic weblogs<br></code></pre></td></tr></table></figure>
<p>一定确保上述都启动成功能，利用jps查看各个节点进程情况。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbmh1n31j309v042glj.jpg"><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbmovok3j309n03sa9y.jpg"><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbmw14tjj309o02cweb.jpg"></p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus"><span class="hljs-number">5</span>、各个节点启动flume<br>#三节点启动flume<br>/opt/modules/flume-<span class="hljs-number">1.7</span>.<span class="hljs-number">0</span>-bin/flume-kfk-start<span class="hljs-selector-class">.sh</span><br><br><span class="hljs-number">6</span>、在节点<span class="hljs-number">2</span>和<span class="hljs-number">3</span>启动日志模拟生产<br>/opt/datas/weblog-shell<span class="hljs-selector-class">.sh</span><br><br><span class="hljs-number">7</span>、启动kafka消费程序<br>#消费（或者使用写好的脚本kfk-test-consumer.sh）<br>bin/kafka-console-consumer<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--zookeeper</span> bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro03<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span> <span class="hljs-attr">--from-beginning</span> <span class="hljs-attr">--topic</span> weblogs<br><br><span class="hljs-number">8</span>、查看hbase数据写入情况<br>./hbase-shell<br>count <span class="hljs-string">&#x27;weblogs&#x27;</span><br></code></pre></td></tr></table></figure>
<p>结果：<br>kafka不断消费<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbszmkybj30rh0940ue.jpg"><br>hbase数据不断增加<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbtek6eqj30rv0ar0ud.jpg"></p>
<h3 id="mysql、Hive安装与集成"><a href="#mysql、Hive安装与集成" class="headerlink" title="mysql、Hive安装与集成"></a>mysql、Hive安装与集成</h3><p><strong>为什么要用mysql?</strong></p>
<p>一方面，本项目用来存储Hive的元数据；另一方面，可以把离线分析结果放入mysql中；</p>
<p><strong>安装mysql</strong></p>
<p>通过yum在线mysql，具体操作命令如下所示(关于yum源可以修改为阿里的，比较快和稳定)</p>
<figure class="highlight subunit"><table><tr><td class="code"><pre><code class="hljs subunit">1、在线安装mysql<br>通过yum在线mysql，具体操作命令如下所示。<br>yum clean all<br>yum install mysql-server<br>2、mysql 服务启动并测试<br>sudo chown -R kfk:kfk /usr/bin/mysql    修改权限给kfk<br>1）查看mysql服务状态<br>sudo service mysqld status  <br>2）启动mysql服务<br>sudo service mysqld start<br>3）设置mysql密码<br>/usr/bin/mysqladmin -u root password &#x27;123456&#x27;<br>4）连接mysql<br>mysql –uroot -p123456<br>a）查看数据库<br>show databases;<br>mysql<br><span class="hljs-keyword">test</span><br><span class="hljs-keyword"></span>b）查看数据库<br>use test;<br>c）查看表列表<br>show tables;<br></code></pre></td></tr></table></figure>
<p>出现问题，大多数是权限问题，利用sudo执行或者重启mysql.</p>
<p><strong>安装Hive</strong></p>
<p>Hive在本项目中功能是，将hbase中的数据进行离线分析，输出处理结果，可以到mysql或者hbase，然后进行可视化。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzfpw9k0v7j30kv09rtfp.jpg"><br>这里版本采用的是：apache-hive-2.1.0-bin.tar.gz<br>（之前用apache-hive-0.13.1-bin.tar.gz出现和hbase集成失败，原因很奇怪，下一章详细讲）。<br>1、解压</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk">步骤都老生常谈了。。。<br>tar -zxf apache-hive-<span class="hljs-number">2.1</span>.<span class="hljs-number">0</span>-bin.tar.gz -C <span class="hljs-regexp">/opt/m</span>odules/<br>mv  apache-hive-<span class="hljs-number">2.1</span>.<span class="hljs-number">0</span>-bin hive-<span class="hljs-number">2.1</span>.<span class="hljs-number">0</span>     <span class="hljs-regexp">//</span>重命名<br></code></pre></td></tr></table></figure>
<p>2、修改配置文件</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">1</span>）hive-log4j.properties<br><span class="hljs-comment">#日志目录需要提前创建</span><br><span class="hljs-attribute">hive</span>.log.dir=/opt/modules/hive-<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">0</span>/logs<br><span class="hljs-attribute">2</span>）修改hive-env.sh配置文件<br><span class="hljs-attribute">HADOOP_HOME</span>=/opt/modules/hadoop-<span class="hljs-number">2</span>.<span class="hljs-number">6</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">HBASE_HOME</span>=/opt/modules/hbase-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span><br><span class="hljs-comment"># Hive Configuration Directory can be controlled by:</span><br><span class="hljs-attribute">export</span> HIVE_CONF_DIR=/opt/modules/hive-<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">0</span>/conf<br></code></pre></td></tr></table></figure>
<p>3、启动进行测试<br>首先启动HDFS，然后创建Hive的目录<br>bin/hdfs dfs -mkdir -p /tmp<br>bin/hdfs dfs -chmod g+w /tmp<br>bin/hdfs dfs -mkdir -p /user/hive/warehouse<br>bin/hdfs dfs -chmod g+w /user/hive/warehouse<br>4、测试</p>
<figure class="highlight gauss"><table><tr><td class="code"><pre><code class="hljs gauss">./hive<br><span class="hljs-meta">#查看数据库</span><br><span class="hljs-keyword">show</span> databases;<br><span class="hljs-meta">#使用默认数据库</span><br><span class="hljs-keyword">use</span> default;<br><span class="hljs-meta">#查看表</span><br><span class="hljs-keyword">show</span> tables;<br><br></code></pre></td></tr></table></figure>
<p><strong>Hive与mysql集成</strong></p>
<p>利用mysql放Hive的元数据。<br>1、在/opt/modules/hive-2.1.0/conf目录下创建hive-site.xml文件，配置mysql元数据库。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="hljs-meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><br><br><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>jdbc:mysql://bigdata-pro01.kfk.com/metastore?createDatabaseIfNotExist=true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br> <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>root<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>123456<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>   <br>	<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com,bigdata-pro02.kfk.com,bigdata-pro03.kfk.com<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure>
<p>2、设置用户连接信息</p>
<p>1）查看用户信息<br>mysql -uroot -p123456<br>show databases;<br>use mysql;<br>show tables;<br>select User,Host,Password from user;<br>2）更新用户信息<br>update user set Host=’%’ where User = ‘root’ and Host=’localhost’<br>3）删除用户信息<br>delete from user where user=’root’ and host=’127.0.0.1’<br>select User,Host,Password from user;<br>delete from user where host=’localhost’;<br>删除到只剩图中这一行数据<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzfqckmjxej30ej031q2s.jpg"><br>4）刷新信息<br>flush privileges;<br>3.拷贝mysql驱动包到hive的lib目录下<br>cp  mysql-connector-java-5.1.35.jar /opt/modules/hive-2.1.0/lib/<br>4.保证第三台集群到其他节点无秘钥登录</p>
<p><strong>Hive与mysql测试</strong></p>
<p>1.启动HDFS和YARN服务<br>2.启动hive<br>./hive<br>3.通过hive服务创建表<br>CREATE TABLE stu(id INT,name STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’ ;<br>4.创建数据文件<br>vi /opt/datas/stu.txt<br>00001    zhangsan<br>00002    lisi<br>00003    wangwu<br>00004    zhaoliu<br>5.加载数据到hive表中<br>load data local inpath ‘/opt/datas/stu.txt’ into table stu;<br>直接在hive查看表中内容就ok。<br>在mysql数据库中hive的metastore元数据。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzfqeibkrtj306103ta9v.jpg"></p>
<h3 id="Hive与Hbase集成"><a href="#Hive与Hbase集成" class="headerlink" title="Hive与Hbase集成"></a>Hive与Hbase集成</h3><p><strong>Hive与HBase集成配置</strong></p>
<p>1）在hive-site.xml文件中配置Zookeeper，hive通过这个参数去连接HBase集群。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>   <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com,bigdata-pro02.kfk.com,bigdata-pro03.kfk.com<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br></code></pre></td></tr></table></figure>
<p>2）需要把hbase中的部分jar包拷贝到hive中<br>这里采用软连接的方式：<br>执行如下命令：</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">export</span> HBASE_HOME=/opt/modules/hbase-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">export</span> HIVE_HOME=/opt/modules/hive-<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-server-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-server-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar<br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-client-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-client-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar<br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-protocol-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-protocol-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar <br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-it-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-it-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar <br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/htrace-core-<span class="hljs-number">3</span>.<span class="hljs-number">0</span>.<span class="hljs-number">4</span>.jar $HIVE_HOME/lib/htrace-core-<span class="hljs-number">3</span>.<span class="hljs-number">0</span>.<span class="hljs-number">4</span>.jar<br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-hadoop2-compat-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-hadoop2-compat-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar <br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-hadoop-compat-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-hadoop-compat-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar<br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/high-scale-lib-<span class="hljs-number">1</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span>.jar $HIVE_HOME/lib/high-scale-lib-<span class="hljs-number">1</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span>.jar <br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-common-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-common-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar <br></code></pre></td></tr></table></figure>
<p>3）测试<br>在hbase中建立一个表，里面存有数据（实际底层就是在hdfs上），然后Hive创建一个表与HBase中的表建立联系。</p>
<ol>
<li>先在hbase建立一个表<br>（不熟悉的，看指令<a href="https://www.cnblogs.com/cxzdy/p/5583239.html%EF%BC%89">https://www.cnblogs.com/cxzdy/p/5583239.html）</a><br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzgupdmei1j30h5037mx4.jpg"></li>
<li>启动hive,建立联系（之前要先启动mysql，因为元数据在里面)</li>
</ol>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">external</span> <span class="hljs-keyword">table</span> t1(<br>key <span class="hljs-type">int</span>,<br><span class="hljs-type">name</span> string,<br>age string<br>)  <br>STORED <span class="hljs-keyword">BY</span>  <span class="hljs-string">&#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span> <br><span class="hljs-keyword">WITH</span> SERDEPROPERTIES(&quot;hbase.columns.mapping&quot; = &quot;:key,info:name,info:age&quot;) <br>TBLPROPERTIES(&quot;hbase.table.name&quot; = &quot;t1&quot;);<br></code></pre></td></tr></table></figure>
<ol start="3">
<li>hive结果<br>执行 select * from t1;<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzgutrr5x7j30b0035glg.jpg"></li>
<li>为项目中的weblogs建立联系<br>之前我们把数据通过flume导入到hbase中了，所以同样我们在hive中建立联系，可以用hive对hbase中的数据进行简单的sql分析，离线分析。</li>
</ol>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><code class="hljs n1ql"><span class="hljs-keyword">create</span> external table weblogs(<br>id <span class="hljs-keyword">string</span>,<br>datatime <span class="hljs-keyword">string</span>,<br>userid <span class="hljs-keyword">string</span>,<br>searchname <span class="hljs-keyword">string</span>,<br>retorder <span class="hljs-keyword">string</span>,<br>cliorder <span class="hljs-keyword">string</span>,<br>cliurl <span class="hljs-keyword">string</span><br>)  <br>STORED <span class="hljs-keyword">BY</span>  <span class="hljs-string">&#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span> <br><span class="hljs-keyword">WITH</span> SERDEPROPERTIES(<span class="hljs-string">&quot;hbase.columns.mapping&quot;</span> = <span class="hljs-string">&quot;:key,info:datatime,info:userid,info:searchname,info:retorder,info:cliorder,info:cliurl&quot;</span>) <br>TBLPROPERTIES(<span class="hljs-string">&quot;hbase.table.name&quot;</span> = <span class="hljs-string">&quot;weblogs&quot;</span>);<br></code></pre></td></tr></table></figure>

<p><strong>Hive与HBase集成中的致命bug</strong></p>
<p>问题如图：<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzguxe0p4ej30nu0hl0ua.jpg"></p>
<p>hbase 1.x之后的版本，需要更高版本的hive匹配，最好是hive 2.x,上述的错误是因为hive-0.13.1-bin和hbase-1.0.0-cdh5.4.0，应该是不兼容导致的，莫名bug。于是采用了 hive-2.1.0，查了下这个版本与hadoop其他组件也是兼容的，所以，采用这个。配置仍然采用刚才的方法（上一章和这一章），主要有mysql元数据配置（驱动包别忘了），各种xml配置，测试下。最后，在重启hive之前，<strong>先把hbase重启了</strong>，很重要。</p>
<h3 id="HUE大数据可视化分析"><a href="#HUE大数据可视化分析" class="headerlink" title="HUE大数据可视化分析"></a>HUE大数据可视化分析</h3><p><strong>下载和安装Hue</strong></p>
<p>版本选择： hue-3.9.0-cdh5.15.0<br>1、首先需要利用yum安装依赖包，虚拟机需要联网，这里安装在节点3上。</p>
<figure class="highlight brainfuck"><table><tr><td class="code"><pre><code class="hljs brainfuck"><span class="hljs-comment">yum</span> <span class="hljs-literal">-</span><span class="hljs-comment">y install ant asciidoc cyrus</span><span class="hljs-literal">-</span><span class="hljs-comment">sasl</span><span class="hljs-literal">-</span><span class="hljs-comment">devel</span> <span class="hljs-comment">cyrus</span><span class="hljs-literal">-</span><span class="hljs-comment">sasl</span><span class="hljs-literal">-</span><span class="hljs-comment">gssapi</span> <span class="hljs-comment">gcc</span> <span class="hljs-comment">gcc</span><span class="hljs-literal">-</span><span class="hljs-comment">c</span>++ <span class="hljs-comment">krb5</span><span class="hljs-literal">-</span><span class="hljs-comment">devel</span> <span class="hljs-comment">libtidy</span> <span class="hljs-comment">libxml2</span><span class="hljs-literal">-</span><span class="hljs-comment">devel libxslt-devel openldap-devel python-devel sqlite</span><span class="hljs-literal">-</span><span class="hljs-comment">devel openssl-devel mysql-devel gmp-devel </span> <br></code></pre></td></tr></table></figure>
<p>2、解压<br>tar -zxf hue-3.9.0-cdh5.15.0.tar.gz -C /opt/modules/<br>3、编译<br>cd  hue-3.9.0-cdh5.15.0<br>make apps<br>4、基本配置与测试</p>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-number">1</span>）修改配置文件<br>cd desktop<br>cd conf<br>vi hue.ini<br>#秘钥<br>secret_key=jFE93j;<span class="hljs-number">2</span>[<span class="hljs-number">290</span>-eiw.KEiwN2s3[<span class="hljs-string">&#x27;d;/.q[eIW^y#e=+Iei*@Mn &lt; qW5o</span><br><span class="hljs-string">#host port</span><br><span class="hljs-string">http_host=bigdata-pro03.kfk.com</span><br><span class="hljs-string">http_port=8888</span><br><span class="hljs-string">#时区</span><br><span class="hljs-string">time_zone=Asia/Shanghai</span><br><span class="hljs-string">2）修改desktop.db 文件权限</span><br><span class="hljs-string">chmod o+w desktop/desktop.db</span><br><span class="hljs-string">3）启动Hue服务</span><br><span class="hljs-string">/opt/modules/hue-3.9.0-cdh5.15.0/build/env/bin/supervisor</span><br><span class="hljs-string">4）查看Hue web界面</span><br><span class="hljs-string">bigdata-pro03.kfk.com:8888</span><br></code></pre></td></tr></table></figure>
<p><strong>Hue与HDFS集成</strong></p>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk"><span class="hljs-number">1</span>）修改hadoop中core-site.xml配置文件，添加如下内容<br>&lt;property&gt;<br>    &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt;<br>    &lt;value&gt;*&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;property&gt;<br>    &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt;<br>    &lt;value&gt;*&lt;/value&gt;<br>&lt;/property&gt;<br><br><span class="hljs-number">2</span>）修改hue.ini配置文件<br>fs_defaultfs=hdfs:<span class="hljs-regexp">//</span>ns<br>webhdfs_url=http:<span class="hljs-regexp">//</span>bigdata-pro01.kfk.com:<span class="hljs-number">50070</span><span class="hljs-regexp">/webhdfs/</span>v1<br>hadoop_hdfs_home=<span class="hljs-regexp">/opt/m</span>odules/hadoop-<span class="hljs-number">2.6</span>.<span class="hljs-number">0</span><br>hadoop_bin=<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>bin<br>hadoop_conf_dir=<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc/hadoop<br><span class="hljs-number">3</span>）将core-site.xml配置文件分发到其他节点<br>scp core-site.xml bigdata-pro02.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc/hadoop<br>scp core-site.xml bigdata-pro01.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc/hadoop<br><span class="hljs-number">4</span>）重新启动hue<br>先启动zk,hdfs，再启动hue<br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hue-3.9.0-cdh5.15.0/</span>build<span class="hljs-regexp">/env/</span>bin/supervisor<br></code></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzk8cc0l9rj30ev0ebmx9.jpg"></p>
<p><strong>Hue与YARN集成</strong></p>
<p>1、修改hue.ini配置文件,参考<a href="https://www.cnblogs.com/zlslch/p/6817226.html">https://www.cnblogs.com/zlslch/p/6817226.html</a><br>区分yarn是不是HA</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[[yarn_clusters]]</span><br><br>   <span class="hljs-section">[[[default]]]</span><br>     <span class="hljs-attr">resourcemanager_host</span>=rs<br>     <span class="hljs-attr">resourcemanager_port</span>=<span class="hljs-number">8032</span><br>     <span class="hljs-attr">submit_to</span>=<span class="hljs-literal">True</span><br>     <span class="hljs-attr">logical_name</span>=rm1<br>     <span class="hljs-attr">resourcemanager_api_url</span>=http://bigdata-pro01.kfk.com:<span class="hljs-number">8088</span><br>     <span class="hljs-attr">proxy_api_url</span>=http://bigdata-pro01.kfk.com:<span class="hljs-number">8088</span><br>     <span class="hljs-attr">history_server_api_url</span>=http://bigdata-pro01.kfk.com:<span class="hljs-number">19888</span><br><br>    <span class="hljs-section">[[[ha]]]</span><br>     <span class="hljs-attr">logical_name</span>=rm2<br>     <span class="hljs-attr">submit_to</span>=<span class="hljs-literal">True</span><br>     <span class="hljs-attr">resourcemanager_api_url</span>=http://bigdata-pro02.kfk.com:<span class="hljs-number">8088</span><br>  <span class="hljs-attr">history_server_api_url</span>=http://bigdata-pro01.kfk.com:<span class="hljs-number">19888</span><br></code></pre></td></tr></table></figure>
<p>2、测试<br>启动yarn，再重启hue。<br>图中的任务是我之前进行的任务<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzk8i4ao6kj314r07j74m.jpg"></p>
<p><strong>Hue与mysql、hive集成</strong></p>
<p>1、修改hue.ini配置</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><code class="hljs routeros">  [beeswax]<br><br><br>    <span class="hljs-attribute">hive_server_host</span>=bigdata-pro03.kfk.com<br>    <span class="hljs-attribute">hive_server_port</span>=10000<br>    <span class="hljs-attribute">hive_conf_dir</span>=/opt/modules/hive-2.1.0/conf<br><br><br><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span>.中间其他<span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><br>   [[[mysql]]]<br>    <span class="hljs-attribute">nice_name</span>=<span class="hljs-string">&quot;My SQL DB&quot;</span><br><br>    <span class="hljs-attribute">name</span>=metastore<br>    <span class="hljs-attribute">engine</span>=mysql<br><br>    <span class="hljs-attribute">host</span>=bigdata-pro01.kfk.com<br>    <span class="hljs-attribute">port</span>=3306<br>    <span class="hljs-attribute">user</span>=root<br>    <span class="hljs-attribute">password</span>=123456<br></code></pre></td></tr></table></figure>
<p>2、测试<br>启动节点1的mysql（这是元数据），再启动节点3的hive服<br>/opt/modules/hive-2.1.0/bin/hive –service hiveserver2 &amp;    ##配合hue服务<br>再重启hue。<br>图中是利用hive中的sql查询，hive中的表。但是有一个问题是：我用hive查询hbase中的表，无法查询，出现超时情况，目前还没解决，搞了2天难受，（本来想直接在hue中用hive来处理hbase中的表进行离线计算，但是没法查询，只能查询hive本身自己的表，另外hive的beeline模式也无法查询hbase表，但是hive cli模式可以的查询）<br>问题日志：:java.io.IOException: org.apache.hadoop.hbase.client.RetriesExhaustedException<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzk8rvrxu6j30n20e3q35.jpg"></p>
<p><strong>Hue与hbase集成</strong></p>
<p>1、修改hue.ini配置</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[hbase]</span><br>   <span class="hljs-attr">hbase_clusters</span>=(Cluster|bigdata-pro01.kfk.com:<span class="hljs-number">9090</span>)<br>   <span class="hljs-attr">hbase_conf_dir</span>=/opt/modules/hbase-<span class="hljs-number">1.0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4.0</span>/conf<br>  <span class="hljs-attr">thrift_transport</span>=buffered<br></code></pre></td></tr></table></figure>
<p>2、启动测试<br>先启动hbase,再启动HBase中启动thrift服务<br>/opt/modules/hbase-1.0.0-cdh5.4.0/bin/hbase-daemon.sh start thrift<br>然后重启hue<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzk9mr2i83j30kp0ddaa9.jpg"></p>
<h3 id="Spark2-X集群安装与spark-on-yarn部署"><a href="#Spark2-X集群安装与spark-on-yarn部署" class="headerlink" title="Spark2.X集群安装与spark on yarn部署"></a>Spark2.X集群安装与spark on yarn部署</h3><p><strong>spark集群安装</strong></p>
<p>版本是spark-2.2.0-bin-hadoop2.6.tgz，之前用的是hadoop2.6.0.<br>环境要求：scala-2.11.12.tgz/java8/hadoop2.6.0.<br>1、官网下载<br><a href="https://spark.apache.org/downloads.html">https://spark.apache.org/downloads.html</a><br>2、spark配置<br>配置spark-env.sh</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/opt/modules/jdk1.8.0_191<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SCALA_HOME</span>=/opt/modules/scala-2.11.12<br><br><span class="hljs-built_in">export</span> <span class="hljs-attribute">HADOOP_CONF_DIR</span>=/opt/modules/hadoop-2.6.0/etc/hadoop<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_CONF_DIR</span>=/opt/modules/spark-2.2.0/conf<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_MASTER_HOST</span>=bigdata-pro02.kfk.com<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_MASTER_PORT</span>=7077<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_MASTER_WEBUI_PORT</span>=8080<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_WORKER_CORES</span>=1<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_WORKER_MEMORY</span>=1g<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_WORKER_PORT</span>=7078<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_WORKER_WEBUI_PORT</span>=8081<br></code></pre></td></tr></table></figure>
<p>配置slaves</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span><br>bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span><br>bigdata-pro03<span class="hljs-selector-class">.kfk</span>.com<br></code></pre></td></tr></table></figure>
<p>如果整合hive,hive用到mysql数据库的话，需要将mysql数据库连接驱动jmysql-connector-java-5.1.7-bin.jar放到$SPARK_HOME/jars目录下<br>3、分发至各个节点<br>4、设定的主节点上启动测试(这是standalone模式)<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzogese9lvj30on05o74m.jpg"><br>打开spark服务网址：<a href="http://bigdata-pro02.kfk.com:8080/">http://bigdata-pro02.kfk.com:8080/</a><br>可以查看到各个节点的情况。<br>5、可以stop-all，因为yarn模式下根本不需要。</p>
<p><strong>spark on Yarn</strong></p>
<p>standalonen模式和spark on Yarn模式比较： <a href="https://blog.csdn.net/lxhandlbb/article/details/70214003">https://blog.csdn.net/lxhandlbb/article/details/70214003</a><br>spark on Yarn原理：<a href="https://blog.csdn.net/liuwei0376/article/details/78637732">https://blog.csdn.net/liuwei0376/article/details/78637732</a><br>1、前提条件<br>已经安装了hadoop2.6.0，并可以运行，因为spark运行需要依赖hadoop.<br>2、运行zk、hdfs和yarn<br>高可用下的zk也要运行<br>hadoop:<a href="http://bigdata-pro01.kfk.com:50070/">http://bigdata-pro01.kfk.com:50070</a><br>yarn：<a href="http://bigdata-pro01.kfk.com:8088/">http://bigdata-pro01.kfk.com:8088</a><br>3、主节点运行spark<br>./spark-shell –master yarn –deploy-mode client<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzohi45ckpj30o70auq3g.jpg"><br>在yarn的网页中也可以看到。<br>虚拟机内存小的话，会出现问题：</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">17</span>/<span class="hljs-number">09</span>/<span class="hljs-number">08</span> <span class="hljs-number">10</span>:<span class="hljs-number">36</span>:<span class="hljs-number">08</span> ERROR spark.SparkContext: Error initializing SparkContext.<br><span class="hljs-attribute">org</span>.apache.spark.SparkException: Yarn application has already ended! It might have been killed or unable to launch application master.<br></code></pre></td></tr></table></figure>
<p>解决办法：先停止YARN服务，然后修改yarn-site.xml，分发至各个节点。再重启。<br>增加如下内容</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>false<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>4<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br></code></pre></td></tr></table></figure>
<p>4、测试下程序运行</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">sc</span>.parallelize(<span class="hljs-number">1</span> to <span class="hljs-number">100</span>,<span class="hljs-number">5</span>).count<br></code></pre></td></tr></table></figure>
<p>查看程序运行情况：<br>1）入口yarn的web网页，<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzohlg58qyj31dz0b8tam.jpg"><br>2）点击applicationmaster进入<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzoi538xtkj31ck0bwjrz.jpg"><br>可能出现问题进不去网页：<br>配置显示在主节点：这里配置节点1，那么RM应该在1的时候可以显示，之前我配集群总名称rs，没法用。<br>修改yarn-site.xml，分发至各个节点，然后重启。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs <property>">	&lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;<br>	&lt;value&gt;bigdata-pro01.kfk.com:8088&lt;/value&gt;<br>&lt;/property&gt;<br></code></pre></td></tr></table></figure>

<h3 id="基于IDEA环境下的Spark2-X程序开发"><a href="#基于IDEA环境下的Spark2-X程序开发" class="headerlink" title="基于IDEA环境下的Spark2.X程序开发"></a>基于IDEA环境下的Spark2.X程序开发</h3><p><strong>开发环境配置</strong></p>
<p>1、安装idea<br>2、安装maven<br>官网下载：apache-maven-3.6.0<br>3、安装java8，并配置环境变量<br>4、安装scala，直接从idea插件下载安装<br>5、安装hadoop在Windows中的运行环境，并配置环境变量</p>
<p><strong>IDEA程序开发</strong></p>
<p>可以参考这个链接很全：<a href="https://blog.csdn.net/zkf541076398/article/details/79297820">https://blog.csdn.net/zkf541076398/article/details/79297820</a><br>1、新建maven项目<br>2、配置maven<br>3、选择配置scala和java版本<br>4、新建scala目录并设置为source(看图)<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzokar9bjxj30pj0dc13u.jpg"><br>5、编写pom.xml文件<br>这里主要你需要什么就放什么，可以github上找例子<br><a href="https://github.com/apache/spark/blob/master/examples/pom.xml">https://github.com/apache/spark/blob/master/examples/pom.xml</a><br>我的pom，我自己可以用</p>
<figure class="highlight dust"><table><tr><td class="code"><pre><code class="hljs dust"><span class="language-xml"><span class="hljs-meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">project</span> <span class="hljs-attr">xmlns</span>=<span class="hljs-string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span> <span class="hljs-attr">xmlns:xsi</span>=<span class="hljs-string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span> <span class="hljs-attr">xsi:schemaLocation</span>=<span class="hljs-string">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">modelVersion</span>&gt;</span>4.0.0<span class="hljs-tag">&lt;/<span class="hljs-name">modelVersion</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">packaging</span>&gt;</span>war<span class="hljs-tag">&lt;/<span class="hljs-name">packaging</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>TestSpark<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>com.kfk.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>TestSpark<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>1.0-SNAPSHOT<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml"></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">properties</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">scala.version</span>&gt;</span>2.11.12<span class="hljs-tag">&lt;/<span class="hljs-name">scala.version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">scala.binary.version</span>&gt;</span>2.11<span class="hljs-tag">&lt;/<span class="hljs-name">scala.binary.version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">spark.version</span>&gt;</span>2.2.0<span class="hljs-tag">&lt;/<span class="hljs-name">spark.version</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;/<span class="hljs-name">properties</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">dependencies</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-core_$</span><span class="hljs-template-variable">&#123;scala.binary.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$</span><span class="hljs-template-variable">&#123;spark.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-streaming_$</span><span class="hljs-template-variable">&#123;scala.binary.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$</span><span class="hljs-template-variable">&#123;spark.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-sql_$</span><span class="hljs-template-variable">&#123;scala.binary.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$</span><span class="hljs-template-variable">&#123;spark.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-hive_$</span><span class="hljs-template-variable">&#123;scala.binary.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$</span><span class="hljs-template-variable">&#123;spark.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_$</span><span class="hljs-template-variable">&#123;scala.binary.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$</span><span class="hljs-template-variable">&#123;spark.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.hadoop<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>hadoop-client<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>2.6.0<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;/<span class="hljs-name">dependencies</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">project</span>&gt;</span></span><br><span class="language-xml"></span><br></code></pre></td></tr></table></figure>
<p>6、编写测试程序</p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><code class="hljs reasonml">import org.apache.spark.sql.SparkSession<br><br><span class="hljs-keyword">object</span> test &#123;<br>  def main(args: Array<span class="hljs-literal">[S<span class="hljs-identifier">tring</span>]</span>): Unit = &#123;<br><br>     <span class="hljs-keyword">val</span> spark = SparkSession<br>      .builder<br>       .master(<span class="hljs-string">&quot;yarn-cluster&quot;</span>)<br>     <span class="hljs-comment">//  .master(&quot;local[2]&quot;)</span><br>      .app<span class="hljs-constructor">Name(<span class="hljs-string">&quot;HdfsTest&quot;</span>)</span><br>      .get<span class="hljs-constructor">OrCreate()</span><br><br>    <span class="hljs-keyword">val</span> path = args(<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">val</span> out = args(<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">val</span> rdd = spark.sparkContext.text<span class="hljs-constructor">File(<span class="hljs-params">path</span>)</span><br>    <span class="hljs-keyword">val</span> lines = rdd.flat<span class="hljs-constructor">Map(<span class="hljs-params">_</span>.<span class="hljs-params">split</span>(<span class="hljs-string">&quot; &quot;</span>)</span>).map(x=&gt;(x,<span class="hljs-number">1</span>)).reduce<span class="hljs-constructor">ByKey((<span class="hljs-params">a</span>,<span class="hljs-params">b</span>)</span>=&gt;(a+b)).save<span class="hljs-constructor">AsTextFile(<span class="hljs-params">out</span>)</span><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>
<p>7、本地测试<br>直接master(“local[2]”)，指定windows下的路径就可以了。如果不能运行一定是开发环境有问题，主要看看hadoop环境变量配置了吗<br>8、打成jar包<br>可参考：<a href="https://jingyan.baidu.com/article/c275f6ba0bbb65e33d7567cb.html">https://jingyan.baidu.com/article/c275f6ba0bbb65e33d7567cb.html</a><br>9、上传至虚拟机中进行jar包方式提交到spark on yarn.<br>运行底层还是依赖于hdfs，前提要启动zk /hadoop /yarn.</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk">bin<span class="hljs-regexp">/spark-submit --class  test  --master yarn --deploy-mode cluster /</span>opt<span class="hljs-regexp">/jars/</span>TestSpark.jar  hdfs:<span class="hljs-regexp">//</span>ns<span class="hljs-regexp">/input/</span>stu.txt  hdfs:<span class="hljs-regexp">//</span>ns/out<br></code></pre></td></tr></table></figure>
<p>运行结束去，可以在yarn的web:<a href="http://bigdata-pro01.kfk.com:8088/cluster/">http://bigdata-pro01.kfk.com:8088/cluster/</a><br>看见调度success标志。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzol6uy6oaj30of09hjsa.jpg"><br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzom68klrwj315f0l23zq.jpg"></p>
<p>10、如果运行失败怎么办？看日志<br>有一个比较好的入口上图圈中的logs：<br>先配置yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log.server.url<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>http://bigdata-pro01.kfk.com:19888/jobhistory/logs<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br></code></pre></td></tr></table></figure>
<p>需要重启yarn，<br>并在你配置节点启动历史服务器./mr-jobhistory-daemon.sh start historyserver<br>点击：<a href="http://bigdata-pro01.kfk.com:8088/cluster">http://bigdata-pro01.kfk.com:8088/cluster</a><br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzom51hwo5j30pa0npaci.jpg"></p>
<h3 id="Spark-Streaming实时数据处理"><a href="#Spark-Streaming实时数据处理" class="headerlink" title="Spark Streaming实时数据处理"></a>Spark Streaming实时数据处理</h3><p><strong>Spark Streaming简介</strong></p>
<p>本质上就是利用批处理时间间隔来处理一小批的RDD集合。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzosp71irxj30g108hq75.jpg"></p>
<p><strong>idea中程序测试读取socket</strong></p>
<p>1、在节点1启动nc<br>nc -lk 9999<br>输入一些单词<br>2、在idea中运行程序</p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><code class="hljs reasonml">import org.apache.spark.SparkConf<br>import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;<br><span class="hljs-keyword">object</span> TestStreaming &#123;<br><br>  def main(args: Array<span class="hljs-literal">[S<span class="hljs-identifier">tring</span>]</span>): Unit = &#123;<br><br>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-constructor">SparkConf()</span>.set<span class="hljs-constructor">Master(<span class="hljs-string">&quot;local[2]&quot;</span>)</span>.set<span class="hljs-constructor">AppName(<span class="hljs-string">&quot;NetworkWordCount&quot;</span>)</span><br>    <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-constructor">StreamingContext(<span class="hljs-params">conf</span>, Seconds(5)</span>)<br><br>    <span class="hljs-keyword">val</span> lines = ssc.socket<span class="hljs-constructor">TextStream(<span class="hljs-string">&quot;bigdata-pro01.kfk.com&quot;</span>,9999)</span><br>    <span class="hljs-keyword">val</span> words = lines.flat<span class="hljs-constructor">Map(<span class="hljs-params">_</span>.<span class="hljs-params">split</span>(<span class="hljs-string">&quot; &quot;</span>)</span>)<br>    <span class="hljs-comment">//map reduce 计算</span><br>    <span class="hljs-keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="hljs-number">1</span>)).reduce<span class="hljs-constructor">ByKey(<span class="hljs-params">_</span> + <span class="hljs-params">_</span>)</span><br>    wordCounts.print<span class="hljs-literal">()</span><br>    ssc.start<span class="hljs-literal">()</span><br>    ssc.await<span class="hljs-constructor">Termination()</span><br><br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzot0y7ib8j30tl0a4mxy.jpg"></p>
<p><strong>sparkstreaming和kafka进行集成</strong></p>
<p>版本问题：<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzr9u0l3szj31cl048js0.jpg"><br>遇到了版本问题，之前用的是kafka0.9，现在和idea集成开发一般是kafka0.10了，还好官网里有支持kafka0.9程序案例，要不然就完犊子了，参考官网进行编写：<br><a href="http://spark.apache.org/docs/2.2.0/streaming-kafka-0-8-integration.html">http://spark.apache.org/docs/2.2.0/streaming-kafka-0-8-integration.html</a><br>代码案例：<a href="https://github.com/apache/spark/blob/v2.2.0/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala">https://github.com/apache/spark/blob/v2.2.0/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala</a><br>基于kafka0.9的测试程序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">import</span> kafka.serializer.<span class="hljs-type">StringDecoder</span><br><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span><br><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka.<span class="hljs-type">KafkaUtils</span><br><span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;<span class="hljs-type">Seconds</span>, <span class="hljs-type">StreamingContext</span>&#125;<br><br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">KfkStreaming</span> </span>&#123;<br>   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br><br>     <span class="hljs-keyword">val</span> spark  = <span class="hljs-type">SparkSession</span>.builder()<br>       .master(<span class="hljs-string">&quot;local[2]&quot;</span>)<br>       .appName(<span class="hljs-string">&quot;kfkstreaming&quot;</span>).getOrCreate()<br><br>     <span class="hljs-keyword">val</span> sc =spark.sparkContext<br>     <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sc, <span class="hljs-type">Seconds</span>(<span class="hljs-number">5</span>))<br><br>     <span class="hljs-keyword">val</span> topicsSet = <span class="hljs-type">Set</span>(<span class="hljs-string">&quot;weblogs&quot;</span>)<br>     <span class="hljs-keyword">val</span> kafkaParams = <span class="hljs-type">Map</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<span class="hljs-string">&quot;metadata.broker.list&quot;</span> -&gt; <span class="hljs-string">&quot;bigdata-pro01.kfk.com:9092&quot;</span>)<br>     <span class="hljs-keyword">val</span> messages = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>, <span class="hljs-type">StringDecoder</span>, <span class="hljs-type">StringDecoder</span>](<br>       ssc, kafkaParams, topicsSet)<br><br>     <span class="hljs-keyword">val</span> lines = messages.map(_._2)<br>     <span class="hljs-keyword">val</span> words = lines.flatMap(_.split(<span class="hljs-string">&quot; &quot;</span>))<br>     <span class="hljs-keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="hljs-number">1</span>L)).reduceByKey(_ + _)<br>     wordCounts.print()<br><br>     <span class="hljs-comment">// Start the computation</span><br>     ssc.start()<br>     ssc.awaitTermination()<br><br>   &#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure>
<p>在节点1上启动kafka程序</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">bin/kafka-server-start<span class="hljs-selector-class">.sh</span> config/server<span class="hljs-selector-class">.properties</span><br>bin/kafka-console-producer<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--broker-list</span> bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">9092</span> <span class="hljs-attr">--topic</span> weblogs<br><br></code></pre></td></tr></table></figure>
<p>运行结果：<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzr9tkp4szj30om07paas.jpg"></p>
]]></content>
      <tags>
        <tag>大数据项目</tag>
      </tags>
  </entry>
  <entry>
    <title>随笔</title>
    <url>/2022/03/11/%E9%9A%8F%E7%AC%94/</url>
    <content><![CDATA[<p> offer！offer！offer！！！</p>
<span id="more"></span>

<ul>
<li><p>Zookeeper</p>
</li>
<li><p>Kafka</p>
</li>
<li><p>Hadoop</p>
</li>
<li><p>HBase</p>
</li>
<li><p>Spark</p>
</li>
<li><p>MySQL</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka</title>
    <url>/2022/03/05/Kafka/</url>
    <content><![CDATA[<p>Kafka面试题</p>
<span id="more"></span>

<h4 id="1-kafka用途和使用场景"><a href="#1-kafka用途和使用场景" class="headerlink" title="1 kafka用途和使用场景"></a>1 kafka用途和使用场景</h4><p>日志收集</p>
<p>消息系统，解耦生产者和消费者</p>
<p>数据埋点收集，比如用户行为发送到特定kafka的topic，供数据分析使用</p>
<p>流式处理，比如spark streaming和storm （持续的从一个无边界的数据集读取数据，然后对它们进行处理并生成结果，就是流式处理）</p>
<h4 id="2-kafka中的ISR、AR代表什么，ISR的伸缩值什么？"><a href="#2-kafka中的ISR、AR代表什么，ISR的伸缩值什么？" class="headerlink" title="2 kafka中的ISR、AR代表什么，ISR的伸缩值什么？"></a>2 kafka中的ISR、AR代表什么，ISR的伸缩值什么？</h4><p>AR：assigned Replicas：分区中所有的副本统称为AR。AR = ISR+OSR</p>
<p>ISR：In-sync Replicas 所有与leader副本保持一定程度同步的副本（包括leader副本）组成IRS。</p>
<p>OSR：（out-sync Replicas）与leader副本同步滞后很多的副本（不包括leader副本）组成OSR。</p>
<p>-—-</p>
<p>ISR集合的副本必须满足的条件：</p>
<p>i) 副本所在的节点必须维持着与zk的链接</p>
<p>ii) 副本最后一条消息的offset与leader副本最后一条消息的offset之间的差值不能超过一定的阈值</p>
<p>-—</p>
<p>每个分区的leader副本都会维护此分区的ISR集合，写请求首先由leader副本处理，之后follower副本会从leader副本拉取数据，这个过程有延迟，导致follower副本中保存的消息略少于leader副本，只要不超过阈值就可以容忍。</p>
<p>-—</p>
<p>IRS的伸缩性指的是Kafka在启动的时候会开启两个与ISR相关的定时任务，分别是isr-expiration和isr-change-propagation。isr-expiration会周期性的检测每个分区是否需要缩减其ISR。</p>
<h4 id="3-Kafka中的HW、LEO、LSO、LW分别代表什么？"><a href="#3-Kafka中的HW、LEO、LSO、LW分别代表什么？" class="headerlink" title="3 Kafka中的HW、LEO、LSO、LW分别代表什么？"></a>3 Kafka中的HW、LEO、LSO、LW分别代表什么？</h4><p>高水位主要定义消息可见性，帮助kafka完成副本同步；</p>
<p>hw：high watermark（高水位）：表示一个特定消息的偏移量，消费者只能拉取到这个offset之前的消息。</p>
<p>leo：log end offset：表示当前日志文件中下一条写入消息的offset</p>
<p>lso：last stable offset：它与kafka的事务有关。消费者端可以配置参数isolation.level，这个参数用于配置消费者事务的隔离级别。有两个值：read_uncommitted(默认值)和read_committed。如果开启了事务，消费者只能读取到iso之前的消息。</p>
<p>LW：low watermark（低水位）：代表AR中最小的logStartOffset。</p>
<h4 id="4-Kafka怎么实现消息顺序性"><a href="#4-Kafka怎么实现消息顺序性" class="headerlink" title="4 Kafka怎么实现消息顺序性"></a>4 Kafka怎么实现消息顺序性</h4><p>对于生产者端：对于同一个topic，假如有三个分区，我们可以根据key，保证消息的顺序性。对于相同的key，取模以后会发送到相同的分区中。在同一个分区的消息是有序的。</p>
<p>对于消费者端：对于topic的每个分区，只能被消费者组中的一个消费者实例去消费，又因为同一个分区中消息是有顺序的，所以就保证了消费的顺序性。</p>
<p>如果需要保证整个topic的消息有序性，就只设置一个分区。</p>
<h4 id="5-分区器、序列化器、拦截器，以及它们的处理顺序"><a href="#5-分区器、序列化器、拦截器，以及它们的处理顺序" class="headerlink" title="5 分区器、序列化器、拦截器，以及它们的处理顺序"></a>5 分区器、序列化器、拦截器，以及它们的处理顺序</h4><p>整个kafka生产者客户端有两个小城协调运行。这两个线程是主线程和sender线程（发送线程）。</p>
<p>主线程的作用：由KafkaProducer创建消息，然后通过可能的拦截器、序列化器、分区器的作用之后缓存到消息累加器。</p>
<p>发送线程的作用：负责将消息累加器中的消息发送到kafka中。</p>
<p>拦截器：分为生产者拦截器和消费者拦截器。生产者拦截器用于在消息发送前做一些准备工作，也可以用来在发送回调逻辑前做一些定制化的需求。</p>
<p>序列化器：将key和value序列化成字节数组才可以将消息传入kafka。同样消费者需要用反序列化器把从kafka收到的字节数组转化成相应的对象。</p>
<p>分区器：如果ProducerRecord指定了分区，就不需要分区器的作用。否则会经过分区器。分区器中的partition方法会根据入参(topic, key 序列化前后的value等)最终返回一个int类型的分区号。</p>
<p>消息累加器：主要用来缓存消息以便Sender线程批量发送，从而减少网络传输资源。</p>
<h4 id="6-生产者客户端的整体结构"><a href="#6-生产者客户端的整体结构" class="headerlink" title="6 生产者客户端的整体结构"></a>6 生产者客户端的整体结构</h4><p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h06wrc8wgdj315a0mcacn.jpg" alt="img"></p>
<p>可参考文档：<a href="https://blog.csdn.net/ThreeAspects/article/details/108130254">https://blog.csdn.net/ThreeAspects/article/details/108130254</a></p>
<h4 id="7-消费者组的消费者个数如果超过topic的分区数，那么就会有消费者消费不到数据，这句话是否正确。"><a href="#7-消费者组的消费者个数如果超过topic的分区数，那么就会有消费者消费不到数据，这句话是否正确。" class="headerlink" title="7 消费者组的消费者个数如果超过topic的分区数，那么就会有消费者消费不到数据，这句话是否正确。"></a>7 消费者组的消费者个数如果超过topic的分区数，那么就会有消费者消费不到数据，这句话是否正确。</h4><p>首先一个消费者组可以订阅多个topic，默认情况当消费者组中消费者的实例小于订阅topic的分区总数时，会有消费者消费不到数据。但是可以通过自定义分区分配策略，可以将一个消费者实例指定消费所有分区。</p>
<h4 id="8-有哪些情况会导致重复消费"><a href="#8-有哪些情况会导致重复消费" class="headerlink" title="8 有哪些情况会导致重复消费"></a>8 有哪些情况会导致重复消费</h4><p>如果设置的手动提交位移，但是消费者消费后没有commit offset（程序崩溃、强行kill、消费超时）</p>
<h4 id="9-有哪些情况会导致消息漏消费"><a href="#9-有哪些情况会导致消息漏消费" class="headerlink" title="9 有哪些情况会导致消息漏消费"></a>9 有哪些情况会导致消息漏消费</h4><p>与消费者有关的几个配置参数</p>
<p>enable.auto.commit，默认是true，表示会自动提交位移</p>
<p>auto.commit.interval.ms 默认是5s，在enable.auto.commit为true时，自动提交的间隔。</p>
<p>max.poll.records 默认是500，单词消费者拉取消息的最大条数</p>
<p>max.poll.interval.ms 默认是5min，表示如果5分钟内消费者没有消费完上次poll的消息，那么该消费者实例就主动发起离开group的请求。</p>
<p>-—</p>
<p>当enable.auto.commit为true时，当上一次poll拉取的数据消费完会进行下一次的poll，在经过auto.commit.interval.ms间隔后，下一次poll的时候，会提交之前所有消费消息的offset。</p>
<p>举例说明：</p>
<p>假如只设置一个patition。对于消费者组的中这个消费者实例，我们设置每次拉取20条消息，消费每条消息耗时1s。设置auto.commit.interval.ms为30s。</p>
<p>可以知道消费组的offset每40s更新一次。因为每次poll会拉20条消息，消费这20条消息需要20s，在下一次拉取消息的时候还不到30s，所以不会提交offset。等第二次消费完成（就是第40s的时候），再次poll的时候，大于30s了，就会提交位移，此时会提交前40条消息的位移。</p>
<p>—-基于以上的消费逻辑，可知重复消费的情况</p>
<p>1）customer在消费过程中，应用被强制kill掉或异常退出。比如一次poll了500条，消费到200时，应用被强制退出导致offset没有提交。重启后就会重复消费</p>
<p>2）消费者消费时间过长，超过了max.poll.interval.ms，如果在该时间段内没有消费完，消费者实例退出group会导致rebalance，进而导致消息重复消费。</p>
<p>—-解决方案：</p>
<p>1）提高消费消息的速度，可以异步处理</p>
<p>2）适当增到max.poll.interval.ms时间，减少没必要的rebalance</p>
<p>3）在消费者应用程序中先保存消息的唯一标识，每次poll过来先去重再消费。</p>
<h4 id="10-kafkaConsumer是非线程安全的，如何实现多线程消费？"><a href="#10-kafkaConsumer是非线程安全的，如何实现多线程消费？" class="headerlink" title="10 kafkaConsumer是非线程安全的，如何实现多线程消费？"></a>10 kafkaConsumer是非线程安全的，如何实现多线程消费？</h4><h4 id="11-简述消费者和消费者组的关系"><a href="#11-简述消费者和消费者组的关系" class="headerlink" title="11 简述消费者和消费者组的关系"></a>11 简述消费者和消费者组的关系</h4><h4 id="12-当你通过kafka-topics-sh创建-删除一个topic时，kakfa背后的执行逻辑"><a href="#12-当你通过kafka-topics-sh创建-删除一个topic时，kakfa背后的执行逻辑" class="headerlink" title="12 当你通过kafka-topics.sh创建/删除一个topic时，kakfa背后的执行逻辑"></a>12 当你通过kafka-topics.sh创建/删除一个topic时，kakfa背后的执行逻辑</h4><p>1）会在zk中的/brokers/topics节点下创建一个新的topic节点</p>
<p>2）触发controller的监听程序</p>
<p>3）kafka controller负责topic的创建工作，并更新metadata cache</p>
<h4 id="13-topic的分区是否可以增加（减少），如何增加（减少），如果不可以，为什么？"><a href="#13-topic的分区是否可以增加（减少），如何增加（减少），如果不可以，为什么？" class="headerlink" title="13 topic的分区是否可以增加（减少），如何增加（减少），如果不可以，为什么？"></a>13 topic的分区是否可以增加（减少），如何增加（减少），如果不可以，为什么？</h4><p>可以增加，不可以减少，因为被删除的分区的数据没有办法处理</p>
<p>增加的命令：bin/kafka-topics.sh –zookeeper localhost：2181/kafka –alter –topic topic-config –patition 3(只能大于之前的值，否则会报错)</p>
<h4 id="14-创建topic时，如何选择合适的分区数"><a href="#14-创建topic时，如何选择合适的分区数" class="headerlink" title="14 创建topic时，如何选择合适的分区数"></a>14 创建topic时，如何选择合适的分区数</h4><p>根据业务量吧，先测试一个分期的吞吐量，在根据业务数据量，确认需要多少分区</p>
<h4 id="15-kafka内部有哪些topic，特征和作用是什么"><a href="#15-kafka内部有哪些topic，特征和作用是什么" class="headerlink" title="15 kafka内部有哪些topic，特征和作用是什么"></a>15 kafka内部有哪些topic，特征和作用是什么</h4><p>_customer_offset 保存消费组的偏移量</p>
<h4 id="16-简述kafka的日志目录结构，kafka有哪些索引文件"><a href="#16-简述kafka的日志目录结构，kafka有哪些索引文件" class="headerlink" title="16 简述kafka的日志目录结构，kafka有哪些索引文件"></a>16 简述kafka的日志目录结构，kafka有哪些索引文件</h4><p>每个分区对应一个文件夹，文件夹的命名 topic-0，topic-1，内部有.log文件和.index文件。</p>
<h4 id="17-kafka的过期数据清理"><a href="#17-kafka的过期数据清理" class="headerlink" title="17 kafka的过期数据清理"></a>17 kafka的过期数据清理</h4><p>清理的策略有两个，删除和压缩</p>
<h4 id="18-kafka中的幂等如何实现"><a href="#18-kafka中的幂等如何实现" class="headerlink" title="18 kafka中的幂等如何实现"></a>18 kafka中的幂等如何实现</h4><p>只能保存同一个分区内保存幂等。通过produceId累加的方式保证幂等</p>
<h4 id="19-kafka的事务"><a href="#19-kafka的事务" class="headerlink" title="19 kafka的事务"></a>19 kafka的事务</h4><p>设置read committed</p>
<p>设置enable.idempotence = true</p>
<p>上面代码段能够保证 Record1 和 Record2 被当作一个事务统一提交到 Kafka，要么它们全部提交成功，要么全部写入失败。实际上即使写入失败，Kafka 也会把它们写入到底层的日志中，也就是说 Consumer 还是会看到这些消息（如果事务失败中止了，Kafka没法像数据库一样执行回滚操作）。因此在 Consumer 端，读取事务型 Producer 发送的消息也是需要一些变更的。修改起来也很简单，设置 isolation.level 参数的值即可。</p>
<h4 id="20-kafka哪些地方需要选举，这些地方的选举策略有哪些"><a href="#20-kafka哪些地方需要选举，这些地方的选举策略有哪些" class="headerlink" title="20 kafka哪些地方需要选举，这些地方的选举策略有哪些"></a>20 kafka哪些地方需要选举，这些地方的选举策略有哪些</h4><p>partition leader的选举，用到的选举是ISR</p>
<p>然后是kafka的controller， 用到的是先到先得的策略</p>
<h4 id="21-kafka中的延迟队列如何实现"><a href="#21-kafka中的延迟队列如何实现" class="headerlink" title="21 kafka中的延迟队列如何实现"></a>21 kafka中的延迟队列如何实现</h4><p>当你在网上购物的时候是否会遇到这样的提示：“三十分钟之内未付款，订单自动取消”？这个是延迟队列的一种典型应用场景。</p>
<p>延迟队列存储的是对应的延迟消息，所谓“延迟消息”是指当消息被发送以后，并不想让消费者立刻拿到消息，而是等待特定时间后，消费者才能拿到这个消息进行消费。</p>
<h4 id="22-kafka如何实现高吞吐"><a href="#22-kafka如何实现高吞吐" class="headerlink" title="22 kafka如何实现高吞吐"></a>22 kafka如何实现高吞吐</h4><p>使用分区的策略</p>
<h4 id="生产者客户端"><a href="#生产者客户端" class="headerlink" title="生产者客户端"></a>生产者客户端</h4><p>​    生产者端可以自定义消息的分区策略，</p>
<p>​    1)自定义一个实现类，继承producer.Partitioner接口，并实现其partition接口即可，接口参数（topic，key keyByte, value valueByte, Cluster）</p>
<p>​    2)同时配置 kafkaProps.put(“partitioner.class”, “com.xx.partitioner.MyPartitioner”);</p>
<h4 id="消费者客户端"><a href="#消费者客户端" class="headerlink" title="消费者客户端"></a>消费者客户端</h4><p>​    1）为了实现消费者的吞吐量，引入了消费者组的概念。指的是多个消费者实例共同组成一个组来消费一组主题。这组主题中的每个分区都只会被组内的一个消费者实例消费，其他消费者实例不能消费它。同时消费者组内的消费者中如果有新加入或者退出，kafka会自动检测，进行重平衡。</p>
<p>​    2）Rebalance 就是让一个Consumer Group 下所有的Consumer实例就如何消费订阅主题的所有分区达成共识的过程。在 Rebalance 过程中，所有 Consumer 实例共同参与，在协调者组件的帮助下，完成订阅主题分区的分配。所谓协调者，在 Kafka 中对应的术语是 Coordinator，它专门为 Consumer Group 服务，负责为 Group 执行 Rebalance 以及提供位移管理和组成员管理等。</p>
<p>​    具体来讲，Consumer 端应用程序在提交位移时，其实是向 Coordinator 所在的 Broker 提交位移。同样地，当 Consumer 应用启动时，也是向 Coordinator 所在的 Broker 发送各种请求，然后由 Coordinator 负责执行消费者组的注册、成员管理记录等元数据管理操作。</p>
<p>​    3）根据groupId的hashcode，对分区数取模，找出一个分区id，然后根据该分区leader副本所在的broker作为对应的Coordinator。当kafka集群中的第一个consumer程序启动时，就会自动创建位移主题</p>
<p>​    4）从Kafka 0.10.1.0 版本开始，KafkaConsumer就变为了双线程的设计，即用户主线程和心跳线程。但是心跳线程知识标识消费者是否存活。可以理解为KafkaConsumer仍是单线程设计。如何支持多线程有两个方案</p>
<p>​    i）消费者程序启动多个线程，每个线程维护专属的kafkaconsumer实例</p>
<p>​    ii）获取消费由一个或多个线程处理，处理消息交给特定的一个线程池来做，实现消息获取和处理的解耦。</p>
<p>5）重平衡触发的条件：组成员数量发生变化、订阅主题数量发生变化、订阅主题的分区数发生变化</p>
<h4 id="kafka如何实现高可用："><a href="#kafka如何实现高可用：" class="headerlink" title="kafka如何实现高可用："></a>kafka如何实现高可用：</h4><p>​    多个不同的broker分散在不同的机器上：当集群中一台机器宕机，即使它上面运行的所有broker进程都挂了，其他机器上的ebroker还能对外提供服务。</p>
<p>​    副本机制：相同的数据拷贝在多台机器上。一个topic可以分为多个分区，对分区进行副本机制的管理，每个分区有一个leader分区和多个follower分区。</p>
<p>kafka的伸缩性</p>
<p>​    分区概念的引入：一个topic可以有多个分区。副本就是在分区这个层面定义的</p>
<h4 id="kakfa服务器"><a href="#kakfa服务器" class="headerlink" title="kakfa服务器"></a>kakfa服务器</h4><p>​    kafka的服务器端由Broker服务进程构成，一个kafka集群由多个broker组成。broker负责接收和处理客户端发送过程的请求，以及对消息进行持久化。</p>
<p>​    如何持久化数据：</p>
<p>​        kafka使用消息日志（log）保存数据，一个日志就是磁盘上的一个只能追加写的物理文件。因为只能追加写是顺序io，实现了kafka高吞吐的一个重要手段。</p>
<p>​        当然kafka也会定期的删除磁盘的一些数据。    使用的是日志段机制（log segemnt）。在kafka底层，一个日志又分为多个日志段，消息被追加写入到最新的日志段中。当一个日志段写满以后，kafka会自动切出一个新的日志段，并将老的日志段封存起来。kafka会在后台定期检查老的日志段是否能够被删除，从而实现磁盘空间的收回。</p>
<h4 id="磁盘大小预估："><a href="#磁盘大小预估：" class="headerlink" title="磁盘大小预估："></a>磁盘大小预估：</h4><p>​    考虑因素：新增消息数、消息大小、消息存留时间、备份数、是否启用压缩    </p>
<h4 id="broker端如何处理请求流程"><a href="#broker端如何处理请求流程" class="headerlink" title="broker端如何处理请求流程"></a>broker端如何处理请求流程</h4><p>kafka的客户端和broker的交互都是通过“请求/相应”的方式完成的，在tcp协议的基础上定义了一组自己的请求协议。比如PRODUCE请求用于生产消息；FETCH请求用于消息消息；METADATE请求用于请求集群元数据。所有请求都是通过tcp网络以socket的方式进行通讯</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h06wsc5z9rj30n30jygoc.jpg" alt="img"></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h06wsn3r8aj30mv0msn04.jpg" alt="img"></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h06wt89flwj30mv0o1q8g.jpg" alt="img"></p>
<h4 id="kafka的控制器组件"><a href="#kafka的控制器组件" class="headerlink" title="kafka的控制器组件"></a>kafka的控制器组件</h4><p>1）主要作用是在apache zookeeper帮助下管理和协调整个kafka集群，集群中任意一台 Broker 都能充当控制器的角色，但是只有一个broker充当控制器。</p>
<p>2）zk是一个提供高可靠性的分布式协调服务框架。它使用的是类似于文件系统的树形结构，根节点以/开始，该结构的每个节点成为znode。用于保存元数据协调信息。</p>
<p>3）ZooKeeper 赋予客户端监控 znode 变更的能力，即所谓的 Watch 通知功能。一旦 znode 节点被创建、删除，子节点数量发生变化，抑或是 znode 所存的数据本身变更，ZooKeeper 会通过节点变更监听器 (ChangeHandler) 的方式显式通知客户端。kafka大量使用watch功能实现对集群的协调管理。</p>
<p>4）对于控制器的选取，kafka每个broker启动的时候，都会实例化一个KafkaController，会尝试去 ZooKeeper 中创建 /controller 节点。Kafka 当前选举控制器的规则是：第一个成功创建 /controller 节点的 Broker 会被指定为控制器。</p>
<p>—=</p>
<h4 id="控制器职责："><a href="#控制器职责：" class="headerlink" title="控制器职责："></a>控制器职责：</h4><p>主题管理（创建删除增加分区）</p>
<p>分区重分配</p>
<p>领导者选举（领导者选举主要是 Kafka 为了避免部分 Broker 负载过重而提供的一种换 Leader 的方案。这也是控制器的职责范围）</p>
<p>集群成员管理（新增 Broker、Broker 主动关闭、Broker 宕机）</p>
<h4 id="高水位的更新机制"><a href="#高水位的更新机制" class="headerlink" title="高水位的更新机制"></a>高水位的更新机制</h4><p>每个副本对象都保存了一组高水位值和 LEO 值，但实际上，在 Leader 副本所在的 Broker 上，还保存了其他 Follower 副本的 LEO 值。</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h06wthq5lvj30lb0lcwgq.jpg" alt="img"></p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h06wtrf9y7j30lo0cymyw.jpg" alt="img"></p>
<p>在新一轮的拉取请求中，由于位移值是 0 的消息已经拉取成功，因此 Follower 副本这次请求拉取的是位移值 =1 的消息。Leader 副本接收到此请求后，更新远程副本 LEO 为 1，然后更新 Leader 高水位为 1。做完这些之后，它会将当前已更新过的高水位值 1 发送给 Follower 副本。Follower 副本接收到以后，也将自己的高水位值更新成 1。至此，一次完整的消息同步周期就结束了。事实上，Kafka 就是利用这样的机制，实现了 Leader 和 Follower 副本之间的同步。</p>
<p>—数据丢失</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h06wu2gjjdj317w0mzn1k.jpg" alt="img"></p>
<h4 id="Leader-Epoch实现高水位更新机制"><a href="#Leader-Epoch实现高水位更新机制" class="headerlink" title="Leader Epoch实现高水位更新机制"></a>Leader Epoch实现高水位更新机制</h4><p>上述的高水位更新需要额外多一次的拉取，当有多个follower时，就可能出现时间上的错配。</p>
<p>所谓leader epoch可以认为是leader的版本，由两部分组成：</p>
<p>i) Epoch 一个单调增加的版本号。每当leader副本变更时，都会增加该版本号。小版本号的leader表示已过期的leader，不再行使leader权利</p>
<p>ii) 起始位置（start oddset）leader副本在该epoch值上写入的首条消息的位移。</p>
<p>leader端多开辟一段内存保存leader的epoch信息。</p>
<p>leader broker中会保存这样的一个缓存，并定期地写入到一个checkpoint文件中。</p>
<p>当leader写底层log时它会尝试更新整个缓存——如果这个leader首次写消息，则会在缓存中增加一个条目；否则就不做更新。而每次副本重新成为leader时会查询这部分缓存，获取出对应leader版本的位移，这就不会发生数据不一致和丢失的情况。</p>
<h4 id="调优kafka"><a href="#调优kafka" class="headerlink" title="调优kafka"></a>调优kafka</h4><p>kafka的性能一般指吞吐量和延时</p>
<p>调优优先级：应用程序层 -&gt; 框架层 -&gt; JVM层  -&gt; 操作系统层</p>
<p>第 1 层：应用程序层。它是指优化 Kafka 客户端应用程序代码。比如，使用合理的数据结构、缓存计算开销大的运算结果，抑或是复用构造成本高的对象实例等。这一层的优化效果最为明显，通常也是比较简单的。</p>
<p>第 2 层：框架层。它指的是合理设置 Kafka 集群的各种参数。毕竟，直接修改 Kafka 源码进行调优并不容易，但根据实际场景恰当地配置关键参数的值，还是很容易实现的。</p>
<p>第 3 层：JVM 层。Kafka Broker 进程是普通的 JVM 进程，各种对 JVM 的优化在这里也是适用的。优化这一层的效果虽然比不上前两层，但有时也能带来巨大的改善效果。</p>
<p>第 4 层：操作系统层。对操作系统层的优化很重要，但效果往往不如想象得那么好。与应用程序层的优化效果相比，它是有很大差距的。</p>
]]></content>
      <tags>
        <tag>中间件</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark</title>
    <url>/2022/03/04/Spark/</url>
    <content><![CDATA[<blockquote>
<p>Spark大数据的计算分析引擎  官网 <a href="https://spark.apache.org/">https://spark.apache.org/</a></p>
</blockquote>
<ul>
<li>本文摘录于《Spark大数据处理技术》</li>
</ul>
<span id="more"></span>

<h4 id="Spark概述"><a href="#Spark概述" class="headerlink" title="Spark概述"></a><strong>Spark概述</strong></h4><blockquote>
<p>Apache Spark is a unified analytics engine for large-scale data processing</p>
</blockquote>
<p>按照现在流行的大数据处理场景划分，可以将大数据处理分为三种情况：</p>
<ul>
<li>复杂的批量数据处理，通常时间跨度为数十分钟到数小时</li>
<li>基于历史数据的交互式查询，通常时间跨度为数十秒到数分钟</li>
<li>基于实时数据流的数据处理，通常时间跨度为数百毫秒到数秒</li>
</ul>
<p>在Spark Core基础上衍生出能同时处理上面三种情形的统一大数据处理平台</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h02j0862b7j30ky04ijrl.jpg"></p>
<p>Spark要做的是将批处理，交互式处理，流式处理融合到一个软件栈中</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h02j2w0ltxj30mu092mxt.jpg"></p>
<h4 id="Spark-RDD及编程接口"><a href="#Spark-RDD及编程接口" class="headerlink" title="Spark RDD及编程接口"></a>Spark RDD及编程接口</h4><p><strong>HelloWorld</strong></p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h02j5gw5b9j30pg047t90.jpg"></p>
<ul>
<li>对于Spark程序来讲，所有的操作的前提是要有一个Spark上下文，创建上下文过程中，程序会向集群申请资源并构建运行环境。</li>
<li>通过sc变量并利用textFile接口从HDFS文件系统读取文件，返回file变量</li>
<li>对file进行过滤操作，生成新的变量filterRDD</li>
<li>对filterRDD进行cache操作(方便重用filterRDD)</li>
<li>对filterRDD进行计数，返回结果</li>
</ul>
<p>这个程序中涉及到众多概念：</p>
<ul>
<li>弹性式分布式数据集 RDD</li>
<li>创建操作(creation operation)：RDD初始创建是SparkContext负责，将内存集合或外部文件系统作为输入源</li>
<li>转换操作(transformation operation)：将一个RDD通过一定操作转变为另一个RDD</li>
<li>控制操作(control operation)：对RDD进行持久化，让RDD保存磁盘或内存中，方便重复使用</li>
<li>行动操作(action operation)：Spark是惰性计算的，RDD所有行动操作，都会触发Spark作业运行</li>
</ul>
<p>RDD和操作之间关系：经过输入操作，转换操作，控制操作，输出操作来完成一个作业</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h02jiomgszj30lh0ckdgp.jpg"></p>
<p><strong>Spark RDD</strong></p>
<p>RDD是弹性分布式数据集，即一个RDD代表一个被分区的只读数据集，RDD有两种生成途径：来自内存集合和外部存储系统；通过RDD的转换操作</p>
<p>RDD继承关系(lineage)构建可以通过记录作用在RDD上的转换操作，可以有效进行容错处理</p>
<p>一般情况下抽象的RDD需要包含这五个接口</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h02jtb7ww4j30ki05jt96.jpg"></p>
<p><strong>RDD分区(partitions)</strong></p>
<p>对于RDD来说，分区数涉及到RDD进行并行计算的粒度，每一个RDD分区的计算操作都在一个单独的任务中被执行。</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">100</span>,<span class="hljs-number">2</span>)<br><span class="hljs-comment">//1-100的数组转为RDD，第二个参数执行分区数</span><br>rdd<span class="hljs-selector-class">.partitions</span><span class="hljs-selector-class">.size</span><br><span class="hljs-comment">//查看RDD被划分的分区数</span><br></code></pre></td></tr></table></figure>

<p><strong>RDD优先位置(preferredLocations)</strong></p>
<p>RDD优先位置属性与Spark中的调度相关，返回的是RDD的每个partition存储的位置，“移动数据不如移动计算”所以Spark任务调度时候尽可能将任务分配到数据块存储位置</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">val rdd = sc<span class="hljs-selector-class">.textFile</span>(<span class="hljs-string">&quot;hdfs://10.0.2.19:8000/bigfile&quot;</span>)<br><span class="hljs-comment">//读取bigfile生成rdd</span><br>val hadoopRDD =rdd<span class="hljs-selector-class">.dependencies</span>(<span class="hljs-number">0</span>)<span class="hljs-selector-class">.rdd</span><br><span class="hljs-comment">//通过rdd依赖关系找到原始hadoopRDD</span><br>hadoopRDD<span class="hljs-selector-class">.partitions</span><span class="hljs-selector-class">.size</span><br>hadoopRDD<span class="hljs-selector-class">.preferredLocations</span>(hadoopRDD<span class="hljs-selector-class">.partition</span>(<span class="hljs-number">0</span>))<br><span class="hljs-comment">//返回partition(0)所在的机器位置</span><br></code></pre></td></tr></table></figure>

<p><strong>RDD依赖关系(dependencies)</strong></p>
<p>由于RDD是粗粒度操作数据集，每个转换操作都会生成一个新的RDD，所以RDD之间会形成类似流水线(pipline)的前后依赖关系，Spark中有两种类型依赖。</p>
<p>窄依赖(Narrow Dependencies)：每一个父RDD的分区最多只能被子RDD的一个分区使用</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h02kef8mpbj30hr0cmq3c.jpg"></p>
<p>宽依赖(Wide Dependencies)：多个子RDD的分区会依赖于同一个父RDD的分区</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h02kewzqs2j30ik07cweq.jpg"></p>
<p>Spark中明确区分宽窄依赖原因？</p>
<ol>
<li>窄依赖可以在集群的一个节点上如流水线一样执行，可以计算所有父RDD分区，相反宽依赖需要取得父RDD的所有分区上的数据进行计算，将执行类似MapReduce一样的Shuffle操作</li>
<li>对于窄依赖，节点计算失败后恢复会更有效，只需重新计算对应父RDD的分区，而且可以在其他节点上并行的计算，相反在宽依赖依赖关系中，一个节点失败会导致其父RDD的多个分区重新计算</li>
</ol>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">val</span> rdd =sc.make<span class="hljs-constructor">RDD(1 <span class="hljs-params">to</span> 10)</span><br><span class="hljs-keyword">val</span> mapRDD= rdd.map(x=&gt;(x,x))<br>mapRDD.dependencies<br><span class="hljs-keyword">val</span> shuffleRDD = mapRDD.partition<span class="hljs-constructor">By(<span class="hljs-params">new</span> <span class="hljs-params">org</span>.<span class="hljs-params">apache</span>.<span class="hljs-params">spark</span>.HashPartitioner(3)</span>)<br>shuffleRDD.dependencies<br></code></pre></td></tr></table></figure>

<p><strong>RDD分区计算(compute)</strong></p>
<p>RDD的计算都是以partition为单位的，而且RDD中的compute函数都是在对迭代器进行复合，不需要保存每次计算结果</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">10</span>,<span class="hljs-number">2</span>)<br>val map_rdd = rdd<span class="hljs-selector-class">.map</span>(a=&gt;a+<span class="hljs-number">1</span>)<br>val filter_rdd = map_rdd<span class="hljs-selector-class">.filter</span>(a=&gt;(a&gt;<span class="hljs-number">3</span>))<br>val context = new org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.TaskContext</span>(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>)<br>val iter0 = filter_rdd<span class="hljs-selector-class">.compute</span>(filter_rdd<span class="hljs-selector-class">.partition</span>(<span class="hljs-number">0</span>),context)<br>iter0<span class="hljs-selector-class">.toList</span><br>val iter1 = filter_rdd<span class="hljs-selector-class">.compute</span>(filter_rdd<span class="hljs-selector-class">.partition</span>(<span class="hljs-number">0</span>),context)<br>iter1<span class="hljs-selector-class">.toList</span><br><span class="hljs-comment">//在rdd上进行map和filter，由于compute函数只返回相应分区数据的迭代器，只有最后实例化才能显示出两个分区最终计算结果</span><br></code></pre></td></tr></table></figure>

<p><strong>RDD分区函数(partitioner)</strong></p>
<p>Spark目前有两种类型分区函数：HashPartitioner(哈希分区)和RangePartitioner(区域分区)，而且partitioner这个属性只存在于(K,V)类型的RDD中，对于非(K,V)类型的partitioner的值就是None，partitioner函数既决定了RDD本身的分区数量，也可作为其父RDD Shuffle输出(MapOutput)中每个分区进行数据切割的依据</p>
<p>使用HashPartitioner说明一下partitioner的功能</p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">val</span> rdd =sc.make<span class="hljs-constructor">RDD(1 <span class="hljs-params">to</span> 10,2)</span>.map(x=&gt;(x,x))<br>rdd.partitioner<br><span class="hljs-keyword">val</span> group_rdd = rdd.group<span class="hljs-constructor">ByKey(<span class="hljs-params">new</span> <span class="hljs-params">org</span>.<span class="hljs-params">apache</span>.<span class="hljs-params">spark</span>.HashPartitioner(3)</span>)<br><span class="hljs-comment">//new 了HashPartitioner对象</span><br>group_rdd.partitioner<br>group_rdd.collect<span class="hljs-constructor">Partitions()</span><br></code></pre></td></tr></table></figure>

<p>HashPartitioner的原理图：</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h02nqgxyjwj3084071mx8.jpg"></p>
<p><strong>集合创建操作</strong></p>
<p>RDD的创建可由内部集合类型来生成，Spark提供了parallelize和makeRDD两类函数实现从集合生成RDD，但makeRDD中提供了一个可以指定每一个分区preferredLocations参数的实现版本</p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">val</span> rdd = sc.make<span class="hljs-constructor">RDD(1 <span class="hljs-params">to</span> 10,3)</span><br>rdd.collect<span class="hljs-constructor">Partitions()</span><br><span class="hljs-keyword">val</span> collect = <span class="hljs-constructor">Seq(1 <span class="hljs-params">to</span> 10,Seq(<span class="hljs-string">&quot;host1&quot;</span>,<span class="hljs-string">&quot;host3&quot;</span>)</span>),(<span class="hljs-number">11</span> <span class="hljs-keyword">to</span> <span class="hljs-number">20</span>, <span class="hljs-constructor">Seq(<span class="hljs-string">&quot;host2&quot;</span>)</span>))<br><span class="hljs-keyword">val</span> rdd =sc.make<span class="hljs-constructor">RDD(<span class="hljs-params">collect</span>)</span><br>rdd.preferreed<span class="hljs-constructor">Locations(<span class="hljs-params">rdd</span>.<span class="hljs-params">partitions</span>(0)</span>)<br>rdd.preferred<span class="hljs-constructor">Locations(<span class="hljs-params">rdd</span>.<span class="hljs-params">partitions</span>(1)</span>)<br></code></pre></td></tr></table></figure>

<p><strong>存储创建操作</strong></p>
<p>主要是hadoopRDD和newHadoopRDD两个编程接口，包含了四个参数(输入格式；键类型；值类型；分区值)</p>
<p><strong>转换操作(transformation operation)</strong></p>
<table>
<thead>
<tr>
<th align="left">Transformation</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>map</strong>(<em>func</em>)</td>
<td align="left">将处理的数据逐条进行映射转换(类型或值)</td>
</tr>
<tr>
<td align="left"><strong>filter</strong>(<em>func</em>)</td>
<td align="left">将数据根据指定的规则进行筛选过滤</td>
</tr>
<tr>
<td align="left"><strong>flatMap</strong>(<em>func</em>)</td>
<td align="left">将处理的数据进行扁平化后再进行映射处理</td>
</tr>
<tr>
<td align="left"><strong>mapPartitions</strong>(<em>func</em>)</td>
<td align="left">将待处理的数据以分区为单位发送到计算节点进行处理</td>
</tr>
<tr>
<td align="left"><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td>
<td align="left">将待处理的数据以分区为单位发送到计算节点进行处理，在处理时同时可以获取当前分区索引。</td>
</tr>
<tr>
<td align="left"><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td>
<td align="left">根据指定的规则从数据集中抽取数据</td>
</tr>
<tr>
<td align="left"><strong>union</strong>(<em>otherDataset</em>)</td>
<td align="left">对源 RDD 和参数 RDD 求并集后返回一个新的 RDD</td>
</tr>
<tr>
<td align="left"><strong>intersection</strong>(<em>otherDataset</em>)</td>
<td align="left">对源 RDD 和参数 RDD 求交集后返回一个新的 RDD</td>
</tr>
<tr>
<td align="left"><strong>distinct</strong>([<em>numPartitions</em>]))</td>
<td align="left">将数据集中重复的数据去重</td>
</tr>
<tr>
<td align="left"><strong>groupByKey</strong>([<em>numPartitions</em>])</td>
<td align="left">将数据源的数据根据 key 对 value 进行分组</td>
</tr>
<tr>
<td align="left"><strong>reduceByKey</strong>(<em>func</em>, [<em>numPartitions</em>])</td>
<td align="left">可以将数据按照相同的 Key 对 Value 进行聚合</td>
</tr>
<tr>
<td align="left"><strong>aggregateByKey</strong>(<em>zeroValue</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numPartitions</em>])</td>
<td align="left">将数据根据不同的规则进行分区内计算和分区间计算</td>
</tr>
<tr>
<td align="left"><strong>sortByKey</strong>([<em>ascending</em>], [<em>numPartitions</em>])</td>
<td align="left">在一个(K,V)的 RDD 上调用，K 必须实现 Ordered 接口(特质)，返回一个按照 key 进行排序的RDD</td>
</tr>
<tr>
<td align="left"><strong>join</strong>(<em>otherDataset</em>, [<em>numPartitions</em>])</td>
<td align="left">在类型为(K,V)和(K,W)的 RDD 上调用，返回一个相同 key 对应的所有元素连接在一起的 (K,(V,W))的 RDD</td>
</tr>
<tr>
<td align="left"><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numPartitions</em>])</td>
<td align="left">在类型为(K,V)和(K,W)的 RDD 上调用，返回一个(K,(Iterable,Iterable))类型的 RDD</td>
</tr>
<tr>
<td align="left"><strong>coalesce</strong>(<em>numPartitions</em>)</td>
<td align="left">根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率 当 spark 程序中，存在过多的小任务的时候，可以通过 coalesce 方法，收缩合并分区，减少 分区的个数，减小任务调度成本</td>
</tr>
<tr>
<td align="left"><strong>repartition</strong>(<em>numPartitions</em>)</td>
<td align="left">该操作内部其实执行的是 coalesce 操作，参数 shuffle 的默认值为 true。无论是将分区数多的 RDD 转换为分区数少的 RDD，还是将分区数少的 RDD 转换为分区数多的 RDD，repartition 操作都可以完成，因为无论如何都会经 shuffle 过程。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
<p><font color="red">reduceByKey 和 groupByKey 的区别？</font></p>
<p>Shuffle角度：reduceByKey 和 groupByKey 都存在 shuffle 的操作，但是 reduceByKey 可以在 shuffle 前对分区内相同 key 的数据进行预聚合（combine）功能，这样会减少落盘的 数据量，而 groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能比较 高。</p>
<p> 功能的角度：reduceByKey 其实包含分组和聚合的功能。GroupByKey 只能分组，不能聚 合，所以在分组聚合的场合下，推荐使用 reduceByKey，如果仅仅是分组而不需要聚合。那 么还是只能使用 groupByKe</p>
<p><strong>控制操作(control operation)</strong></p>
<ul>
<li>cache():RDD[T]</li>
<li>persist():RDD[T]</li>
<li>persist(level:StorageLevel):RDD[T]</li>
</ul>
<p>Spark通过持久化操作将RDD持久化到不同层次的存储介质中，方便重复使用</p>
<ul>
<li>checkpoint</li>
</ul>
<p>checkpoint接口将RDD持久化到HDFS上，于persist(若也存在磁盘上)的区别是checkpoint会切断此RDD之前的依赖关系</p>
<p>checkpoint主要作用：Spark长时间运行，过长的依赖将会耗用大量系统资源，定期将RDD进行checkpoint操作，可有效地节省系统资源；过长的依赖关系会出现节点失败RDD容错重新计算的成本将会大大提升</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql">val rdd =sc.makeRDD(<span class="hljs-number">1</span> <span class="hljs-keyword">to</span> <span class="hljs-number">4</span>,<span class="hljs-number">1</span>)<br>val flatMapRDD= rdd.flatMap(x=&gt;Seq(x,x))<br>sc.setCheckpointDir(&quot;temp&quot;)<br>flatMapRDD.<span class="hljs-keyword">checkpoint</span>()<br>flatMapRDD.dependencies.head.rdd<br>flatMapRDD.collect()<br>flatMapRDD.dependencies.head.rdd<br></code></pre></td></tr></table></figure>



<p><strong>行动操作(action operation)</strong></p>
<p>Spark中触发action operation将会触发一次Spark调度并返回相应结果</p>
<table>
<thead>
<tr>
<th align="left">Action</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>reduce</strong>(<em>func</em>)</td>
<td align="left">聚集 RDD 中的所有元素，先聚合分区内数据，再聚合分区间数据</td>
</tr>
<tr>
<td align="left"><strong>collect</strong>()</td>
<td align="left">在驱动程序中，以数组 Array 的形式返回数据集的所有元素</td>
</tr>
<tr>
<td align="left"><strong>count</strong>()</td>
<td align="left">返回 RDD 中元素的个数</td>
</tr>
<tr>
<td align="left"><strong>first</strong>()</td>
<td align="left">返回 RDD 中的第一个元素</td>
</tr>
<tr>
<td align="left"><strong>take</strong>(<em>n</em>)</td>
<td align="left">返回一个由 RDD 的前 n 个元素组成的数组</td>
</tr>
<tr>
<td align="left"><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td>
<td align="left">返回该 RDD 排序后的前 n 个元素组成的数组</td>
</tr>
<tr>
<td align="left"><strong>saveAsTextFile</strong>(<em>path</em>)</td>
<td align="left">将数据保存到不同格式的文件中</td>
</tr>
<tr>
<td align="left"><strong>countByKey</strong>()</td>
<td align="left">统计每种 key 的个数</td>
</tr>
<tr>
<td align="left"><strong>foreach</strong>(<em>func</em>)</td>
<td align="left">分布式遍历 RDD 中的每一个元素，调用指定函数</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
<h4 id="Spark调度管理原理"><a href="#Spark调度管理原理" class="headerlink" title="Spark调度管理原理"></a>Spark调度管理原理</h4><p><strong>调度概述</strong></p>
<p>调度的前期是判断多个作业任务的依赖关系，这些作业任务之间可能存在因果的依赖关系，所以DAG有向无环图来表示这种关系</p>
<p>DAGScheduler负责将作业拆分为不同阶段的具有依赖关系的多批任务，简单理解为任务的逻辑调度</p>
<p>TaskScheduler负责每个具体任务的实际物理调度</p>
<blockquote>
<p>Task：单个分区数据集上最小处理流程单元</p>
<p>TaskSet：由一组关联的但相互间没有Shuffle依赖关系的任务所组成的任务集</p>
<p>Stage：一个任务集对应的调度阶段</p>
<p>Job：由一个RDD action生成的一个或多个调度阶段所组成的一次计算作业</p>
<p>Application：spark应用程序，由一到多个Job组成</p>
</blockquote>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h03z89yka2j30ds06zgm0.jpg"></p>
<p>在Spark运行时，这些计算操作是延迟执行的，并不是所有的RDD操作都会触发Spark向集群提交实际作业，只有需要返回数据或输出数据操作才会触发实际计算工作，其他变换操作知识生成对应的RDD关系链，用来记录依赖关系和所需执行的运算；另外DAGScheduler内部维度了各种“任务/调度阶段/作业”的状态和互相之间的映射关系表，用于在任务状态更新，集群状态更新等情况下，正确维护作业运行逻辑</p>
<p><strong>作业调度具体工作流程</strong></p>
<p>每个作业从提交到完成，要经历拆分成任务为最小单位，按一定逻辑依赖关系一次提交执行，并返回结果</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h03zhvod2yj30jb0a1gm9.jpg"></p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h03zi7zjjdj30i90bkgmn.jpg"></p>
<p><strong>调度任务拆分：</strong></p>
<p>当一个RDD操作触发计算，向DAGScheduler提交作业时，DAGScheduler需要从RDD依赖链末端的RDD触发，遍历整个RDD依赖链，划分调度阶段，并决定各个调度阶段间的依赖关系，调度阶段的划分是以ShuffleDependency为依据。</p>
<p><strong>调度阶段的提交：</strong></p>
<p>在划分调度阶段步骤中会得到一个或多个有依赖关系的调度阶段，其中直接触发作业的RDD关联的调度阶段叫做FinalStage，DAGScheduler进一步从FinalStage生成一个作业实例，两者关系存储在映射表，用于调度阶段全部完成时做一些处理如报告状态，清理作业相关数据等</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h03zoa22rzj30ne05eaau.jpg"></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h03znoilg5j30kd09k3zc.jpg"></p>
<p>当一个属于中间过程调度阶段的任务完成后，DAGScheduler会检查对应的调度阶段的所有任务是否都完成了，都完成了将再次扫描一次等待列表中的所有调度阶段的列表，检查是否还有任何依赖的调度阶段未完成，若没有，说明调度阶段处理就绪状态，可以再次尝试提交</p>
<p><strong>任务集的提交</strong></p>
<p>调度阶段的提交，最终会被转换成一个任务集的提交，DAGScheduler通过TaskScheduler接口提交任务集，这个任务集最终会触发TaskScheduler构建一个TaskSetManager的实例来管理这个任务集的生命周期，对于DAGScheduler来说提交调度阶段的工作到此完成，而TaskScheduler的具体实现会在得到资源时，通过TaskSetManager调度具体的任务到对应的Executor节点上进行运算</p>
<h4 id="Spark存储管理"><a href="#Spark存储管理" class="headerlink" title="Spark存储管理"></a>Spark存储管理</h4><p><strong>RDD持久化</strong></p>
<p>分区和数据块的关系：</p>
<p>在操作RDD时，这些操作都将施行在每一个分区上，可以说RDD上的运算都是基于分区的，但在存储管理模块，接触到的是数据块(block)，存储管理模块中对数据的存取都是以数据块为单位的，分区其实是一个逻辑上的概念，而数据块是物理上的数据实体</p>
<p>在Spark中，分区和数据块是一一对应的，存储模块只关心数据块，对于数据块和分区之间的映射是通过名称上的约定进行的，ID号+索引号作为块的名称就建立了分区和块的映射</p>
<p>在显示调用函数缓存RDD时，Spark内部就建立了RDD分区和数据块之间的映射，当读取缓存的RDD时，根据映射关系，就能从存储管理模块中获取到对应的数据块</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h03xgwdkm8j30hp06adg2.jpg"></p>
<p>内存缓存：</p>
<p>当Spark基于内存持久化缓存RDD时，RDD中每一个分区对应的数据块都是会被存储管理模块中的内存缓存(Memory Store所管理的)，内存缓存在内部维护了一个以数据块名称为Key，块内容为Value的哈希表</p>
<p><font color="red">当内存不是或达到设置的阈值时如何处理?</font></p>
<p>Spark通过spark.storage.memoryFration来配置(0.6)，JVM内存的60%可被内存缓存用来存储块内容，当占用&gt;60%，spark会丢一些数据块或将一些数据块存储到磁盘上来释放内存缓存空间，若丢掉的RDD 所依赖的父RDD是可被回溯并可用的，是不影响错误恢复机制的</p>
<p>磁盘缓存：</p>
<p>spark会将数据块放到磁盘目录下，通过spark.local.dir就配置了缓存在磁盘的目录</p>
<p>持久化选项：</p>
<p>当需要将RDD持久化，Spark可以调用persist()或cache()函数，对于RDD持久化，Spark提供了多种持久化选项</p>
<ul>
<li>MEMORY_ONLY：RDD以Java对象存储到到JVM内存heap中，超出内存缓存部分将不会缓存，下次需要重新计算</li>
<li>MEMORY_AND_DISK：和MEMORY_ONLY不同点是超出内存缓存部分将存在磁盘，需要将从磁盘读取</li>
<li>MEMORY_ONLY_SER：将序列化后的RDD存储到JVM中，占用空间更小，但读取时候需耗费更多CPU资源进行反序列化</li>
<li>MEMORY_AND_DISK_SER：和MEMORY_ONLY_SER不同是会把内存无法容下分区写入磁盘缓存</li>
<li>DISK_ONLY：将RDD的分区只缓存到磁盘中</li>
</ul>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h03y9juahsj30n80cnmz7.jpg"></p>
<p><strong>Shuffle数据持久化</strong></p>
<p>每一个Map任务会根据Reduce任务的数量创建出相应的桶，桶的数量是M*R。Map任务产生的结果会根据所设置的分区算法填充到每个桶中，当Reduce任务启动时，会根据自己任务ID和所依赖的Map任务的ID从远端或本地存储管理模块中取得相应的桶作为输入处理</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h03yah7hklj30ny092q3o.jpg"></p>
<p>和RDD持久化不同的是：首先Shuffle数据块必须在磁盘进行缓存，而不能选择在内存缓存，其次RDD基于磁盘持久化中，每个数据块对应一个文件，在Shuffle数据块持久化中，Shuffle数据块表示的只是逻辑概念，Shuffle有两种存储方式：</p>
<p>一种是将Shuffle数据块映射成文件，另一种是将Shuffle数据块映射成文件中的一段，这种方式要spark.shuffle.consolidateFiles设置为true，这种方式将分时运行的Map任务产生的Shuffle数据块合并到同一个文件中，来减少Shuffle文件的总数</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h03yisqxnfj30n109rgmk.jpg"></p>
<p><strong>广播(Broadcast)变量持久化</strong></p>
<p>为了加速一些对小块数据的读取，最好数据在所有节点都有一份拷贝，每个任务都能从本节点拷贝读取数据而不用通过远程传输获取数据，广播变量实现了这个功能，广播变量由存储管理模块进行管理的，另外广播变量数据块是以MEMORY_AND_DISK持久化存储本节点的存储管理模块，通过设置过期清洗机制，Spark内部会清理过期的广播变量</p>
<h4 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h4><p>用于快速构建可扩展，高吞吐，高容错的流处理程序</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h04s6ljwgfj30pl08pmxw.jpg"></p>
<p>DStream(离散数据流)代表了一个数据流，数据流可从外部输入源获得也可以通过输入流的转换获得，DStream是通过一组时间序列上连续的RDD来表示</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h04s9jnhcij30hv04zq32.jpg"></p>
<p><strong>DEMO</strong></p>
<p>获取指定端口上的数据并进行词频统计</p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">object</span> NetworkWordCount &#123;<br><br>  def main(args: Array<span class="hljs-literal">[S<span class="hljs-identifier">tring</span>]</span>) &#123;<br><br>    <span class="hljs-comment">/*指定时间间隔为 5s*/</span><br>    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-constructor">SparkConf()</span>.set<span class="hljs-constructor">AppName(<span class="hljs-string">&quot;NetworkWordCount&quot;</span>)</span>.set<span class="hljs-constructor">Master(<span class="hljs-string">&quot;local[2]&quot;</span>)</span><br>    <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-constructor">StreamingContext(<span class="hljs-params">sparkConf</span>, Seconds(5)</span>)<br><br>    <span class="hljs-comment">/*创建文本输入流,并进行词频统计*/</span><br>    <span class="hljs-keyword">val</span> lines = ssc.socket<span class="hljs-constructor">TextStream(<span class="hljs-string">&quot;hadoop001&quot;</span>, 9999)</span><br>    lines.flat<span class="hljs-constructor">Map(<span class="hljs-params">_</span>.<span class="hljs-params">split</span>(<span class="hljs-string">&quot; &quot;</span>)</span>).map(x =&gt; (x, <span class="hljs-number">1</span>)).reduce<span class="hljs-constructor">ByKey(<span class="hljs-params">_</span> + <span class="hljs-params">_</span>)</span>.print<span class="hljs-literal">()</span><br><br>    <span class="hljs-comment">/*启动服务*/</span><br>    ssc.start<span class="hljs-literal">()</span><br>    <span class="hljs-comment">/*等待服务结束*/</span><br>    ssc.await<span class="hljs-constructor">Termination()</span><br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p><code>nc -lk 9999</code>打开端口写入测试数据</p>
<p>Spark Streaming 编程的入口类是 StreamingContext，在创建时候需要指明 <code>sparkConf</code> 和 <code>batchDuration</code>(批次时间)，Spark 流处理本质是将流数据拆分为一个个批次，然后进行微批处理，<code>batchDuration</code> 就是批次拆分的时间间隔。这个时间可以根据业务需求和服务器性能进行指定，如果业务要求低延迟并且服务器性能也允许，则这个时间可以指定得很短。</p>
<p>DStream 是 Spark Streaming 提供的基本抽象。它表示连续的数据流。在内部，DStream 由一系列连续的 RDD 表示。所以从本质上而言，应用于 DStream 的任何操作都会转换为底层 RDD 上的操作。</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h04sd4t22lj30pe083mxw.jpg"></p>
<h4 id="Spark调优"><a href="#Spark调优" class="headerlink" title="Spark调优"></a>Spark调优</h4><p><strong>RDD调优</strong></p>
<ul>
<li>RDD复用</li>
<li>尽早进行filter</li>
<li>读取大量小文件用wholeTextFiles</li>
<li>mapPartition和foreachPartition替代map和foreach操作</li>
<li>filter+coalesce/repartition(减少分区)</li>
<li>设置合理的并行度</li>
<li>repartition/coalesce调节并行度</li>
<li>reduceByKey本地预聚合</li>
<li>使用持久化+checkpoint</li>
<li>使用广播变量使用Kryo序列化</li>
</ul>
<p><strong>Shuffle调优</strong></p>
<ul>
<li>map和reduce端缓冲区大小</li>
<li>reduce端重试次数和等待时间间隔</li>
<li>bypass机制开启阈值</li>
</ul>
]]></content>
      <tags>
        <tag>数据 Spark</tag>
      </tags>
  </entry>
</search>

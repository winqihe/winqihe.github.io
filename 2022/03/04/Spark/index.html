<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.0.0/css/all.min.css" integrity="sha256-jTIdiMuX/e3DGJUGwl3pKSxuc6YOuqtJYkM0bGQESA4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.10.1","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Spark大数据的计算分析引擎  官网 https:&#x2F;&#x2F;spark.apache.org&#x2F;   本文摘录于《Spark大数据处理技术》">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark">
<meta property="og:url" content="http://example.com/2022/03/04/Spark/index.html">
<meta property="og:site_name" content="WenQi&#96;s Blog">
<meta property="og:description" content="Spark大数据的计算分析引擎  官网 https:&#x2F;&#x2F;spark.apache.org&#x2F;   本文摘录于《Spark大数据处理技术》">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva4.sinaimg.cn/large/0082w7mmly8h02j0862b7j30ky04ijrl.jpg">
<meta property="og:image" content="https://tva2.sinaimg.cn/large/0082w7mmly8h02j2w0ltxj30mu092mxt.jpg">
<meta property="og:image" content="https://tva3.sinaimg.cn/large/0082w7mmly8h02j5gw5b9j30pg047t90.jpg">
<meta property="og:image" content="https://tva2.sinaimg.cn/large/0082w7mmly8h02jiomgszj30lh0ckdgp.jpg">
<meta property="og:image" content="https://tva4.sinaimg.cn/large/0082w7mmly8h02jtb7ww4j30ki05jt96.jpg">
<meta property="og:image" content="https://tva3.sinaimg.cn/large/0082w7mmly8h02kef8mpbj30hr0cmq3c.jpg">
<meta property="og:image" content="https://tva2.sinaimg.cn/large/0082w7mmly8h02kewzqs2j30ik07cweq.jpg">
<meta property="og:image" content="https://tva3.sinaimg.cn/large/0082w7mmly8h02nqgxyjwj3084071mx8.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082w7mmly8h02qwqpzbmj30lj0b1abb.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082w7mmly8h02qzccphpj30fr07cmxc.jpg">
<meta property="og:image" content="https://tva3.sinaimg.cn/large/0082w7mmly8h01kwjxgm6j30uw06nwfg.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082w7mmly8h01l88927fj30lt09mwey.jpg">
<meta property="article:published_time" content="2022-03-04T13:01:52.000Z">
<meta property="article:modified_time" content="2022-03-08T12:47:06.051Z">
<meta property="article:author" content="Keven He">
<meta property="article:tag" content="数据 Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva4.sinaimg.cn/large/0082w7mmly8h02j0862b7j30ky04ijrl.jpg">


<link rel="canonical" href="http://example.com/2022/03/04/Spark/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2022/03/04/Spark/","path":"2022/03/04/Spark/","title":"Spark"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Spark | WenQi`s Blog</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">WenQi`s Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">No thing is impossible!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首&emsp;&emsp;页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关&emsp;&emsp;于</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标&emsp;&emsp;签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分&emsp;&emsp;类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归&emsp;&emsp;档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜&emsp;&emsp;索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">Spark概述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark-RDD%E5%8F%8A%E7%BC%96%E7%A8%8B%E6%8E%A5%E5%8F%A3"><span class="nav-number">2.</span> <span class="nav-text">Spark RDD及编程接口</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%E5%8F%8A%E5%8E%9F%E7%90%86"><span class="nav-number">3.</span> <span class="nav-text">Spark运行模式及原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark%E8%B0%83%E5%BA%A6%E7%AE%A1%E7%90%86%E5%8E%9F%E7%90%86"><span class="nav-number">4.</span> <span class="nav-text">Spark调度管理原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86"><span class="nav-number">5.</span> <span class="nav-text">Spark存储管理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark-Streaming"><span class="nav-number">6.</span> <span class="nav-text">Spark Streaming</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">7.</span> <span class="nav-text"></span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Keven He"
      src="/images/avator.jpg">
  <p class="site-author-name" itemprop="name">Keven He</p>
  <div class="site-description" itemprop="description">心有猛虎，细嗅蔷薇。</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/yourname" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/weixin_45462732" title="Blog → https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_45462732" rel="noopener" target="_blank"><i class="fa fa-coffee fa-fw"></i>Blog</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/04/Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avator.jpg">
      <meta itemprop="name" content="Keven He">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WenQi`s Blog">
      <meta itemprop="description" content="心有猛虎，细嗅蔷薇。">
    </span>
    
    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Spark | WenQi`s Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-04 21:01:52" itemprop="dateCreated datePublished" datetime="2022-03-04T21:01:52+08:00">2022-03-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-03-08 20:47:06" itemprop="dateModified" datetime="2022-03-08T20:47:06+08:00">2022-03-08</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>14 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <blockquote>
<p>Spark大数据的计算分析引擎  官网 <a target="_blank" rel="noopener" href="https://spark.apache.org/">https://spark.apache.org/</a></p>
</blockquote>
<ul>
<li>本文摘录于《Spark大数据处理技术》</li>
</ul>
<span id="more"></span>

<h4 id="Spark概述"><a href="#Spark概述" class="headerlink" title="Spark概述"></a><strong>Spark概述</strong></h4><blockquote>
<p>Apache Spark is a unified analytics engine for large-scale data processing</p>
</blockquote>
<p>按照现在流行的大数据处理场景划分，可以将大数据处理分为三种情况：</p>
<ul>
<li>复杂的批量数据处理，通常时间跨度为数十分钟到数小时</li>
<li>基于历史数据的交互式查询，通常时间跨度为数十秒到数分钟</li>
<li>基于实时数据流的数据处理，通常时间跨度为数百毫秒到数秒</li>
</ul>
<p>在Spark Core基础上衍生出能同时处理上面三种情形的统一大数据处理平台</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h02j0862b7j30ky04ijrl.jpg"></p>
<p>Spark要做的是将批处理，交互式处理，流式处理融合到一个软件栈中</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h02j2w0ltxj30mu092mxt.jpg"></p>
<h4 id="Spark-RDD及编程接口"><a href="#Spark-RDD及编程接口" class="headerlink" title="Spark RDD及编程接口"></a>Spark RDD及编程接口</h4><p><strong>HelloWorld</strong></p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h02j5gw5b9j30pg047t90.jpg"></p>
<ul>
<li>对于Spark程序来讲，所有的操作的前提是要有一个Spark上下文，创建上下文过程中，程序会向集群申请资源并构建运行环境。</li>
<li>通过sc变量并利用textFile接口从HDFS文件系统读取文件，返回file变量</li>
<li>对file进行过滤操作，生成新的变量filterRDD</li>
<li>对filterRDD进行cache操作(方便重用filterRDD)</li>
<li>对filterRDD进行计数，返回结果</li>
</ul>
<p>这个程序中涉及到众多概念：</p>
<ul>
<li>弹性式分布式数据集 RDD</li>
<li>创建操作(creation operation)：RDD初始创建是SparkContext负责，将内存集合或外部文件系统作为输入源</li>
<li>转换操作(transformation operation)：将一个RDD通过一定操作转变为另一个RDD</li>
<li>控制操作(control operation)：对RDD进行持久化，让RDD保存磁盘或内存中，方便重复使用</li>
<li>行动操作(action operation)：Spark是惰性计算的，RDD所有行动操作，都会触发Spark作业运行</li>
</ul>
<p>RDD和操作之间关系：经过输入操作，转换操作，控制操作，输出操作来完成一个作业</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h02jiomgszj30lh0ckdgp.jpg"></p>
<p><strong>Spark RDD</strong></p>
<p>RDD是弹性分布式数据集，即一个RDD代表一个被分区的只读数据集，RDD有两种生成途径：来自内存集合和外部存储系统；通过RDD的转换操作</p>
<p>RDD继承关系(lineage)构建可以通过记录作用在RDD上的转换操作，可以有效进行容错处理</p>
<p>一般情况下抽象的RDD需要包含这五个接口</p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h02jtb7ww4j30ki05jt96.jpg"></p>
<p><strong>RDD分区(partitions)</strong></p>
<p>对于RDD来说，分区数涉及到RDD进行并行计算的粒度，每一个RDD分区的计算操作都在一个单独的任务中被执行。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(1 to 100,2)</span><br><span class="line">//1-100的数组转为RDD，第二个参数执行分区数</span><br><span class="line">rdd.partitions.size</span><br><span class="line">//查看RDD被划分的分区数</span><br></pre></td></tr></table></figure>

<p><strong>RDD优先位置(preferredLocations)</strong></p>
<p>RDD优先位置属性与Spark中的调度相关，返回的是RDD的每个partition存储的位置，“移动数据不如移动计算”所以Spark任务调度时候尽可能将任务分配到数据块存储位置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.textFile(&quot;hdfs://10.0.2.19:8000/bigfile&quot;)</span><br><span class="line">//读取bigfile生成rdd</span><br><span class="line">val hadoopRDD =rdd.dependencies(0).rdd</span><br><span class="line">//通过rdd依赖关系找到原始hadoopRDD</span><br><span class="line">hadoopRDD.partitions.size</span><br><span class="line">hadoopRDD.preferredLocations(hadoopRDD.partition(0))</span><br><span class="line">//返回partition(0)所在的机器位置</span><br></pre></td></tr></table></figure>

<p><strong>RDD依赖关系(dependencies)</strong></p>
<p>由于RDD是粗粒度操作数据集，每个转换操作都会生成一个新的RDD，所以RDD之间会形成类似流水线(pipline)的前后依赖关系，Spark中有两种类型依赖。</p>
<p>窄依赖(Narrow Dependencies)：每一个父RDD的分区最多只能被子RDD的一个分区使用</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h02kef8mpbj30hr0cmq3c.jpg"></p>
<p>宽依赖(Wide Dependencies)：多个子RDD的分区会依赖于同一个父RDD的分区</p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h02kewzqs2j30ik07cweq.jpg"></p>
<p>Spark中明确区分宽窄依赖原因？</p>
<ol>
<li>窄依赖可以在集群的一个节点上如流水线一样执行，可以计算所有父RDD分区，相反宽依赖需要取得父RDD的所有分区上的数据进行计算，将执行类似MapReduce一样的Shuffle操作</li>
<li>对于窄依赖，节点计算失败后恢复会更有效，只需重新计算对应父RDD的分区，而且可以在其他节点上并行的计算，相反在宽依赖依赖关系中，一个节点失败会导致其父RDD的多个分区重新计算</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd =sc.makeRDD(1 to 10)</span><br><span class="line">val mapRDD= rdd.map(x=&gt;(x,x))</span><br><span class="line">mapRDD.dependencies</span><br><span class="line">val shuffleRDD = mapRDD.partitionBy(new org.apache.spark.HashPartitioner(3))</span><br><span class="line">shuffleRDD.dependencies</span><br></pre></td></tr></table></figure>

<p><strong>RDD分区计算(compute)</strong></p>
<p>RDD的计算都是以partition为单位的，而且RDD中的compute函数都是在对迭代器进行复合，不需要保存每次计算结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(1 to 10,2)</span><br><span class="line">val map_rdd = rdd.map(a=&gt;a+1)</span><br><span class="line">val filter_rdd = map_rdd.filter(a=&gt;(a&gt;3))</span><br><span class="line">val context = new org.apache.spark.TaskContext(0,0,0)</span><br><span class="line">val iter0 = filter_rdd.compute(filter_rdd.partition(0),context)</span><br><span class="line">iter0.toList</span><br><span class="line">val iter1 = filter_rdd.compute(filter_rdd.partition(0),context)</span><br><span class="line">iter1.toList</span><br><span class="line">//在rdd上进行map和filter，由于compute函数只返回相应分区数据的迭代器，只有最后实例化才能显示出两个分区最终计算结果</span><br></pre></td></tr></table></figure>

<p><strong>RDD分区函数(partitioner)</strong></p>
<p>Spark目前有两种类型分区函数：HashPartitioner(哈希分区)和RangePartitioner(区域分区)，而且partitioner这个属性只存在于(K,V)类型的RDD中，对于非(K,V)类型的partitioner的值就是None，partitioner函数既决定了RDD本身的分区数量，也可作为其父RDD Shuffle输出(MapOutput)中每个分区进行数据切割的依据</p>
<p>使用HashPartitioner说明一下partitioner的功能</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val rdd =sc.makeRDD(1 to 10,2).map(x=&gt;(x,x))</span><br><span class="line">rdd.partitioner</span><br><span class="line">val group_rdd = rdd.groupByKey(new org.apache.spark.HashPartitioner(3))</span><br><span class="line">//new 了HashPartitioner对象</span><br><span class="line">group_rdd.partitioner</span><br><span class="line">group_rdd.collectPartitions()</span><br></pre></td></tr></table></figure>

<p>HashPartitioner的原理图：</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h02nqgxyjwj3084071mx8.jpg"></p>
<p><strong>集合创建操作</strong></p>
<p>RDD的创建可由内部集合类型来生成，Spark提供了parallelize和makeRDD两类函数实现从集合生成RDD，但makeRDD中提供了一个可以指定每一个分区preferredLocations参数的实现版本</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.makeRDD(1 to 10,3)</span><br><span class="line">rdd.collectPartitions()</span><br><span class="line">val collect = Seq(1 to 10,Seq(&quot;host1&quot;,&quot;host3&quot;)),(11 to 20, Seq(&quot;host2&quot;)))</span><br><span class="line">val rdd =sc.makeRDD(collect)</span><br><span class="line">rdd.preferreedLocations(rdd.partitions(0))</span><br><span class="line">rdd.preferredLocations(rdd.partitions(1))</span><br></pre></td></tr></table></figure>

<p><strong>存储创建操作</strong></p>
<p>主要是hadoopRDD和newHadoopRDD两个编程接口，包含了四个参数(输入格式；键类型；值类型；分区值)</p>
<p><strong>转换操作(transformation operation)</strong></p>
<table>
<thead>
<tr>
<th align="left">Transformation</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>map</strong>(<em>func</em>)</td>
<td align="left">将处理的数据逐条进行映射转换(类型或值)</td>
</tr>
<tr>
<td align="left"><strong>filter</strong>(<em>func</em>)</td>
<td align="left">将数据根据指定的规则进行筛选过滤</td>
</tr>
<tr>
<td align="left"><strong>flatMap</strong>(<em>func</em>)</td>
<td align="left">将处理的数据进行扁平化后再进行映射处理</td>
</tr>
<tr>
<td align="left"><strong>mapPartitions</strong>(<em>func</em>)</td>
<td align="left">将待处理的数据以分区为单位发送到计算节点进行处理</td>
</tr>
<tr>
<td align="left"><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td>
<td align="left">将待处理的数据以分区为单位发送到计算节点进行处理，在处理时同时可以获取当前分区索引。</td>
</tr>
<tr>
<td align="left"><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td>
<td align="left">根据指定的规则从数据集中抽取数据</td>
</tr>
<tr>
<td align="left"><strong>union</strong>(<em>otherDataset</em>)</td>
<td align="left">对源 RDD 和参数 RDD 求并集后返回一个新的 RDD</td>
</tr>
<tr>
<td align="left"><strong>intersection</strong>(<em>otherDataset</em>)</td>
<td align="left">对源 RDD 和参数 RDD 求交集后返回一个新的 RDD</td>
</tr>
<tr>
<td align="left"><strong>distinct</strong>([<em>numPartitions</em>]))</td>
<td align="left">将数据集中重复的数据去重</td>
</tr>
<tr>
<td align="left"><strong>groupByKey</strong>([<em>numPartitions</em>])</td>
<td align="left">将数据源的数据根据 key 对 value 进行分组</td>
</tr>
<tr>
<td align="left"><strong>reduceByKey</strong>(<em>func</em>, [<em>numPartitions</em>])</td>
<td align="left">可以将数据按照相同的 Key 对 Value 进行聚合</td>
</tr>
<tr>
<td align="left"><strong>aggregateByKey</strong>(<em>zeroValue</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numPartitions</em>])</td>
<td align="left">将数据根据不同的规则进行分区内计算和分区间计算</td>
</tr>
<tr>
<td align="left"><strong>sortByKey</strong>([<em>ascending</em>], [<em>numPartitions</em>])</td>
<td align="left">在一个(K,V)的 RDD 上调用，K 必须实现 Ordered 接口(特质)，返回一个按照 key 进行排序的RDD</td>
</tr>
<tr>
<td align="left"><strong>join</strong>(<em>otherDataset</em>, [<em>numPartitions</em>])</td>
<td align="left">在类型为(K,V)和(K,W)的 RDD 上调用，返回一个相同 key 对应的所有元素连接在一起的 (K,(V,W))的 RDD</td>
</tr>
<tr>
<td align="left"><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numPartitions</em>])</td>
<td align="left">在类型为(K,V)和(K,W)的 RDD 上调用，返回一个(K,(Iterable,Iterable))类型的 RDD</td>
</tr>
<tr>
<td align="left"><strong>coalesce</strong>(<em>numPartitions</em>)</td>
<td align="left">根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率 当 spark 程序中，存在过多的小任务的时候，可以通过 coalesce 方法，收缩合并分区，减少 分区的个数，减小任务调度成本</td>
</tr>
<tr>
<td align="left"><strong>repartition</strong>(<em>numPartitions</em>)</td>
<td align="left">该操作内部其实执行的是 coalesce 操作，参数 shuffle 的默认值为 true。无论是将分区数多的 RDD 转换为分区数少的 RDD，还是将分区数少的 RDD 转换为分区数多的 RDD，repartition 操作都可以完成，因为无论如何都会经 shuffle 过程。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
<p><font color="red">reduceByKey 和 groupByKey 的区别？</font></p>
<p>Shuffle角度：reduceByKey 和 groupByKey 都存在 shuffle 的操作，但是 reduceByKey 可以在 shuffle 前对分区内相同 key 的数据进行预聚合（combine）功能，这样会减少落盘的 数据量，而 groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能比较 高。</p>
<p> 功能的角度：reduceByKey 其实包含分组和聚合的功能。GroupByKey 只能分组，不能聚 合，所以在分组聚合的场合下，推荐使用 reduceByKey，如果仅仅是分组而不需要聚合。那 么还是只能使用 groupByKe</p>
<p><strong>控制操作(control operation)</strong></p>
<ul>
<li>cache():RDD[T]</li>
<li>persist():RDD[T]</li>
<li>persist(level:StorageLevel):RDD[T]</li>
</ul>
<p>Spark通过持久化操作将RDD持久化到不同层次的存储介质中，方便重复使用</p>
<ul>
<li>checkpoint</li>
</ul>
<p>checkpoint接口将RDD持久化到HDFS上，于persist(若也存在磁盘上)的区别是checkpoint会切断此RDD之前的依赖关系</p>
<p>checkpoint主要作用：Spark长时间运行，过长的依赖将会耗用大量系统资源，定期将RDD进行checkpoint操作，可有效地节省系统资源；过长的依赖关系会出现节点失败RDD容错重新计算的成本将会大大提升</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val rdd =sc.makeRDD(1 to 4,1)</span><br><span class="line">val flatMapRDD= rdd.flatMap(x=&gt;Seq(x,x))</span><br><span class="line">sc.setCheckpointDir(&quot;temp&quot;)</span><br><span class="line">flatMapRDD.checkpoint()</span><br><span class="line">flatMapRDD.dependencies.head.rdd</span><br><span class="line">flatMapRDD.collect()</span><br><span class="line">flatMapRDD.dependencies.head.rdd</span><br></pre></td></tr></table></figure>



<p><strong>行动操作(action operation)</strong></p>
<p>Spark中触发action operation将会触发一次Spark调度并返回相应结果</p>
<table>
<thead>
<tr>
<th align="left">Action</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>reduce</strong>(<em>func</em>)</td>
<td align="left">聚集 RDD 中的所有元素，先聚合分区内数据，再聚合分区间数据</td>
</tr>
<tr>
<td align="left"><strong>collect</strong>()</td>
<td align="left">在驱动程序中，以数组 Array 的形式返回数据集的所有元素</td>
</tr>
<tr>
<td align="left"><strong>count</strong>()</td>
<td align="left">返回 RDD 中元素的个数</td>
</tr>
<tr>
<td align="left"><strong>first</strong>()</td>
<td align="left">返回 RDD 中的第一个元素</td>
</tr>
<tr>
<td align="left"><strong>take</strong>(<em>n</em>)</td>
<td align="left">返回一个由 RDD 的前 n 个元素组成的数组</td>
</tr>
<tr>
<td align="left"><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td>
<td align="left">返回该 RDD 排序后的前 n 个元素组成的数组</td>
</tr>
<tr>
<td align="left"><strong>saveAsTextFile</strong>(<em>path</em>)</td>
<td align="left">将数据保存到不同格式的文件中</td>
</tr>
<tr>
<td align="left"><strong>countByKey</strong>()</td>
<td align="left">统计每种 key 的个数</td>
</tr>
<tr>
<td align="left"><strong>foreach</strong>(<em>func</em>)</td>
<td align="left">分布式遍历 RDD 中的每一个元素，调用指定函数</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
<h4 id="Spark运行模式及原理"><a href="#Spark运行模式及原理" class="headerlink" title="Spark运行模式及原理"></a>Spark运行模式及原理</h4><p>实际应用中，Spark应用程序的运行模式取决于SparkContext的MASTER环境变量的值</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h02qwqpzbmj30lj0b1abb.jpg"></p>
<p>其实这些模式从根本上都是将Spark应用分为任务调度和任务执行</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h02qzccphpj30fr07cmxc.jpg"></p>
<p><strong>Local模式</strong></p>
<p><strong>Standalone模式</strong></p>
<p><strong>Local Cluster模式</strong></p>
<p><strong>Mesos模式</strong></p>
<p><strong>YARN standalone/YARN cluster模式</strong></p>
<p><strong>YARN client模式</strong></p>
<p><strong>各种模式细节比较</strong></p>
<h4 id="Spark调度管理原理"><a href="#Spark调度管理原理" class="headerlink" title="Spark调度管理原理"></a>Spark调度管理原理</h4><h4 id="Spark存储管理"><a href="#Spark存储管理" class="headerlink" title="Spark存储管理"></a>Spark存储管理</h4><h4 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h4><h4 id><a href="#" class="headerlink" title></a></h4><p>RDD：弹性分布式数据集，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型；</p>
<p>DAG：有向无环图，反应RDD之间依赖关系</p>
<p>Executor：运行在Work Node上的进程，负责运行任务，并为应用程序存储数据</p>
<p>Application：用户编写的Spark程序</p>
<p>Task：运行在Executor上的工作单元</p>
<p>Job：一个Job包含多个RDD以及作用在RDD上的各种操作</p>
<p>Stage：作业的基本调度单位，一个作业会分为多组任务，每组任务叫做Stage</p>
<p>Spark运行场景：</p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h01kwjxgm6j30uw06nwfg.jpg"></p>
<p><strong>Spark运行架构</strong></p>
<p>采用主从架构，包含一个Master(Driver)和多个Worker</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h01l88927fj30lt09mwey.jpg"></p>
<p><strong>Spark部署</strong></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%95%B0%E6%8D%AE-Spark/" rel="tag"><i class="fa fa-tag"></i># 数据 Spark</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/03/03/Hadoop/" rel="prev" title="Hadoop">
                  <i class="fa fa-chevron-left"></i> Hadoop
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/03/05/Kafka/" rel="next" title="Kafka">
                  Kafka <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fab fa-microblog"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Keven He</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">24k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">54 分钟</span>
  </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
-->

<!--以下为添加的代码-->
      <!--统计start-->
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
        <div class="powered-by">
        <i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
          本站访客数:<span id="busuanzi_value_site_uv"></span>
        </span>
        </div>


    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  





</body>

</html>

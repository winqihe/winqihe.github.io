

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/avator.png">
  <link rel="icon" href="/img/avator.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Keven He">
  <meta name="keywords" content="大数据开发,Spark,Flink,Kafka">
  
    <meta name="description" content="大数据新闻热点项目">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据项目">
<meta property="og:url" content="http://example.com/2022/03/12/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A1%B9%E7%9B%AE/index.html">
<meta property="og:site_name" content="WenQi&#96;s Blog">
<meta property="og:description" content="大数据新闻热点项目">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva2.sinaimg.cn/large/0082w7mmly8h06thvxvapj30op0ee40r.jpg">
<meta property="og:image" content="https://tva3.sinaimg.cn/large/0082w7mmly8h06tiyis0nj30m20ck75k.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082w7mmly8h06tt0ow1oj30dp0bo3zb.jpg">
<meta property="og:image" content="https://tva4.sinaimg.cn/large/0082w7mmly8h06twirurij30au032t91.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082w7mmly8h06txd90zdj30py096dgy.jpg">
<meta property="og:image" content="https://tva2.sinaimg.cn/large/0082w7mmly8h06txpr4o5j30q10ftqds.jpg">
<meta property="og:image" content="https://tva4.sinaimg.cn/large/0082w7mmly8h06tyjius2j30qd07043b.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082w7mmly8h06tzghav5j30hp0azt99.jpg">
<meta property="og:image" content="https://tva4.sinaimg.cn/large/0082w7mmly8h06u1030pij30mb01xgmb.jpg">
<meta property="og:image" content="https://tva4.sinaimg.cn/large/0082w7mmly8h06u1grjbnj31cb0b2jut.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fynxzha2acj30hr053weh.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fyny4rdsrfj30pu0cjdh8.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fynycepyczj30xp0cl3zb.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fyososol82j306q02o0sk.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzd1atesvcj30dw0fmaae.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzd2e99ywhj30go0gp43u.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzd3g6rboxj30go0gp43u.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzd3eylp5zj30l008fab1.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzdb284hefj30he0chn95.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbmh1n31j309v042glj.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbmovok3j309n03sa9y.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbmw14tjj309o02cweb.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbszmkybj30rh0940ue.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbtek6eqj30rv0ar0ud.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzfpw9k0v7j30kv09rtfp.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzfqckmjxej30ej031q2s.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzfqeibkrtj306103ta9v.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzgupdmei1j30h5037mx4.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzgutrr5x7j30b0035glg.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzguxe0p4ej30nu0hl0ua.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzk8cc0l9rj30ev0ebmx9.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzk8i4ao6kj314r07j74m.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzk8rvrxu6j30n20e3q35.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzk9mr2i83j30kp0ddaa9.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzogese9lvj30on05o74m.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzohi45ckpj30o70auq3g.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzohlg58qyj31dz0b8tam.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzoi538xtkj31ck0bwjrz.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzokar9bjxj30pj0dc13u.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzol6uy6oaj30of09hjsa.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzom68klrwj315f0l23zq.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzom51hwo5j30pa0npaci.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzosp71irxj30g108hq75.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzot0y7ib8j30tl0a4mxy.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzr9u0l3szj31cl048js0.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005BOtkIly1fzr9tkp4szj30om07paas.jpg">
<meta property="article:published_time" content="2022-03-12T00:55:40.000Z">
<meta property="article:modified_time" content="2022-03-12T01:51:03.521Z">
<meta property="article:author" content="Keven He">
<meta property="article:tag" content="大数据项目">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://tva2.sinaimg.cn/large/0082w7mmly8h06thvxvapj30op0ee40r.jpg">
  
  
  <title>大数据项目 - WenQi`s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"Java"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>

  
<meta name="generator" content="Hexo 5.4.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>KevenHe</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archive
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Category
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tag
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/background.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="大数据项目"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
  </div>

  <div class="mt-1">
    

    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">大数据项目</h1>
            
            <div class="markdown-body">
              
              <blockquote>
<p>大数据新闻热点项目</p>
</blockquote>
<span id="more"></span>

<h3 id="项目需求设计与分析"><a href="#项目需求设计与分析" class="headerlink" title="项目需求设计与分析"></a>项目需求设计与分析</h3><p><strong>目标</strong></p>
<p>1、完成大数据项目的架构设计，安装部署，架构继承与开发、用户可视化交互设计</p>
<p>2、完成实时在线数据分析</p>
<p>3、完成离线数据分析</p>
<p><strong>具体功能</strong></p>
<p>1、捕获用户浏览日志信息</p>
<p>2、实时分析前20名流量最高的新闻话题</p>
<p>3、实时统计当前线上已曝光的新闻话题</p>
<p>4、统计哪个时段用户浏览量最高</p>
<p>5、报表</p>
<p><strong>项目技术栈</strong></p>
<p>Hadoop2.x、Zookeeper、Flume、Hive、Hbase、Kafka、Spark2.x、SparkStreaming、MySQL、Hue、J2EE、websoket、Echarts</p>
<p><strong>项目架构</strong></p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h06thvxvapj30op0ee40r.jpg" srcset="/img/loading.gif" lazyload></p>
<p><strong>集群资源规划</strong></p>
<p><img src="https://tva3.sinaimg.cn/large/0082w7mmly8h06tiyis0nj30m20ck75k.jpg" srcset="/img/loading.gif" lazyload></p>
<h3 id="环境准备与设置"><a href="#环境准备与设置" class="headerlink" title="环境准备与设置"></a>环境准备与设置</h3><p><strong>Linux重要配置</strong></p>
<p><strong>1）设置ip地址</strong> 项目视频里面直接使用界面修改ip比较方便，如果Linux没有安装操作界面，需要使用命令：vi /etc/sysconfig/network-scripts/ifcfg-eth0 来修改ip地址，然后重启网络服务service network restart即可。 参考链接：<a target="_blank" rel="noopener" href="https://www.willxu.xyz/2018/08/23/hadoop/1%E3%80%81vmware%E4%B8%8A%E7%BD%91%E9%85%8D%E7%BD%AE/">请点击。</a></p>
<p><strong>2）创建用户</strong> 大数据项目开发中，一般不直接使用root用户，需要我们创建新的用户来操作，比如kfk。 a）创建用户命令：adduser kfk b）设置用户密码命令：passwd kfk</p>
<p><strong>3）文件中设置主机名</strong> Linux系统的主机名默认是localhost，显然不方便后面集群的操作，我们需要手动修改Linux系统的主机名。 a）查看主机名命令：hostname b）修改主机名称 vi /etc/sysconfig/network NETWORKING=yes HOSTNAME=bigdata-pro01.kfk.com</p>
<p><strong>4）主机名映射</strong> 如果想通过主机名访问Linux系统，还需要配置主机名跟ip地址之间的映射关系。 vi /etc/hosts 192.168.31.151 bigdata-pro01.kfk.com 配置完成之后，reboot重启Linux系统即可。 如果需要在windows也能通过hostname访问Linux系统，也需要在windows下的hosts文件中配置主机名称与ip之间的映射关系。在windows系统下找到C:\WINDOWS\system32\drivers\etc\路径，打开HOSTS文件添加如下内容： 192.168.31.151 bigdata-pro01.kfk.com</p>
<p><strong>5）root用户下设置无密码用户切换</strong> 在Linux系统中操作是，kfk用户经常需要操作root用户权限下的文件，但是访问权限受限或者需要输入密码。修改/etc/sudoers这个文件添加如下代码，即可实现无密码用户切换操作。 vi /etc/sudoers 。。。添加如下内容即可 kfk ALL=(root)NOPASSWD:ALL</p>
<p><strong>6）关闭防火墙</strong> 我们都知道防火墙对我们的服务器是进行一种保护，但是有时候防火墙也会给我们带来很大的麻烦。 比如它会妨碍hadoop集群间的相互通信，所以我们需要关闭防火墙。 那么我们永久关闭防火墙的方法如下: vi /etc/sysconfig/selinux SELINUX=disabled 保存、重启后，验证机器的防火墙是否已经关闭。 a）查看防火墙状态：service iptables status b）打开防火墙：service iptables start c）关闭防火墙：service iptables stop</p>
<p><strong>7）卸载Linux本身自带的jdk</strong> 一般情况下jdk需要我们手动安装兼容的版本，此时Linux自带的jdk需要手动删除掉，具体操作如下所示： a）查看Linux自带的jdk rpm -qa|grep java b）删除Linux自带的jdk rpm -e –nodeps [jdk进程名称1 jdk进程名称2 …]</p>
<p><strong>克隆虚拟机并进行相关的配置</strong></p>
<p>前面我们已经做好了Linux的系统常规设置，接下来需要克隆虚拟机并进行相关的配置。 <strong>1）kfk用户下创建我们将要使用的各个目录</strong></p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs awk">软件目录<br>mkdir <span class="hljs-regexp">/opt/</span>softwares<br>模块目录<br>mkdir <span class="hljs-regexp">/opt/m</span>odules<br>工具目录<br>mkdir <span class="hljs-regexp">/opt/</span>tools<br>数据目录<br>mkdir <span class="hljs-regexp">/opt/</span>datas<br></code></pre></td></tr></table></figure>

<p><strong>2）jdk安装(1.7以上，1.9以下)</strong> 大数据平台运行环境依赖JVM，所以我们需要提前安装和配置好jdk。 前面我们已经安装了64位的centos系统，所以我们的jdk也需要安装64位的，与之相匹配 下面步骤给的是1.7的。我自己用的是jdk1.8.0_191 a）将jdk安装包通过工具上传到/opt/softwares目录下 b）解压jdk安装包</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment">#解压命令</span><br><span class="hljs-attribute">tar</span> -zxf jdk-<span class="hljs-number">7</span>u67-linux-x64.tar.gz /opt/modules/<br><span class="hljs-comment">#查看解压结果</span><br><span class="hljs-attribute">ls</span><br><span class="hljs-attribute">jdk1</span>.<span class="hljs-number">7</span>.<span class="hljs-number">0</span>_67<br></code></pre></td></tr></table></figure>

<p>c）配置Java 环境变量</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">vi /etc<span class="hljs-built_in">/profile</span><br><span class="hljs-built_in"></span><span class="hljs-built_in">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/opt/modules/jdk1.7.0_67<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-variable">$PATH</span>:$JAVA_HOME/bin<br></code></pre></td></tr></table></figure>

<p>d）查看Java是否安装成功</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">java </span>-version<br><span class="hljs-keyword">java </span>version <span class="hljs-string">&quot;1.7.0_67&quot;</span><br><span class="hljs-keyword">Java(TM) </span>SE Runtime Environment (<span class="hljs-keyword">build </span><span class="hljs-number">1</span>.<span class="hljs-number">7</span>.<span class="hljs-number">0</span>_67-<span class="hljs-keyword">b15)</span><br><span class="hljs-keyword"></span><span class="hljs-keyword">Java </span>HotSpot(TM) <span class="hljs-number">64</span>-<span class="hljs-keyword">Bit </span>Server VM (<span class="hljs-keyword">build </span><span class="hljs-number">24</span>.<span class="hljs-number">79</span>-<span class="hljs-keyword">b02, </span>mixed mode)<br></code></pre></td></tr></table></figure>

<p><strong>3）克隆虚拟机</strong></p>
<p>在克隆虚拟机之前，需要关闭虚拟机，然后右键选中虚拟机——》选择管理——》选择克隆——》选择下一步——》选择下一步——》选择创建完整克隆，下一步——》选择克隆虚拟机位置（提前创建好），修改虚拟机名称为Hadoop-Linux-pro-2，然后选择完成即可。 然后使用同样的方式创建第三个虚拟机Hadoop-Linux-pro-3。</p>
<p><strong>4）修改克隆虚拟机配置</strong> 克隆完虚拟机Hadoop-Linux-pro-2和Hadoop-Linux-pro-3之后，可以按照Hadoop-Linux-pro-1的方式配置好ip地址、hostname，以及ip地址与hostname之间的关系</p>
<h3 id="Hadoop2-X分布式集群部署"><a href="#Hadoop2-X分布式集群部署" class="headerlink" title="Hadoop2.X分布式集群部署"></a>Hadoop2.X分布式集群部署</h3><p><strong>配置要点</strong></p>
<p><strong>1）hadoop2.x版本下载及安装</strong> 官网下载2.x版本就好</p>
<p><strong>2）hadoop配置要点</strong> 参考官网给的例子：<a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html</a> 网站左下角有全部配置信息 <strong>1）hadoop2.x分布式集群配置-HDFS</strong><br>安装hdfs需要修改4个配置文件：hadoop-env.sh、core-site.xml、hdfs-site.xml和slaves <strong>2）hadoop2.x分布式集群配置-YARN</strong> 安装yarn需要修改4个配置文件：yarn-env.sh、mapred-env.sh、yarn-site.xml和mapred-site.xml</p>
<p><strong>3）分发配置到节点</strong> 最好先SCP设置成无密码访问，需要生成秘钥，自己百度吧 hadoop相关配置在第一个节点配置好之后，可以通过脚本命令分发给另外两个节点即可，具体操作如下所示。 将安装包分发给第二个节点 scp -r hadoop-2.5.0 <a href="mailto:kaf@bigdata-pro02.kfk.com">kaf@bigdata-pro02.kfk.com</a>:/opt/modules/ 将安装包分发给第三个节点 scp -r hadoop-2.5.0 <a href="mailto:kaf@bigdata-pro02.kfk.com">kaf@bigdata-pro02.kfk.com</a>:/opt/modules/</p>
<p><strong>4）HDFS启动集群运行测试</strong> hdfs相关配置好之后，可以启动hdfs集群。 1.格式化NameNode 通过命令：bin/hdfs namenode -format 格式化NameNode。 2.启动各个节点机器服务 1）启动NameNode命令：sbin/hadoop-daemon.sh start namenode 2) 启动DataNode命令：sbin/hadoop-daemon.sh start datanode 3）启动ResourceManager命令：sbin/yarn-daemon.sh start resourcemanager 4）启动NodeManager命令：sbin/yarn-daemon.sh start resourcemanager 5）启动log日志命令：sbin/mr-jobhistory-daemon.sh start historyserver</p>
<p><strong>5）YARN集群运行MapReduce程序测试</strong> 前面hdfs和yarn都启动起来之后，可以通过运行WordCount程序检测一下集群是否能run起来。 集群自带的WordCount程序执行命令：bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0.jar wordcount input output</p>
<p><strong>6）ssh无秘钥登录</strong> （可以提前设置好） 在集群搭建的过程中，需要不同节点分发文件，那么节点间分发文件每次都需要输入密码，比较麻烦。另外在hadoop 集群启动过程中，也需要使用批量脚本统一启动各个节点服务，此时也需要节点之间实现无秘钥登录。具体操作步骤如下所示： 1.主节点上创建 .ssh 目录，然后生成公钥文件id_rsa.pub和私钥文件id_rsa mkdir .ssh ssh-keygen -t rsa 2.拷贝公钥到各个机器 ssh-copy-id bigdata-pro1.kfk.com ssh-copy-id bigdata-pro2.kfk.com ssh-copy-id bigdata-pro3.kfk.com 3.测试ssh连接 ssh bigdata-pro1.kfk.com ssh bigdata-pro2.kfk.com ssh bigdata-pro3.kfk.com 4.测试hdfs ssh无秘钥登录做好之后，可以在主节点通过一键启动命令，启动hdfs各个节点的服务，具体操作如下所示： sbin/start-dfs.sh 如果yarn和hdfs主节点共用，配置一个节点即可。否则，yarn也需要单独配置ssh无秘钥登录。</p>
<p><strong>7）配置集群内机器时间同步（使用Linux ntp进行）</strong> 选择一台机器作为时间服务器，比如bigdata-pro1.kfk.com节点。 1.查看ntp服务是否已经存在 sudo rpm -qa|grep ntp 2.ntp服务相关操作 1）查看ntp状态 sudo service ntpd status 2）启动ntp sudo service ntpd start 3）关闭ntp sudo service ntpd stop 3.设置ntp随机器启动 sudo chkconfig ntpd on 4.修改ntp配置文件 vi /etc/ntp.conf 释放注释并将ip地址修改为 restrict 192.168.31.151 mask 255.255.255.0 nomodify notrap 注释掉以下命令行 server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst 释放以下命令行 server 127.127.1.0 #local clock fudge 127.127.1.0 stratum 10 重启ntp服务 sudo service ntpd restart 5.修改服务器时间</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment">#设置当前日期</span><br><span class="hljs-attribute">sudo</span> date -s <span class="hljs-number">2017</span>-<span class="hljs-number">06</span>-<span class="hljs-number">16</span><br><span class="hljs-comment">#设置当前时间</span><br><span class="hljs-attribute">sudo</span> date -s <span class="hljs-number">22</span>:<span class="hljs-number">06</span>:<span class="hljs-number">00</span><br></code></pre></td></tr></table></figure>

<p>6.其他节点手动同步主服务器时间</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment">#查看ntp位置</span><br>which ntpdate<br><span class="hljs-regexp">/usr/</span>sbin/ntpdate<br><span class="hljs-number">1</span>）手动同步bigdata-pro2.kfk.com节点时间<br>sudo <span class="hljs-regexp">/usr/</span>sbin/ntpdate bigdata-pro2.kfk.com<br><span class="hljs-number">2</span>）手动同步bigdata-pro3.kfk.com节点时间<br>sudo <span class="hljs-regexp">/usr/</span>sbin/ntpdate bigdata-pro3.kfk.com<br><span class="hljs-number">7</span>.其他节点定时同步主服务器时间<br>bigdata-pro2.kfk.com和bigdata-pro3.kfk.com节点分别切换到root用户， 通过crontab -e 命令，每<span class="hljs-number">10</span>分钟同步一次主服务器节点的时间。<br>crontab -e<br><span class="hljs-comment">#定时，每隔10分钟同步bigdata-pro1.kfk.com服务器时间</span><br><span class="hljs-number">0</span>-<span class="hljs-number">59</span><span class="hljs-regexp">/10 * * * *  /u</span>sr<span class="hljs-regexp">/sbin/</span>ntpdate bigdata-pro1.kfk.com<br></code></pre></td></tr></table></figure>

<h3 id="Zookeeper分布式集群部署"><a href="#Zookeeper分布式集群部署" class="headerlink" title="Zookeeper分布式集群部署"></a>Zookeeper分布式集群部署</h3><p><strong>Zookeeper部署步骤</strong></p>
<p><strong>1）下载Zookeeper</strong> 这里选择cdh版本的zookeeper-3.4.5-cdh5.10.0.tar.gz，将下载好的安装包上传至bigdata-pro01.kfk.com节点的/opt/softwares目录下。</p>
<p><strong>2）解压Zookeeper</strong> tar -zxf zookeeper-3.4.5-cdh5.10.0.tar.gz -C /opt/modules/ </p>
<p><strong>3）修改配置</strong> </p>
<ol>
<li>复制配置文件 cp conf/zoo_sample.cfg zoo.cfg</li>
<li>修改配置文件zoo.cfg</li>
</ol>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">vi</span> zoo.cfg<br><span class="hljs-comment">#这个时间是作为Zookeeper服务器之间或客户端与服务器之间维持心跳的时间间隔</span><br><span class="hljs-attribute">tickTime</span>=<span class="hljs-number">2000</span><br><span class="hljs-comment">#配置 Zookeeper 接受客户端初始化连接时最长能忍受多少个心跳时间间隔数。</span><br><span class="hljs-attribute">initLimit</span>=<span class="hljs-number">10</span><br><span class="hljs-comment">#Leader 与 Follower 之间发送消息，请求和应答时间长度</span><br><span class="hljs-attribute">syncLimit</span>=<span class="hljs-number">5</span><br><span class="hljs-comment">#数据目录需要提前创建</span><br><span class="hljs-attribute">dataDir</span>=/opt/modules/zookeeper-<span class="hljs-number">3</span>.<span class="hljs-number">4</span>.<span class="hljs-number">5</span>-cdh5.<span class="hljs-number">10</span>.<span class="hljs-number">0</span>/zkData<br><span class="hljs-comment">#访问端口号</span><br><span class="hljs-attribute">clientPort</span>=<span class="hljs-number">2181</span><br><span class="hljs-comment">#server.每个节点服务编号=服务器ip地址：集群通信端口：选举端口</span><br><span class="hljs-attribute">server</span>.<span class="hljs-number">1</span>=bigdata-pro01.kfk.com:<span class="hljs-number">2888</span>:<span class="hljs-number">3888</span><br><span class="hljs-attribute">server</span>.<span class="hljs-number">2</span>=bigdata-pro02.kfk.com:<span class="hljs-number">2888</span>:<span class="hljs-number">3888</span><br><span class="hljs-attribute">server</span>.<span class="hljs-number">3</span>=bigdata-pro03.kfk.com:<span class="hljs-number">2888</span>:<span class="hljs-number">3888</span><br></code></pre></td></tr></table></figure>

<p><strong>4）分发各个节点</strong> 将Zookeeper安装配置分发到其他两个节点，具体操作如下所示： scp -r zookeeper-3.4.5-cdh5.10.0/ bigdata-pro02.kfk.com:/opt/modules/ scp -r zookeeper-3.4.5-cdh5.10.0/ bigdata-pro03.kfk.com:/opt/modules/ </p>
<p><strong>5）创建相关目录和文件</strong> </p>
<ol>
<li>在3个节点上分别创建数据目录 mkdir /opt/modules/zookeeper-3.4.5-cdh5.10.0/zkData </li>
<li>在各个节点的数据存储目录下创建myid文件，并且编辑每个机器的myid内容为</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#切换到数据目录</span><br><span class="hljs-built_in">cd</span> /opt/modules/zookeeper-3.4.5-cdh5.10.0/zkData<br><span class="hljs-comment">#bigdata-pro01.kfk.com节点</span><br><span class="hljs-built_in">touch</span> myid<br>vi myid<br>1<br><span class="hljs-comment">#bigdata-pro02.kfk.com节点</span><br><span class="hljs-built_in">touch</span> myid<br>vi myid<br>2<br><span class="hljs-comment">#bigdata-pro03.kfk.com节点</span><br><span class="hljs-built_in">touch</span> myid<br>vi myid<br>3<br></code></pre></td></tr></table></figure>

<p><strong>6）启动Zookeeper服务</strong> </p>
<ol>
<li>各个节点使用如下命令启动Zookeeper服务 bin/zkServer.sh start </li>
<li>查看各个节点服务状态 bin/zkServer.sh status 不是follower </li>
<li>关闭各个节点服务 bin/zkServer.sh stop </li>
<li>查看Zookeeper目录树结构 bin/zkCli.sh</li>
</ol>
<h3 id="Hadoop高可用配置-HA"><a href="#Hadoop高可用配置-HA" class="headerlink" title="Hadoop高可用配置(HA)"></a>Hadoop高可用配置(HA)</h3><p><strong>HA原理</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h06tt0ow1oj30dp0bo3zb.jpg" srcset="/img/loading.gif" lazyload></p>
<p>当发生故障时，Active的 NN 挂掉后，Standby NN 会在它成为Active NN 前，读取所有的JN里面的修改日志，这样就能高可靠的保证与挂掉的NN的目录镜像树一致，然后无缝的接替它的职责，维护来自客户端请求，从而达到一个高可用的目的。</p>
<p><strong>HDFS-HA配置</strong></p>
<p>1）修改hdfs-site.xml配置文件 </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.nameservices<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>ns<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.namenodes.ns<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>nn1,nn2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.rpc-address.ns.nn1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com:8020<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.rpc-address.ns.nn2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>	  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro02.kfk.com:8020<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.http-address.ns.nn1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com:50070<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>       <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.http-address.ns.nn2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>       <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro02.kfk.com:50070<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>qjournal://bigdata-pro01.kfk.com:8485;bigdata-pro02.kfk.com:8485;bigdata-pro03.kfk.com:8485/ns<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>       <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>       <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/opt/modules/hadoop-2.6.0/data/jn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled.ns<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>sshfence<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/home/kfk/.ssh/id_rsa<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>3<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.permissions.enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>false<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure>

<p>2）修改core-site.xml配置文件</p>
<figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs dust"><span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://ns<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>kfk<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>	</span><br><span class="language-xml">	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/opt/modules/hadoop-2.6.0/data/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.name.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file://$</span><span class="hljs-template-variable">&#123;hadoop.tmp.dir&#125;</span><span class="language-xml">/dfs/name<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>ha.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com:2181,bigdata-pro02.kfk.com:2181,bigdata-pro03.kfk.com:2181<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span></span><br><span class="language-xml">	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></span><br><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span></span><br></code></pre></td></tr></table></figure>

<p>3）将修改的配置分发到其他节点</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs awk">scp hdfs-site.xml bigdata-pro02.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc<span class="hljs-regexp">/hadoop/</span><br>scp hdfs-site.xml bigdata-pro03.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc<span class="hljs-regexp">/hadoop/</span><br>scp core-site.xml bigdata-pro02.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc<span class="hljs-regexp">/hadoop/</span><br>scp core-site.xml bigdata-pro03.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc<span class="hljs-regexp">/hadoop/</span><br></code></pre></td></tr></table></figure>

<p><strong>HDFS-HA自动故障转移测试</strong></p>
<p>1）在所有节点启动zookeeper cd /opt/modules/zookeeper-3.4.5-cdh5.10.0/ sbin/zkServer.sh start bin/hdfs zkfc -formatZK （第一次使用zkfc需要格式化） </p>
<p>2）启动hdfs bin/hdfs namenode -format （第一次使用hdfs需要格式化，在namenode） sbin/start-dfs.sh （会在各个节点上启动namenode/datanode/journalnode） </p>
<p>3）在HA的namenode节点上启动zkfc线程（两个namenode都要启动） sbin/hadoop-daemon.sh start zkfc 查看两个namenode状态一个是active(先启动zkfc的)，一个是standy，查看网页。 <a target="_blank" rel="noopener" href="http://bigdata-pro01.kfk.com:50070/">http://bigdata-pro01.kfk.com:50070</a> <a target="_blank" rel="noopener" href="http://bigdata-pro02.kfk.com:50070/">http://bigdata-pro02.kfk.com:50070</a> </p>
<p>4）上传文件到hdfs bin/hdfs dfs -mkdir /usr bin/hdfs dfs -put /opt/modules/hadoop-2.6.0/etc/hadoop/hdfs-site.xml /usr 在网页中可以看到 </p>
<p>5）杀死active的namenode </p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h06twirurij30au032t91.jpg" srcset="/img/loading.gif" lazyload></p>
<p>6）再次查看namenode状态 应该完成了主备切换。原来的standy变成了active.</p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h06txd90zdj30py096dgy.jpg" srcset="/img/loading.gif" lazyload></p>
<h3 id="HDFS-HA所遇到的问题（看输出日志和查看日志）"><a href="#HDFS-HA所遇到的问题（看输出日志和查看日志）" class="headerlink" title="HDFS-HA所遇到的问题（看输出日志和查看日志）"></a>HDFS-HA所遇到的问题（看输出日志和查看日志）</h3><p><strong>1）输出提示：无法解析bigdata-pro03.kfk.com:2181</strong> 原因：因为我的core-site.xml配置文件写错了,参数一栏不能有换行，要不然读的不对的。</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-section">&lt;property&gt;</span><br>	<span class="hljs-section">&lt;name&gt;</span><span class="hljs-attribute">ha</span>.zookeeper.quorum&lt;/name&gt;<br>	<span class="hljs-section">&lt;value&gt;</span><span class="hljs-attribute">bigdata</span>-pro01.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro02.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro03.kfk.com:<span class="hljs-number">2181</span>&lt;/value&gt;<br><span class="hljs-section">&lt;/property&gt;</span><br></code></pre></td></tr></table></figure>

<p><strong>2） sbin/start-dfs.sh 启动不成功</strong> 因为这个启动需要配置ssh，所以 （1）在节点1上 ssh-keygen ssh-copy-id bigdata-pro1.kfk.com (包括自己的也要ssh) ssh-copy-id bigdata-pro2.kfk.com ssh-copy-id bigdata-pro3.kfk.com （2）测试ssh连接 ssh bigdata-pro1.kfk.com ssh bigdata-pro2.kfk.com ssh bigdata-pro3.kfk.com </p>
<p><strong>3） namenode准备切换失败</strong> bigdata-pro1.kfk.com可以竞选成active，但是杀掉bigdata-pro1.kfk.com，而bigdata-pro2.kfk.com不会竞选成active，仍然是standby。 查看bigdata-pro2.kfk.com日志： tail -10f hadoop-kfk-zkfc-bigdata-pro02.kfk.com.log </p>
<p><img src="https://tva2.sinaimg.cn/large/0082w7mmly8h06txpr4o5j30q10ftqds.jpg" srcset="/img/loading.gif" lazyload></p>
<p><strong>红线部分说明，在bigdata-pro2.kfk.com准备选举时，需要对pro1进行fence，但是失败了，原因是ssh失败，说明在节点2上没法ssh到节点1上，所以需要在节点2上进行ssh-keygen,然后拷贝到节点1，这样就解决了</strong></p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h06tyjius2j30qd07043b.jpg" srcset="/img/loading.gif" lazyload></p>
<p><strong>YARN-HA原理</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/0082w7mmly8h06tzghav5j30hp0azt99.jpg" srcset="/img/loading.gif" lazyload></p>
<p>ResourceManager HA 由一对Active，Standby结点构成，通过RMStateStore存储内部数据和主要应用的数据及标记。 目前支持的可替代的RMStateStore实现有：基于内存的MemoryRMStateStore，基于文件系统的FileSystemRMStateStore，及基于zookeeper的ZKRMStateStore。 ResourceManager HA的架构模式同NameNode HA的架构模式基本一致，数据共享由RMStateStore，而ZKFC成为 ResourceManager进程的一个服务，非独立存在。</p>
<p><strong>YARN-HA配置</strong></p>
<p><strong>1）修改yarn-site.xml配置文件</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>10000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>rs<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>rm1,rm2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro02.kfk.com<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com:2181,bigdata-pro02.kfk.com:2181,bigdata-pro03.kfk.com:2181<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>	<br><br>	<br><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure>

<p><strong>2）分发至其他节点</strong> </p>
<p>​    <code>scp yarn-site.xml bigdata-pro02.kfk.com:/opt/modules/hadoop-2.6.0/etc/hadoop/</code></p>
<p><code> scp yarn-site.xml bigdata-pro03.kfk.com:/opt/modules/hadoop-2.6.0/etc/hadoop/</code></p>
<p><strong>YARN-HA故障转移测试</strong></p>
<ol>
<li>在rm1节点上启动yarn服务 sbin/start-yarn.sh </li>
<li>在rm2节点上启动ResourceManager服务 sbin/yarn-daemon.sh start resourcemanager </li>
<li>查看yarn的web界面 <a target="_blank" rel="noopener" href="http://bigdata-pro01.kfk.com:8088/">http://bigdata-pro01.kfk.com:8088</a> <a target="_blank" rel="noopener" href="http://bigdata-pro02.kfk.com:8088/">http://bigdata-pro02.kfk.com:8088</a> </li>
<li>上传wordcount所需的文件到hdfs并执行MapReduce例子 bin/hdfs dfs -put data/wc /usr/kfk/data<br>bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /usr/kfk/data/wc /usr/kfk/data/wc.out </li>
<li>执行到一半的时候，kill掉rm1上的resourcemanager 任务会转移到rm2继续处理 这是bigdata-pro01.kfk.com输出的日志（额外打开一个bigdata-pro01.kfk.com进行kill）</li>
</ol>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h06u1030pij30mb01xgmb.jpg" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://tva4.sinaimg.cn/large/0082w7mmly8h06u1grjbnj31cb0b2jut.jpg" srcset="/img/loading.gif" lazyload></p>
<h3 id="HBase分布式部署"><a href="#HBase分布式部署" class="headerlink" title="HBase分布式部署"></a>HBase分布式部署</h3><p><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fynxzha2acj30hr053weh.jpg" srcset="/img/loading.gif" lazyload><br>1、解压安装到/opt/modules/<br>2、修改配置文件<br><strong>a.hbase-env.sh</strong><br>配置jdk<br>export JAVA_HOME=/opt/modules/jdk1.8.0_191<br>使用外部的Zookeeper<br>export HBASE_MANAGES_ZK=false<br><strong>b.hbase-site.xml</strong><br>这里采用hadoop高可用下的配置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.rootdir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://ns/hbase<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.cluster.distributed<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com,bigdata-pro02.kfk.com,bigdata-pro03.kfk.com<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure>
<p><strong>c.regionservers</strong><br>bigdata-pro01.kfk.com<br>bigdata-pro02.kfk.com<br>bigdata-pro03.kfk.com</p>
<p><strong>3、将hadoop中hdfs-site.xml和core-site.xml拷贝到hbase的conf下</strong><br>要不然会启动失败，具体日志如下：不认识ns，因为ns在hadoop中配置的<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fyny4rdsrfj30pu0cjdh8.jpg" srcset="/img/loading.gif" lazyload><br>4、将hbase配置分发到各个节点<br>scp -r hbase-1.0.0-cdh5.4.0 bigdata-pro02.kfk.com:/opt/modules/<br>scp -r hbase-1.0.0-cdh5.4.0 bigdata-pro03.kfk.com:/opt/modules/</p>
<p><strong>HBase启动与测试</strong></p>
<ol>
<li>先启动zookeeper<pre><code class="hljs">zkServer.sh start
</code></pre>
</li>
<li>启动高可用下的hdfs<pre><code class="hljs">sbin/start-dfs.sh （会在各个节点上启动namenode/datanode/journalnode）
</code></pre>
在HA的namenode节点上启动zkfc线程（两个namenode都要启动）<br>sbin/hadoop-daemon.sh start zkfc</li>
<li>启动hbase<br>bin/start-hbase.sh</li>
<li>查看HBase Web界面<br>bigdata-pro01.kfk.com:60010/</li>
<li>HBase的master高可用测试</li>
</ol>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">在bigdata-pro02.kfk.com上启动<span class="hljs-literal">master</span>,<br>./hbase-daemon.sh <span class="hljs-literal">start</span> <span class="hljs-keyword">master</span><br><span class="hljs-title">然后杀死bigdata-pro01</span>.kfk.com的Hmaster<br>zookeeper会自动切换<span class="hljs-literal">master</span><br></code></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fynycepyczj30xp0cl3zb.jpg" srcset="/img/loading.gif" lazyload></p>
<p><strong>HBase的shell测试</strong></p>
<p>1、启动shell<br>bin/hbase shell<br>2、创建表<br>create ‘weblogs’,’info’<br>3、列出表<br>list<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fyososol82j306q02o0sk.jpg" srcset="/img/loading.gif" lazyload></p>
<h3 id="Kafka分布式部署"><a href="#Kafka分布式部署" class="headerlink" title="Kafka分布式部署"></a>Kafka分布式部署</h3><p>1）解压<br>tar -zxf kafka_2.10-0.9.0.0.tgz  -C /opt/modules/<br>2）配置server.properties文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs propertis">#节点唯一标识<br>broker.id=1<br><br>listeners=PLAINTEXT://bigdata-pro01.kfk.com:9092<br>#默认端口号<br>port=9092<br>#主机名绑定<br>host.name=bigdata-pro01.kfk.com<br>#Kafka数据目录<br>log.dirs=/opt/modules/kafka_2.10-0.9.0.0/kafka-logs<br>#配置Zookeeper<br>zookeeper.connect=bigdata-pro01.kfk.com:2181,bigdata-pro02.kfk.com:2181,bigdata-pro03.kfk.com:2181<br></code></pre></td></tr></table></figure>
<p>3）配置zookeeper.properties文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs propertis">#Zookeeper的数据存储路径与Zookeeper集群配置保持一致<br>dataDir=/opt/modules/zookeeper-3.4.5-cdh5.10.0/zkData<br></code></pre></td></tr></table></figure>

<p>4）配置consumer.properties文件</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">#配置Zookeeper地址<br>zookeeper.connect=bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro03<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span><br></code></pre></td></tr></table></figure>
<p>5）配置producer.properties文件</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">#配置Kafka集群地址  ,分布在三台机器上<br>metadata<span class="hljs-selector-class">.broker</span>.list=bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">9092</span>,bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">9092</span>,bigdata-pro03<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">9092</span><br></code></pre></td></tr></table></figure>
<p>6）拷贝<br>scp -r kafka_2.10-0.9.0.0 bigdata-pro02.kfk.com:/opt/modules/<br>scp -r kafka_2.10-0.9.0.0 bigdata-pro03.kfk.com:/opt/modules/<br>7）修改另外两个节点的server.properties</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment">#bigdata-pro02.kfk.com节点</span><br><span class="hljs-attr">broker.id</span>=<span class="hljs-number">2</span><br><span class="hljs-attr">listeners</span>=PLAINTEXT://bigdata-pro02.kfk.com:<span class="hljs-number">9092</span><br><span class="hljs-attr">host.name</span>=bigdata-pro02.kfk.com<br><span class="hljs-comment">#bigdata-pro03.kfk.com节点</span><br><span class="hljs-attr">broker.id</span>=<span class="hljs-number">3</span><br><span class="hljs-attr">listeners</span>=PLAINTEXT://bigdata-pro03.kfk.com:<span class="hljs-number">9092</span><br><span class="hljs-attr">host.name</span>=bigdata-pro03.kfk.com<br></code></pre></td></tr></table></figure>

<p><strong>kafka测试</strong></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-number">1</span>、所有节点启动zk<br>bin/zkServer<span class="hljs-selector-class">.sh</span> start<br><span class="hljs-number">2</span>、各个节点启动Kafka集群<br>bin/kafka-server-start<span class="hljs-selector-class">.sh</span> config/server<span class="hljs-selector-class">.properties</span> &amp;<br><span class="hljs-number">3</span>、创建topic<br>bin/kafka-topics<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--zookeeper</span> localhost:<span class="hljs-number">2181</span> <span class="hljs-attr">--create</span> <span class="hljs-attr">--topic</span> test <span class="hljs-attr">--replication-factor</span> <span class="hljs-number">1</span> <span class="hljs-attr">--partitions</span> <span class="hljs-number">1</span><br><span class="hljs-number">4</span>、查看topic<br>bin/kafka-topics<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--zookeeper</span> localhost:<span class="hljs-number">2181</span> –list<br><br>bin/kafka-topics<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--describe</span> <span class="hljs-attr">--zookeeper</span> localhost:<span class="hljs-number">2181</span> <span class="hljs-attr">--topic</span> test<br>结果：<br>        Topic:test      PartitionCount:<span class="hljs-number">1</span>        ReplicationFactor:<span class="hljs-number">1</span>     Configs:<br>        Topic: test     Partition: <span class="hljs-number">0</span>    Leader: <span class="hljs-number">2</span>       Replicas: <span class="hljs-number">2</span>     Isr: <span class="hljs-number">2</span><br><span class="hljs-number">5</span>、生产者生产数据（节点<span class="hljs-number">1</span>）<br>bin/kafka-console-producer<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--broker-list</span> bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">9092</span> <span class="hljs-attr">--topic</span> test<br><span class="hljs-number">6</span>、消费者消费数据（节点<span class="hljs-number">2</span>）<br>bin/kafka-console-consumer<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--zookeeper</span> bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span> <span class="hljs-attr">--topic</span> test <span class="hljs-attr">--from-beginning</span><br></code></pre></td></tr></table></figure>
<p>说下分区和消费关系：<br>一个主题可以有多个分区，具体分区方法有多种；关于消费，有消费组的概念。一种是指定消费组（每个消费者的组名一致），那么每个分区对应一个消费者；二是指定消费组（每个消费者的组名不一致），那么所有分区每个消息都会送至各个小组的消费者；三是不指定消费组，那么每条消息会发给消费组中一个消费者。</p>
<h3 id="Flume搭建部署"><a href="#Flume搭建部署" class="headerlink" title="Flume搭建部署"></a>Flume搭建部署</h3><p><strong>（先部署和设置了节点2和3采集部分，节点1的汇总分发后面继续）</strong><br>每一步都可以去查官方资料：官方地址：<a target="_blank" rel="noopener" href="http://flume.apache.org/">http://flume.apache.org/</a></p>
<p>1、解压Flume<br>tar -zxf apache-flume-1.7.0-bin.tar.gz  -C /opt/modules/</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros">vi flume-env.sh<br>配置下环境变量问题<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/opt/modules/jdk1.8.0_191<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">HADOOP_HOME</span>=/opt/modules/hadoop-2.6.0<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">HBASE_HOME</span>=/opt/modules/hbase-1.0.0-cdh5.4.0<br></code></pre></td></tr></table></figure>
<p>2、将flume分发到其他两个节点<br>scp -r flume-1.7.0-bin bigdata-pro02.kfk.com:/opt/modules/<br>scp -r flume-1.7.0-bin bigdata-pro03.kfk.com:/opt/modules/<br>3、flume agent-2采集节点服务配置（在bigdata-pro02.kfk.com）<br>三个部分：sources、channels、sinks<br>/opt/datas/weblogs.log是我们要采集的日志</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs stylus">vi flume-conf<span class="hljs-selector-class">.properties</span><br><br>agent2<span class="hljs-selector-class">.sources</span> = r1<br>agent2<span class="hljs-selector-class">.channels</span> = c1<br>agent2<span class="hljs-selector-class">.sinks</span> = k1<br><br>agent2<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.type</span> = exec<br>agent2<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.command</span> = tail -F /opt/datas/weblog-flume<span class="hljs-selector-class">.log</span><br>agent2<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.channels</span> = c1<br><br>agent2<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.type</span> = memory<br>agent2<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.capacity</span> = <span class="hljs-number">10000</span><br>agent2<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.transactionCapacity</span> = <span class="hljs-number">10000</span><br>agent2<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.keep-alive</span> = <span class="hljs-number">5</span><br><br>agent2<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.type</span> = avro<br>agent2<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.channel</span> = c1<br>agent2<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.hostname</span> = bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span><br>agent2<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.port</span> = <span class="hljs-number">5555</span><br></code></pre></td></tr></table></figure>
<p>4、flume agent-3采集节点服务配置（在bigdata-pro03.kfk.com）</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs stylus">vi flume-conf<span class="hljs-selector-class">.properties</span><br><br>agent3<span class="hljs-selector-class">.sources</span> = r1<br>agent3<span class="hljs-selector-class">.channels</span> = c1<br>agent3<span class="hljs-selector-class">.sinks</span> = k1<br><br>agent3<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.type</span> = exec<br>agent3<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.command</span> = tail -F /opt/datas/weblog-flume<span class="hljs-selector-class">.log</span><br>agent3<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.r1</span><span class="hljs-selector-class">.channels</span> = c1<br><br>agent3<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.type</span> = memory<br>agent3<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.capacity</span> = <span class="hljs-number">10000</span><br>agent3<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.transactionCapacity</span> = <span class="hljs-number">10000</span><br>agent3<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.keep-alive</span> = <span class="hljs-number">5</span><br><br>agent3<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.type</span> = avro<br>agent3<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.channel</span> = c1<br>agent3<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.hostname</span> = bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span><br>agent3<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.port</span> = <span class="hljs-number">5555</span><br></code></pre></td></tr></table></figure>

<h3 id="Flume源码修改与HBase-Kafka集成"><a href="#Flume源码修改与HBase-Kafka集成" class="headerlink" title="Flume源码修改与HBase+Kafka集成"></a>Flume源码修改与HBase+Kafka集成</h3><p><strong>如何修改flume源码？</strong></p>
<p>因为我们需要在节点1上将flume同时发送至Hbase以及kafka，但是hbase结构需要自定义，所以由flume发送至hbase代码需要进行修改。<br>步骤：<br>1.下载Flume源码并导入Idea开发工具<br>1）将apache-flume-1.7.0-src.tar.gz源码下载到本地解压<br>2）通过idea导入flume源码<br>打开idea开发工具，选择File——》Open，找到源码包，选中flume-ng-hbase-sink，点击ok加载相应模块的源码。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzd1atesvcj30dw0fmaae.jpg" srcset="/img/loading.gif" lazyload><br>2、自己写个类完成类的修改。KfkAsyncHbaseEventSerializer这个是我自定义的。修改其中的下面这个方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> List&lt;PutRequest&gt; <span class="hljs-title function_">getActions</span><span class="hljs-params">()</span> &#123;<br>        List&lt;PutRequest&gt; actions = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br>        <span class="hljs-keyword">if</span> (payloadColumn != <span class="hljs-literal">null</span>) &#123;<br>            <span class="hljs-type">byte</span>[] rowKey;<br>            <span class="hljs-keyword">try</span> &#123;<br>                <span class="hljs-comment">/*---------------------------代码修改开始---------------------------------*/</span><br>                <span class="hljs-comment">//解析列字段</span><br>                String[] columns = <span class="hljs-keyword">new</span> <span class="hljs-title class_">String</span>(<span class="hljs-built_in">this</span>.payloadColumn).split(<span class="hljs-string">&quot;,&quot;</span>);<br>                <span class="hljs-comment">//解析flume采集过来的每行的值</span><br>                String[] values = <span class="hljs-keyword">new</span> <span class="hljs-title class_">String</span>(<span class="hljs-built_in">this</span>.payload).split(<span class="hljs-string">&quot;,&quot;</span>);<br>                <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i &lt; columns.length;i++) &#123;<br>                    <span class="hljs-type">byte</span>[] colColumn = columns[i].getBytes();<br>                    <span class="hljs-type">byte</span>[] colValue = values[i].getBytes(Charsets.UTF_8);<br><br>                    <span class="hljs-comment">//数据校验：字段和值是否对应</span><br>                    <span class="hljs-keyword">if</span> (colColumn.length != colValue.length) <span class="hljs-keyword">break</span>;<br><br>                    <span class="hljs-comment">//时间</span><br>                    <span class="hljs-type">String</span> <span class="hljs-variable">datetime</span> <span class="hljs-operator">=</span> values[<span class="hljs-number">0</span>].toString();<br>                    <span class="hljs-comment">//用户id</span><br>                    <span class="hljs-type">String</span> <span class="hljs-variable">userid</span> <span class="hljs-operator">=</span> values[<span class="hljs-number">1</span>].toString();<br>                    <span class="hljs-comment">//根据业务自定义Rowkey</span><br>                    rowKey = SimpleRowKeyGenerator.getKfkRowKey(userid, datetime);<br>                    <span class="hljs-comment">//插入数据</span><br>                    <span class="hljs-type">PutRequest</span> <span class="hljs-variable">putRequest</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">PutRequest</span>(table, rowKey, cf,<br>                            colColumn, colValue);<br>                    actions.add(putRequest);<br>                    <span class="hljs-comment">/*---------------------------代码修改结束---------------------------------*/</span><br>                &#125;<br>            &#125; <span class="hljs-keyword">catch</span> (Exception e) &#123;<br>                <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">FlumeException</span>(<span class="hljs-string">&quot;Could not get row key!&quot;</span>, e);<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> actions;<br>    &#125;<br></code></pre></td></tr></table></figure>
<p>修改这个类中自定义KEY生成方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleRowKeyGenerator</span> &#123;<br><br>  <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-type">byte</span>[] getKfkRowKey(String userid,String datetime)<span class="hljs-keyword">throws</span> UnsupportedEncodingException &#123;<br>    <span class="hljs-keyword">return</span> (userid + datetime + String.valueOf(System.currentTimeMillis())).getBytes(<span class="hljs-string">&quot;UTF8&quot;</span>);<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<p>3、应该进行测试，但是这边测试完成，目前不知如何搭建，就直接生成jar包放到虚拟机直接用了。<br>4、生成jar包，idea很好用<br>可参考：<a target="_blank" rel="noopener" href="https://jingyan.baidu.com/article/c275f6ba0bbb65e33d7567cb.html">https://jingyan.baidu.com/article/c275f6ba0bbb65e33d7567cb.html</a><br>1）在idea工具中，选择File——》ProjectStructrue<br>2）左侧选中Artifacts，然后点击右侧的+号，最后选择JAR——》From modules with dependencies<br>3）一定要设置main class这一项选择自己要打包的类，然后直接点击ok<br>4）删除其他依赖包，只把flume-ng-hbase-sink打成jar包就可以了。<br>5）然后依次点击apply，ok<br>6）点击build进行编译，会自动打成jar包<br>7）到项目的apache-flume-1.7.0-src\flume-ng-sinks\flume-ng-hbase-sink\classes\artifacts\flume_ng_hbase_sink_jar目录下找到刚刚打的jar包<br>8）将打包名字替换为flume自带的包名flume-ng-hbase-sink-1.7.0.jar ，然后上传至虚拟机上flume/lib目录下，覆盖原有的jar包即可。</p>
<p><strong>修改flume配置</strong></p>
<p>这里在节点1上修改flume的配置，完成与hbase和kafka的集成。（flume自定义的jar已经上传覆盖）<br>修改flume-conf.properties</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">agent1.sources</span> = r1<br><span class="hljs-attr">agent1.channels</span> = kafkaC hbaseC <br><span class="hljs-attr">agent1.sinks</span> =  kafkaSink hbaseSink<br><br><span class="hljs-attr">agent1.sources.r1.type</span> = avro<br><span class="hljs-attr">agent1.sources.r1.channels</span> = hbaseC kafkaC<br><span class="hljs-attr">agent1.sources.r1.bind</span> = bigdata-pro01.kfk.com<br><span class="hljs-attr">agent1.sources.r1.port</span> = <span class="hljs-number">5555</span><br><span class="hljs-attr">agent1.sources.r1.threads</span> = <span class="hljs-number">5</span><br><span class="hljs-comment"># flume-hbase</span><br><span class="hljs-attr">agent1.channels.hbaseC.type</span> = memory<br><span class="hljs-attr">agent1.channels.hbaseC.capacity</span> = <span class="hljs-number">100000</span><br><span class="hljs-attr">agent1.channels.hbaseC.transactionCapacity</span> = <span class="hljs-number">100000</span><br><span class="hljs-attr">agent1.channels.hbaseC.keep-alive</span> = <span class="hljs-number">20</span><br><br><span class="hljs-attr">agent1.sinks.hbaseSink.type</span> = asynchbase<br><span class="hljs-attr">agent1.sinks.hbaseSink.table</span> = weblogs<br><span class="hljs-attr">agent1.sinks.hbaseSink.columnFamily</span> = info<br><span class="hljs-attr">agent1.sinks.hbaseSink.channel</span> = hbaseC<br><span class="hljs-attr">agent1.sinks.hbaseSink.serializer</span> = org.apache.flume.sink.hbase.KfkAsyncHbaseEventSerializer<br><span class="hljs-attr">agent1.sinks.hbaseSink.serializer.payloadColumn</span> = datatime,userid,searchname,retorder,cliorder,cliurl<br><span class="hljs-comment">#flume-kafka</span><br><span class="hljs-attr">agent1.channels.kafkaC.type</span> = memory<br><span class="hljs-attr">agent1.channels.kafkaC.capacity</span> = <span class="hljs-number">100000</span><br><span class="hljs-attr">agent1.channels.kafkaC.transactionCapacity</span> = <span class="hljs-number">100000</span><br><span class="hljs-attr">agent1.channels.kafkaC.keep-alive</span> = <span class="hljs-number">20</span><br><br><span class="hljs-attr">agent1.sinks.kafkaSink.channel</span> = kafkaC<br><span class="hljs-attr">agent1.sinks.kafkaSink.type</span> = org.apache.flume.sink.kafka.KafkaSink<br><span class="hljs-attr">agent1.sinks.kafkaSink.brokerList</span> = bigdata-pro01.kfk.com:<span class="hljs-number">9092</span>,bigdata-pro02.kfk.com:<span class="hljs-number">9092</span>,bigdata-pro03.kfk.com:<span class="hljs-number">9092</span><br><span class="hljs-attr">agent1.sinks.kafkaSink.topic</span> = weblogs<br><span class="hljs-attr">agent1.sinks.kafkaSink.zookeeperConnect</span> = bigdata-pro01.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro02.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro03.kfk.com:<span class="hljs-number">2181</span><br><span class="hljs-attr">agent1.sinks.kafkaSink.requiredAcks</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">agent1.sinks.kafkaSink.batchSize</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">agent1.sinks.kafkaSink.serializer.class</span> = kafka.serializer.StringEncoder<br></code></pre></td></tr></table></figure>

<p><strong>小结</strong></p>
<p>项目进行到这里，已经完成了节点2和节点3上flume采集配置、节点1上flume采集并发送至kafka和hbase配置。<br>如下图，这部分都已经完成，下一章进行联调。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzd2e99ywhj30go0gp43u.jpg" srcset="/img/loading.gif" lazyload></p>
<h3 id="Flume-HBase-Kafka集成全流程测试"><a href="#Flume-HBase-Kafka集成全流程测试" class="headerlink" title="Flume+HBase+Kafka集成全流程测试"></a>Flume+HBase+Kafka集成全流程测试</h3><p><strong>全流程测试简介</strong></p>
<p>将完成对前面所有的设计进行测试，核心是进行flume日志的采集、汇总以及发送至kafka消费、hbase保存。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzd3g6rboxj30go0gp43u.jpg" srcset="/img/loading.gif" lazyload></p>
<p><strong>原始日志数据简单处理</strong></p>
<p>1、下载搜狗实验室数据<br><a target="_blank" rel="noopener" href="http://www.sogou.com/labs/resource/q.php">http://www.sogou.com/labs/resource/q.php</a><br>2、格式说明<br>数据格式为:访问时间\t用户ID\t[查询词]\t该URL在返回结果中的排名\t用户点击的顺序号\t用户点击的URL<br>其中，用户ID是根据用户使用浏览器访问搜索引擎时的Cookie信息自动赋值，即同一次使用浏览器输入的不同查询对应同一个用户ID<br>3、日志简单处理<br>1）将文件中的tab更换成逗号<br>cat weblog.log|tr “\t” “,” &gt; weblog2.log<br>2）将文件中的空格更换成逗号<br>cat weblog2.log|tr “ “ “,” &gt; weblog3.log<br>处理完：<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzd3eylp5zj30l008fab1.jpg" srcset="/img/loading.gif" lazyload></p>
<p><strong>编写模拟日志生成过程</strong></p>
<p>1、代码实现<br>    实现功能是将原始日志，每次读取一行不断写入到另一个文件中（weblog-flume.log），所以这个文件就相等于服务器中日志不断增加的过程。编写完程序，将该项目打成weblogs.jar包，然后上传至bigdata-pro02.kfk.com节点和bigdata-pro03.kfk.com节点的/opt/jars目录下（目录需要提前创建）<br>2、编写运行模拟日志程序的shell脚本</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-number">1</span>）<br>在bigdata-pro02.kfk.<span class="hljs-keyword">com</span>节点的/<span class="hljs-keyword">opt</span>/datas目录下，创建weblog-<span class="hljs-keyword">shell</span>.<span class="hljs-keyword">sh</span>脚本。<br><span class="hljs-keyword">vi</span> weblog-<span class="hljs-keyword">shell</span>.<span class="hljs-keyword">sh</span><br>#/bin/bash<br><span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;start log......&quot;</span><br>#第一个参数是原日志文件，第二个参数是日志生成输出文件<br>java -jar /<span class="hljs-keyword">opt</span>/jars/weblogs.jar /<span class="hljs-keyword">opt</span>/datas/weblog.<span class="hljs-built_in">log</span> /<span class="hljs-keyword">opt</span>/datas/weblog-flume.<span class="hljs-built_in">log</span><br><br>修改weblog-<span class="hljs-keyword">shell</span>.<span class="hljs-keyword">sh</span>可执行权限<br>chmod <span class="hljs-number">777</span> weblog-<span class="hljs-keyword">shell</span>.<span class="hljs-keyword">sh</span><br><span class="hljs-number">2</span>）<br>将bigdata-pro02.kfk.<span class="hljs-keyword">com</span>节点上的/<span class="hljs-keyword">opt</span>/datas/目录拷贝到bigdata-pro03节点.kfk.<span class="hljs-keyword">com</span><br>scp -r /<span class="hljs-keyword">opt</span>/datas/ bigdata-pro03.kfk.<span class="hljs-keyword">com</span>:/<span class="hljs-keyword">opt</span>/datas/<br></code></pre></td></tr></table></figure>
<p>3、运行测试<br>/opt/datas/weblog-shell.sh<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdb284hefj30he0chn95.jpg" srcset="/img/loading.gif" lazyload></p>
<p><strong>编写一些shell脚本便于执行</strong></p>
<p>1、编写启动flume服务程序的shell脚本</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-number">1</span>.在bigdata-pro02.kfk.<span class="hljs-keyword">com</span>节点的flume安装目录下编写flume启动脚本。<br><span class="hljs-keyword">vi</span> flume-kfk-start.<span class="hljs-keyword">sh</span><br>#/bin/bash<br><span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;flume-2 start ......&quot;</span><br>bin/flume-ng agent --<span class="hljs-keyword">conf</span> <span class="hljs-keyword">conf</span> -<span class="hljs-keyword">f</span> <span class="hljs-keyword">conf</span>/flume-<span class="hljs-keyword">conf</span>.properties -n agent2 -Dflume.root.logger=INFO,console<br><span class="hljs-number">2</span>.在bigdata-pro03.kfk.<span class="hljs-keyword">com</span>节点的flume安装目录下编写flume启动脚本。<br><span class="hljs-keyword">vi</span> flume-kfk-start.<span class="hljs-keyword">sh</span><br>#/bin/bash<br><span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;flume-3 start ......&quot;</span><br>bin/flume-ng agent --<span class="hljs-keyword">conf</span> <span class="hljs-keyword">conf</span> -<span class="hljs-keyword">f</span> <span class="hljs-keyword">conf</span>/flume-<span class="hljs-keyword">conf</span>.properties -n agent3 -Dflume.root.logger=INFO,console<br><span class="hljs-number">3</span>.在bigdata-pro01.kfk.<span class="hljs-keyword">com</span>节点的flume安装目录下编写flume启动脚本。<br><span class="hljs-keyword">vi</span> flume-kfk-start.<span class="hljs-keyword">sh</span><br>#/bin/bash<br><span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;flume-1 start ......&quot;</span><br>bin/flume-ng agent --<span class="hljs-keyword">conf</span> <span class="hljs-keyword">conf</span> -<span class="hljs-keyword">f</span> <span class="hljs-keyword">conf</span>/flume-<span class="hljs-keyword">conf</span>.properties -n agent1 -Dflume.root.logger=INFO,console<br><br></code></pre></td></tr></table></figure>
<p>2、编写Kafka Consumer执行脚本</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-number">1</span>.在bigdata-pro01<span class="hljs-selector-class">.kfk</span>.com节点的Kafka安装目录下编写Kafka Consumer执行脚本<br>vi kfk-test-consumer<span class="hljs-selector-class">.sh</span><br>#/bin/bash<br>echo <span class="hljs-string">&quot;kfk-kafka-consumer.sh start ......&quot;</span><br>bin/kafka-console-consumer<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--zookeeper</span> bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro03<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span> <span class="hljs-attr">--from-beginning</span> <span class="hljs-attr">--topic</span> weblogs<br><span class="hljs-number">2</span>.将kfk-test-consumer.sh脚本分发另外两个节点<br>scp kfk-test-consumer<span class="hljs-selector-class">.sh</span> bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:/opt/modules/kakfa_2.<span class="hljs-number">11</span>-<span class="hljs-number">0.8</span>.<span class="hljs-number">2.1</span>/<br>scp kfk-test-consumer<span class="hljs-selector-class">.sh</span> bigdata-pro03<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:/opt/modules/kakfa_2.<span class="hljs-number">11</span>-<span class="hljs-number">0.8</span>.<span class="hljs-number">2.1</span>/<br><br></code></pre></td></tr></table></figure>
<p><strong>联调测试-数据采集分发</strong></p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-number">1</span>、在各个节点上启动zk<br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/zookeeper-3.4.5-cdh5.10.0/</span>sbin/zkServer.sh start  <br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/zookeeper-3.4.5-cdh5.10.0/</span>bin/zkCli.sh  登陆客户端进行测试是否启动成功<br><br><span class="hljs-number">2</span>、启动hdfs  --- http:<span class="hljs-regexp">//</span>bigdata-pro01.kfk.com:<span class="hljs-number">50070</span>/<br>在节点<span class="hljs-number">1</span>：<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>sbin/start-dfs.sh <br><span class="hljs-comment">#节点1 和 节点2  启动namenode高可用</span><br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>sbin/hadoop-daemon.sh start zkfc<br><br><span class="hljs-number">3</span>、启动hbase  ----http:<span class="hljs-regexp">//</span>bigdata-pro01.kfk.com:<span class="hljs-number">60010</span>/<br><span class="hljs-comment">#节点 1  启动hbase</span><br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hbase-1.0.0-cdh5.4.0/</span>bin/start-hbase.sh<br><span class="hljs-comment">#在节点2 启动备用master</span><br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hbase-1.0.0-cdh5.4.0/</span>bin/hbase-daemon.sh start  master<br><span class="hljs-comment">#启动hbase的shell用于操作</span><br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hbase-1.0.0-cdh5.4.0/</span>bin/hbase shell<br><span class="hljs-comment">#创建hbase业务表</span><br>bin/hbase shell<br>create <span class="hljs-string">&#x27;weblogs&#x27;</span>,<span class="hljs-string">&#x27;info&#x27;</span><br><br><span class="hljs-number">4</span>、启动kafka<br><span class="hljs-comment">#在各个个节点启动kafka</span><br>cd <span class="hljs-regexp">/opt/m</span>odules/kafka_2.<span class="hljs-number">10</span>-<span class="hljs-number">0.9</span>.<span class="hljs-number">0.0</span><br>bin<span class="hljs-regexp">/kafka-server-start.sh config/</span>server.properties &amp;<br><span class="hljs-comment">#创建业务</span><br>bin/kafka-topics.sh --zookeeper bigdata-pro01.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro02.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro03.kfk.com:<span class="hljs-number">2181</span> --create --topic weblogs --replication-factor <span class="hljs-number">2</span> --partitions <span class="hljs-number">1</span><br><span class="hljs-comment">#消费(之前编写的脚本可以用)</span><br>bin/kafka-console-consumer.sh --zookeeper bigdata-pro01.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro02.kfk.com:<span class="hljs-number">2181</span>,bigdata-pro03.kfk.com:<span class="hljs-number">2181</span> --from-beginning --topic weblogs<br></code></pre></td></tr></table></figure>
<p>一定确保上述都启动成功能，利用jps查看各个节点进程情况。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbmh1n31j309v042glj.jpg" srcset="/img/loading.gif" lazyload><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbmovok3j309n03sa9y.jpg" srcset="/img/loading.gif" lazyload><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbmw14tjj309o02cweb.jpg" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-number">5</span>、各个节点启动flume<br>#三节点启动flume<br>/opt/modules/flume-<span class="hljs-number">1.7</span>.<span class="hljs-number">0</span>-bin/flume-kfk-start<span class="hljs-selector-class">.sh</span><br><br><span class="hljs-number">6</span>、在节点<span class="hljs-number">2</span>和<span class="hljs-number">3</span>启动日志模拟生产<br>/opt/datas/weblog-shell<span class="hljs-selector-class">.sh</span><br><br><span class="hljs-number">7</span>、启动kafka消费程序<br>#消费（或者使用写好的脚本kfk-test-consumer.sh）<br>bin/kafka-console-consumer<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--zookeeper</span> bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span>,bigdata-pro03<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">2181</span> <span class="hljs-attr">--from-beginning</span> <span class="hljs-attr">--topic</span> weblogs<br><br><span class="hljs-number">8</span>、查看hbase数据写入情况<br>./hbase-shell<br>count <span class="hljs-string">&#x27;weblogs&#x27;</span><br></code></pre></td></tr></table></figure>
<p>结果：<br>kafka不断消费<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbszmkybj30rh0940ue.jpg" srcset="/img/loading.gif" lazyload><br>hbase数据不断增加<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzdbtek6eqj30rv0ar0ud.jpg" srcset="/img/loading.gif" lazyload></p>
<h3 id="mysql、Hive安装与集成"><a href="#mysql、Hive安装与集成" class="headerlink" title="mysql、Hive安装与集成"></a>mysql、Hive安装与集成</h3><p><strong>为什么要用mysql?</strong></p>
<p>一方面，本项目用来存储Hive的元数据；另一方面，可以把离线分析结果放入mysql中；</p>
<p><strong>安装mysql</strong></p>
<p>通过yum在线mysql，具体操作命令如下所示(关于yum源可以修改为阿里的，比较快和稳定)</p>
<figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs subunit">1、在线安装mysql<br>通过yum在线mysql，具体操作命令如下所示。<br>yum clean all<br>yum install mysql-server<br>2、mysql 服务启动并测试<br>sudo chown -R kfk:kfk /usr/bin/mysql    修改权限给kfk<br>1）查看mysql服务状态<br>sudo service mysqld status  <br>2）启动mysql服务<br>sudo service mysqld start<br>3）设置mysql密码<br>/usr/bin/mysqladmin -u root password &#x27;123456&#x27;<br>4）连接mysql<br>mysql –uroot -p123456<br>a）查看数据库<br>show databases;<br>mysql<br><span class="hljs-keyword">test</span><br><span class="hljs-keyword"></span>b）查看数据库<br>use test;<br>c）查看表列表<br>show tables;<br></code></pre></td></tr></table></figure>
<p>出现问题，大多数是权限问题，利用sudo执行或者重启mysql.</p>
<p><strong>安装Hive</strong></p>
<p>Hive在本项目中功能是，将hbase中的数据进行离线分析，输出处理结果，可以到mysql或者hbase，然后进行可视化。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzfpw9k0v7j30kv09rtfp.jpg" srcset="/img/loading.gif" lazyload><br>这里版本采用的是：apache-hive-2.1.0-bin.tar.gz<br>（之前用apache-hive-0.13.1-bin.tar.gz出现和hbase集成失败，原因很奇怪，下一章详细讲）。<br>1、解压</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">步骤都老生常谈了。。。<br>tar -zxf apache-hive-<span class="hljs-number">2.1</span>.<span class="hljs-number">0</span>-bin.tar.gz -C <span class="hljs-regexp">/opt/m</span>odules/<br>mv  apache-hive-<span class="hljs-number">2.1</span>.<span class="hljs-number">0</span>-bin hive-<span class="hljs-number">2.1</span>.<span class="hljs-number">0</span>     <span class="hljs-regexp">//</span>重命名<br></code></pre></td></tr></table></figure>
<p>2、修改配置文件</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">1</span>）hive-log4j.properties<br><span class="hljs-comment">#日志目录需要提前创建</span><br><span class="hljs-attribute">hive</span>.log.dir=/opt/modules/hive-<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">0</span>/logs<br><span class="hljs-attribute">2</span>）修改hive-env.sh配置文件<br><span class="hljs-attribute">HADOOP_HOME</span>=/opt/modules/hadoop-<span class="hljs-number">2</span>.<span class="hljs-number">6</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">HBASE_HOME</span>=/opt/modules/hbase-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span><br><span class="hljs-comment"># Hive Configuration Directory can be controlled by:</span><br><span class="hljs-attribute">export</span> HIVE_CONF_DIR=/opt/modules/hive-<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">0</span>/conf<br></code></pre></td></tr></table></figure>
<p>3、启动进行测试<br>首先启动HDFS，然后创建Hive的目录<br>bin/hdfs dfs -mkdir -p /tmp<br>bin/hdfs dfs -chmod g+w /tmp<br>bin/hdfs dfs -mkdir -p /user/hive/warehouse<br>bin/hdfs dfs -chmod g+w /user/hive/warehouse<br>4、测试</p>
<figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs gauss">./hive<br><span class="hljs-meta">#查看数据库</span><br><span class="hljs-keyword">show</span> databases;<br><span class="hljs-meta">#使用默认数据库</span><br><span class="hljs-keyword">use</span> default;<br><span class="hljs-meta">#查看表</span><br><span class="hljs-keyword">show</span> tables;<br><br></code></pre></td></tr></table></figure>
<p><strong>Hive与mysql集成</strong></p>
<p>利用mysql放Hive的元数据。<br>1、在/opt/modules/hive-2.1.0/conf目录下创建hive-site.xml文件，配置mysql元数据库。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="hljs-meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><br><br><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>jdbc:mysql://bigdata-pro01.kfk.com/metastore?createDatabaseIfNotExist=true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br> <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>root<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>123456<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>   <br>	<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com,bigdata-pro02.kfk.com,bigdata-pro03.kfk.com<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure>
<p>2、设置用户连接信息</p>
<p>1）查看用户信息<br>mysql -uroot -p123456<br>show databases;<br>use mysql;<br>show tables;<br>select User,Host,Password from user;<br>2）更新用户信息<br>update user set Host=’%’ where User = ‘root’ and Host=’localhost’<br>3）删除用户信息<br>delete from user where user=’root’ and host=’127.0.0.1’<br>select User,Host,Password from user;<br>delete from user where host=’localhost’;<br>删除到只剩图中这一行数据<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzfqckmjxej30ej031q2s.jpg" srcset="/img/loading.gif" lazyload><br>4）刷新信息<br>flush privileges;<br>3.拷贝mysql驱动包到hive的lib目录下<br>cp  mysql-connector-java-5.1.35.jar /opt/modules/hive-2.1.0/lib/<br>4.保证第三台集群到其他节点无秘钥登录</p>
<p><strong>Hive与mysql测试</strong></p>
<p>1.启动HDFS和YARN服务<br>2.启动hive<br>./hive<br>3.通过hive服务创建表<br>CREATE TABLE stu(id INT,name STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’ ;<br>4.创建数据文件<br>vi /opt/datas/stu.txt<br>00001    zhangsan<br>00002    lisi<br>00003    wangwu<br>00004    zhaoliu<br>5.加载数据到hive表中<br>load data local inpath ‘/opt/datas/stu.txt’ into table stu;<br>直接在hive查看表中内容就ok。<br>在mysql数据库中hive的metastore元数据。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzfqeibkrtj306103ta9v.jpg" srcset="/img/loading.gif" lazyload></p>
<h3 id="Hive与Hbase集成"><a href="#Hive与Hbase集成" class="headerlink" title="Hive与Hbase集成"></a>Hive与Hbase集成</h3><p><strong>Hive与HBase集成配置</strong></p>
<p>1）在hive-site.xml文件中配置Zookeeper，hive通过这个参数去连接HBase集群。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>   <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>bigdata-pro01.kfk.com,bigdata-pro02.kfk.com,bigdata-pro03.kfk.com<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br></code></pre></td></tr></table></figure>
<p>2）需要把hbase中的部分jar包拷贝到hive中<br>这里采用软连接的方式：<br>执行如下命令：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">export</span> HBASE_HOME=/opt/modules/hbase-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">export</span> HIVE_HOME=/opt/modules/hive-<span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-server-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-server-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar<br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-client-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-client-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar<br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-protocol-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-protocol-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar <br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-it-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-it-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar <br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/htrace-core-<span class="hljs-number">3</span>.<span class="hljs-number">0</span>.<span class="hljs-number">4</span>.jar $HIVE_HOME/lib/htrace-core-<span class="hljs-number">3</span>.<span class="hljs-number">0</span>.<span class="hljs-number">4</span>.jar<br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-hadoop2-compat-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-hadoop2-compat-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar <br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-hadoop-compat-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-hadoop-compat-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar<br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/high-scale-lib-<span class="hljs-number">1</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span>.jar $HIVE_HOME/lib/high-scale-lib-<span class="hljs-number">1</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span>.jar <br><br><span class="hljs-attribute">ln</span> -s $HBASE_HOME/lib/hbase-common-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar $HIVE_HOME/lib/hbase-common-<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4</span>.<span class="hljs-number">0</span>.jar <br></code></pre></td></tr></table></figure>
<p>3）测试<br>在hbase中建立一个表，里面存有数据（实际底层就是在hdfs上），然后Hive创建一个表与HBase中的表建立联系。</p>
<ol>
<li>先在hbase建立一个表<br>（不熟悉的，看指令<a target="_blank" rel="noopener" href="https://www.cnblogs.com/cxzdy/p/5583239.html%EF%BC%89">https://www.cnblogs.com/cxzdy/p/5583239.html）</a><br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzgupdmei1j30h5037mx4.jpg" srcset="/img/loading.gif" lazyload></li>
<li>启动hive,建立联系（之前要先启动mysql，因为元数据在里面)</li>
</ol>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">external</span> <span class="hljs-keyword">table</span> t1(<br>key <span class="hljs-type">int</span>,<br><span class="hljs-type">name</span> string,<br>age string<br>)  <br>STORED <span class="hljs-keyword">BY</span>  <span class="hljs-string">&#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span> <br><span class="hljs-keyword">WITH</span> SERDEPROPERTIES(&quot;hbase.columns.mapping&quot; = &quot;:key,info:name,info:age&quot;) <br>TBLPROPERTIES(&quot;hbase.table.name&quot; = &quot;t1&quot;);<br></code></pre></td></tr></table></figure>
<ol start="3">
<li>hive结果<br>执行 select * from t1;<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzgutrr5x7j30b0035glg.jpg" srcset="/img/loading.gif" lazyload></li>
<li>为项目中的weblogs建立联系<br>之前我们把数据通过flume导入到hbase中了，所以同样我们在hive中建立联系，可以用hive对hbase中的数据进行简单的sql分析，离线分析。</li>
</ol>
<figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs n1ql"><span class="hljs-keyword">create</span> external table weblogs(<br>id <span class="hljs-keyword">string</span>,<br>datatime <span class="hljs-keyword">string</span>,<br>userid <span class="hljs-keyword">string</span>,<br>searchname <span class="hljs-keyword">string</span>,<br>retorder <span class="hljs-keyword">string</span>,<br>cliorder <span class="hljs-keyword">string</span>,<br>cliurl <span class="hljs-keyword">string</span><br>)  <br>STORED <span class="hljs-keyword">BY</span>  <span class="hljs-string">&#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span> <br><span class="hljs-keyword">WITH</span> SERDEPROPERTIES(<span class="hljs-string">&quot;hbase.columns.mapping&quot;</span> = <span class="hljs-string">&quot;:key,info:datatime,info:userid,info:searchname,info:retorder,info:cliorder,info:cliurl&quot;</span>) <br>TBLPROPERTIES(<span class="hljs-string">&quot;hbase.table.name&quot;</span> = <span class="hljs-string">&quot;weblogs&quot;</span>);<br></code></pre></td></tr></table></figure>

<p><strong>Hive与HBase集成中的致命bug</strong></p>
<p>问题如图：<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzguxe0p4ej30nu0hl0ua.jpg" srcset="/img/loading.gif" lazyload></p>
<p>hbase 1.x之后的版本，需要更高版本的hive匹配，最好是hive 2.x,上述的错误是因为hive-0.13.1-bin和hbase-1.0.0-cdh5.4.0，应该是不兼容导致的，莫名bug。于是采用了 hive-2.1.0，查了下这个版本与hadoop其他组件也是兼容的，所以，采用这个。配置仍然采用刚才的方法（上一章和这一章），主要有mysql元数据配置（驱动包别忘了），各种xml配置，测试下。最后，在重启hive之前，<strong>先把hbase重启了</strong>，很重要。</p>
<h3 id="HUE大数据可视化分析"><a href="#HUE大数据可视化分析" class="headerlink" title="HUE大数据可视化分析"></a>HUE大数据可视化分析</h3><p><strong>下载和安装Hue</strong></p>
<p>版本选择： hue-3.9.0-cdh5.15.0<br>1、首先需要利用yum安装依赖包，虚拟机需要联网，这里安装在节点3上。</p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs brainfuck"><span class="hljs-comment">yum</span> <span class="hljs-literal">-</span><span class="hljs-comment">y install ant asciidoc cyrus</span><span class="hljs-literal">-</span><span class="hljs-comment">sasl</span><span class="hljs-literal">-</span><span class="hljs-comment">devel</span> <span class="hljs-comment">cyrus</span><span class="hljs-literal">-</span><span class="hljs-comment">sasl</span><span class="hljs-literal">-</span><span class="hljs-comment">gssapi</span> <span class="hljs-comment">gcc</span> <span class="hljs-comment">gcc</span><span class="hljs-literal">-</span><span class="hljs-comment">c</span>++ <span class="hljs-comment">krb5</span><span class="hljs-literal">-</span><span class="hljs-comment">devel</span> <span class="hljs-comment">libtidy</span> <span class="hljs-comment">libxml2</span><span class="hljs-literal">-</span><span class="hljs-comment">devel libxslt-devel openldap-devel python-devel sqlite</span><span class="hljs-literal">-</span><span class="hljs-comment">devel openssl-devel mysql-devel gmp-devel </span> <br></code></pre></td></tr></table></figure>
<p>2、解压<br>tar -zxf hue-3.9.0-cdh5.15.0.tar.gz -C /opt/modules/<br>3、编译<br>cd  hue-3.9.0-cdh5.15.0<br>make apps<br>4、基本配置与测试</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-number">1</span>）修改配置文件<br>cd desktop<br>cd conf<br>vi hue.ini<br>#秘钥<br>secret_key=jFE93j;<span class="hljs-number">2</span>[<span class="hljs-number">290</span>-eiw.KEiwN2s3[<span class="hljs-string">&#x27;d;/.q[eIW^y#e=+Iei*@Mn &lt; qW5o</span><br><span class="hljs-string">#host port</span><br><span class="hljs-string">http_host=bigdata-pro03.kfk.com</span><br><span class="hljs-string">http_port=8888</span><br><span class="hljs-string">#时区</span><br><span class="hljs-string">time_zone=Asia/Shanghai</span><br><span class="hljs-string">2）修改desktop.db 文件权限</span><br><span class="hljs-string">chmod o+w desktop/desktop.db</span><br><span class="hljs-string">3）启动Hue服务</span><br><span class="hljs-string">/opt/modules/hue-3.9.0-cdh5.15.0/build/env/bin/supervisor</span><br><span class="hljs-string">4）查看Hue web界面</span><br><span class="hljs-string">bigdata-pro03.kfk.com:8888</span><br></code></pre></td></tr></table></figure>
<p><strong>Hue与HDFS集成</strong></p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-number">1</span>）修改hadoop中core-site.xml配置文件，添加如下内容<br>&lt;property&gt;<br>    &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt;<br>    &lt;value&gt;*&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;property&gt;<br>    &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt;<br>    &lt;value&gt;*&lt;/value&gt;<br>&lt;/property&gt;<br><br><span class="hljs-number">2</span>）修改hue.ini配置文件<br>fs_defaultfs=hdfs:<span class="hljs-regexp">//</span>ns<br>webhdfs_url=http:<span class="hljs-regexp">//</span>bigdata-pro01.kfk.com:<span class="hljs-number">50070</span><span class="hljs-regexp">/webhdfs/</span>v1<br>hadoop_hdfs_home=<span class="hljs-regexp">/opt/m</span>odules/hadoop-<span class="hljs-number">2.6</span>.<span class="hljs-number">0</span><br>hadoop_bin=<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>bin<br>hadoop_conf_dir=<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc/hadoop<br><span class="hljs-number">3</span>）将core-site.xml配置文件分发到其他节点<br>scp core-site.xml bigdata-pro02.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc/hadoop<br>scp core-site.xml bigdata-pro01.kfk.com:<span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hadoop-2.6.0/</span>etc/hadoop<br><span class="hljs-number">4</span>）重新启动hue<br>先启动zk,hdfs，再启动hue<br><span class="hljs-regexp">/opt/m</span>odules<span class="hljs-regexp">/hue-3.9.0-cdh5.15.0/</span>build<span class="hljs-regexp">/env/</span>bin/supervisor<br></code></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzk8cc0l9rj30ev0ebmx9.jpg" srcset="/img/loading.gif" lazyload></p>
<p><strong>Hue与YARN集成</strong></p>
<p>1、修改hue.ini配置文件,参考<a target="_blank" rel="noopener" href="https://www.cnblogs.com/zlslch/p/6817226.html">https://www.cnblogs.com/zlslch/p/6817226.html</a><br>区分yarn是不是HA</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[[yarn_clusters]]</span><br><br>   <span class="hljs-section">[[[default]]]</span><br>     <span class="hljs-attr">resourcemanager_host</span>=rs<br>     <span class="hljs-attr">resourcemanager_port</span>=<span class="hljs-number">8032</span><br>     <span class="hljs-attr">submit_to</span>=<span class="hljs-literal">True</span><br>     <span class="hljs-attr">logical_name</span>=rm1<br>     <span class="hljs-attr">resourcemanager_api_url</span>=http://bigdata-pro01.kfk.com:<span class="hljs-number">8088</span><br>     <span class="hljs-attr">proxy_api_url</span>=http://bigdata-pro01.kfk.com:<span class="hljs-number">8088</span><br>     <span class="hljs-attr">history_server_api_url</span>=http://bigdata-pro01.kfk.com:<span class="hljs-number">19888</span><br><br>    <span class="hljs-section">[[[ha]]]</span><br>     <span class="hljs-attr">logical_name</span>=rm2<br>     <span class="hljs-attr">submit_to</span>=<span class="hljs-literal">True</span><br>     <span class="hljs-attr">resourcemanager_api_url</span>=http://bigdata-pro02.kfk.com:<span class="hljs-number">8088</span><br>  <span class="hljs-attr">history_server_api_url</span>=http://bigdata-pro01.kfk.com:<span class="hljs-number">19888</span><br></code></pre></td></tr></table></figure>
<p>2、测试<br>启动yarn，再重启hue。<br>图中的任务是我之前进行的任务<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzk8i4ao6kj314r07j74m.jpg" srcset="/img/loading.gif" lazyload></p>
<p><strong>Hue与mysql、hive集成</strong></p>
<p>1、修改hue.ini配置</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs routeros">  [beeswax]<br><br><br>    <span class="hljs-attribute">hive_server_host</span>=bigdata-pro03.kfk.com<br>    <span class="hljs-attribute">hive_server_port</span>=10000<br>    <span class="hljs-attribute">hive_conf_dir</span>=/opt/modules/hive-2.1.0/conf<br><br><br><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span>.中间其他<span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><br>   [[[mysql]]]<br>    <span class="hljs-attribute">nice_name</span>=<span class="hljs-string">&quot;My SQL DB&quot;</span><br><br>    <span class="hljs-attribute">name</span>=metastore<br>    <span class="hljs-attribute">engine</span>=mysql<br><br>    <span class="hljs-attribute">host</span>=bigdata-pro01.kfk.com<br>    <span class="hljs-attribute">port</span>=3306<br>    <span class="hljs-attribute">user</span>=root<br>    <span class="hljs-attribute">password</span>=123456<br></code></pre></td></tr></table></figure>
<p>2、测试<br>启动节点1的mysql（这是元数据），再启动节点3的hive服<br>/opt/modules/hive-2.1.0/bin/hive –service hiveserver2 &amp;    ##配合hue服务<br>再重启hue。<br>图中是利用hive中的sql查询，hive中的表。但是有一个问题是：我用hive查询hbase中的表，无法查询，出现超时情况，目前还没解决，搞了2天难受，（本来想直接在hue中用hive来处理hbase中的表进行离线计算，但是没法查询，只能查询hive本身自己的表，另外hive的beeline模式也无法查询hbase表，但是hive cli模式可以的查询）<br>问题日志：:java.io.IOException: org.apache.hadoop.hbase.client.RetriesExhaustedException<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzk8rvrxu6j30n20e3q35.jpg" srcset="/img/loading.gif" lazyload></p>
<p><strong>Hue与hbase集成</strong></p>
<p>1、修改hue.ini配置</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[hbase]</span><br>   <span class="hljs-attr">hbase_clusters</span>=(Cluster|bigdata-pro01.kfk.com:<span class="hljs-number">9090</span>)<br>   <span class="hljs-attr">hbase_conf_dir</span>=/opt/modules/hbase-<span class="hljs-number">1.0</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4.0</span>/conf<br>  <span class="hljs-attr">thrift_transport</span>=buffered<br></code></pre></td></tr></table></figure>
<p>2、启动测试<br>先启动hbase,再启动HBase中启动thrift服务<br>/opt/modules/hbase-1.0.0-cdh5.4.0/bin/hbase-daemon.sh start thrift<br>然后重启hue<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzk9mr2i83j30kp0ddaa9.jpg" srcset="/img/loading.gif" lazyload></p>
<h3 id="Spark2-X集群安装与spark-on-yarn部署"><a href="#Spark2-X集群安装与spark-on-yarn部署" class="headerlink" title="Spark2.X集群安装与spark on yarn部署"></a>Spark2.X集群安装与spark on yarn部署</h3><p><strong>spark集群安装</strong></p>
<p>版本是spark-2.2.0-bin-hadoop2.6.tgz，之前用的是hadoop2.6.0.<br>环境要求：scala-2.11.12.tgz/java8/hadoop2.6.0.<br>1、官网下载<br><a target="_blank" rel="noopener" href="https://spark.apache.org/downloads.html">https://spark.apache.org/downloads.html</a><br>2、spark配置<br>配置spark-env.sh</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/opt/modules/jdk1.8.0_191<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SCALA_HOME</span>=/opt/modules/scala-2.11.12<br><br><span class="hljs-built_in">export</span> <span class="hljs-attribute">HADOOP_CONF_DIR</span>=/opt/modules/hadoop-2.6.0/etc/hadoop<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_CONF_DIR</span>=/opt/modules/spark-2.2.0/conf<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_MASTER_HOST</span>=bigdata-pro02.kfk.com<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_MASTER_PORT</span>=7077<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_MASTER_WEBUI_PORT</span>=8080<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_WORKER_CORES</span>=1<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_WORKER_MEMORY</span>=1g<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_WORKER_PORT</span>=7078<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">SPARK_WORKER_WEBUI_PORT</span>=8081<br></code></pre></td></tr></table></figure>
<p>配置slaves</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span><br>bigdata-pro02<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span><br>bigdata-pro03<span class="hljs-selector-class">.kfk</span>.com<br></code></pre></td></tr></table></figure>
<p>如果整合hive,hive用到mysql数据库的话，需要将mysql数据库连接驱动jmysql-connector-java-5.1.7-bin.jar放到$SPARK_HOME/jars目录下<br>3、分发至各个节点<br>4、设定的主节点上启动测试(这是standalone模式)<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzogese9lvj30on05o74m.jpg" srcset="/img/loading.gif" lazyload><br>打开spark服务网址：<a target="_blank" rel="noopener" href="http://bigdata-pro02.kfk.com:8080/">http://bigdata-pro02.kfk.com:8080/</a><br>可以查看到各个节点的情况。<br>5、可以stop-all，因为yarn模式下根本不需要。</p>
<p><strong>spark on Yarn</strong></p>
<p>standalonen模式和spark on Yarn模式比较： <a target="_blank" rel="noopener" href="https://blog.csdn.net/lxhandlbb/article/details/70214003">https://blog.csdn.net/lxhandlbb/article/details/70214003</a><br>spark on Yarn原理：<a target="_blank" rel="noopener" href="https://blog.csdn.net/liuwei0376/article/details/78637732">https://blog.csdn.net/liuwei0376/article/details/78637732</a><br>1、前提条件<br>已经安装了hadoop2.6.0，并可以运行，因为spark运行需要依赖hadoop.<br>2、运行zk、hdfs和yarn<br>高可用下的zk也要运行<br>hadoop:<a target="_blank" rel="noopener" href="http://bigdata-pro01.kfk.com:50070/">http://bigdata-pro01.kfk.com:50070</a><br>yarn：<a target="_blank" rel="noopener" href="http://bigdata-pro01.kfk.com:8088/">http://bigdata-pro01.kfk.com:8088</a><br>3、主节点运行spark<br>./spark-shell –master yarn –deploy-mode client<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzohi45ckpj30o70auq3g.jpg" srcset="/img/loading.gif" lazyload><br>在yarn的网页中也可以看到。<br>虚拟机内存小的话，会出现问题：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">17</span>/<span class="hljs-number">09</span>/<span class="hljs-number">08</span> <span class="hljs-number">10</span>:<span class="hljs-number">36</span>:<span class="hljs-number">08</span> ERROR spark.SparkContext: Error initializing SparkContext.<br><span class="hljs-attribute">org</span>.apache.spark.SparkException: Yarn application has already ended! It might have been killed or unable to launch application master.<br></code></pre></td></tr></table></figure>
<p>解决办法：先停止YARN服务，然后修改yarn-site.xml，分发至各个节点。再重启。<br>增加如下内容</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>false<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>4<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br></code></pre></td></tr></table></figure>
<p>4、测试下程序运行</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">sc</span>.parallelize(<span class="hljs-number">1</span> to <span class="hljs-number">100</span>,<span class="hljs-number">5</span>).count<br></code></pre></td></tr></table></figure>
<p>查看程序运行情况：<br>1）入口yarn的web网页，<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzohlg58qyj31dz0b8tam.jpg" srcset="/img/loading.gif" lazyload><br>2）点击applicationmaster进入<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzoi538xtkj31ck0bwjrz.jpg" srcset="/img/loading.gif" lazyload><br>可能出现问题进不去网页：<br>配置显示在主节点：这里配置节点1，那么RM应该在1的时候可以显示，之前我配集群总名称rs，没法用。<br>修改yarn-site.xml，分发至各个节点，然后重启。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs <property>">	&lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;<br>	&lt;value&gt;bigdata-pro01.kfk.com:8088&lt;/value&gt;<br>&lt;/property&gt;<br></code></pre></td></tr></table></figure>

<h3 id="基于IDEA环境下的Spark2-X程序开发"><a href="#基于IDEA环境下的Spark2-X程序开发" class="headerlink" title="基于IDEA环境下的Spark2.X程序开发"></a>基于IDEA环境下的Spark2.X程序开发</h3><p><strong>开发环境配置</strong></p>
<p>1、安装idea<br>2、安装maven<br>官网下载：apache-maven-3.6.0<br>3、安装java8，并配置环境变量<br>4、安装scala，直接从idea插件下载安装<br>5、安装hadoop在Windows中的运行环境，并配置环境变量</p>
<p><strong>IDEA程序开发</strong></p>
<p>可以参考这个链接很全：<a target="_blank" rel="noopener" href="https://blog.csdn.net/zkf541076398/article/details/79297820">https://blog.csdn.net/zkf541076398/article/details/79297820</a><br>1、新建maven项目<br>2、配置maven<br>3、选择配置scala和java版本<br>4、新建scala目录并设置为source(看图)<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzokar9bjxj30pj0dc13u.jpg" srcset="/img/loading.gif" lazyload><br>5、编写pom.xml文件<br>这里主要你需要什么就放什么，可以github上找例子<br><a target="_blank" rel="noopener" href="https://github.com/apache/spark/blob/master/examples/pom.xml">https://github.com/apache/spark/blob/master/examples/pom.xml</a><br>我的pom，我自己可以用</p>
<figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs dust"><span class="language-xml"><span class="hljs-meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">project</span> <span class="hljs-attr">xmlns</span>=<span class="hljs-string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span> <span class="hljs-attr">xmlns:xsi</span>=<span class="hljs-string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span> <span class="hljs-attr">xsi:schemaLocation</span>=<span class="hljs-string">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">modelVersion</span>&gt;</span>4.0.0<span class="hljs-tag">&lt;/<span class="hljs-name">modelVersion</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">packaging</span>&gt;</span>war<span class="hljs-tag">&lt;/<span class="hljs-name">packaging</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>TestSpark<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>com.kfk.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>TestSpark<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>1.0-SNAPSHOT<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml"></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">properties</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">scala.version</span>&gt;</span>2.11.12<span class="hljs-tag">&lt;/<span class="hljs-name">scala.version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">scala.binary.version</span>&gt;</span>2.11<span class="hljs-tag">&lt;/<span class="hljs-name">scala.binary.version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">spark.version</span>&gt;</span>2.2.0<span class="hljs-tag">&lt;/<span class="hljs-name">spark.version</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;/<span class="hljs-name">properties</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">dependencies</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-core_$</span><span class="hljs-template-variable">&#123;scala.binary.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$</span><span class="hljs-template-variable">&#123;spark.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-streaming_$</span><span class="hljs-template-variable">&#123;scala.binary.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$</span><span class="hljs-template-variable">&#123;spark.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-sql_$</span><span class="hljs-template-variable">&#123;scala.binary.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$</span><span class="hljs-template-variable">&#123;spark.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-hive_$</span><span class="hljs-template-variable">&#123;scala.binary.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$</span><span class="hljs-template-variable">&#123;spark.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_$</span><span class="hljs-template-variable">&#123;scala.binary.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$</span><span class="hljs-template-variable">&#123;spark.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.hadoop<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>hadoop-client<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>2.6.0<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;/<span class="hljs-name">dependencies</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">project</span>&gt;</span></span><br><span class="language-xml"></span><br></code></pre></td></tr></table></figure>
<p>6、编写测试程序</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">import org.apache.spark.sql.SparkSession<br><br><span class="hljs-keyword">object</span> test &#123;<br>  def main(args: Array<span class="hljs-literal">[S<span class="hljs-identifier">tring</span>]</span>): Unit = &#123;<br><br>     <span class="hljs-keyword">val</span> spark = SparkSession<br>      .builder<br>       .master(<span class="hljs-string">&quot;yarn-cluster&quot;</span>)<br>     <span class="hljs-comment">//  .master(&quot;local[2]&quot;)</span><br>      .app<span class="hljs-constructor">Name(<span class="hljs-string">&quot;HdfsTest&quot;</span>)</span><br>      .get<span class="hljs-constructor">OrCreate()</span><br><br>    <span class="hljs-keyword">val</span> path = args(<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">val</span> out = args(<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">val</span> rdd = spark.sparkContext.text<span class="hljs-constructor">File(<span class="hljs-params">path</span>)</span><br>    <span class="hljs-keyword">val</span> lines = rdd.flat<span class="hljs-constructor">Map(<span class="hljs-params">_</span>.<span class="hljs-params">split</span>(<span class="hljs-string">&quot; &quot;</span>)</span>).map(x=&gt;(x,<span class="hljs-number">1</span>)).reduce<span class="hljs-constructor">ByKey((<span class="hljs-params">a</span>,<span class="hljs-params">b</span>)</span>=&gt;(a+b)).save<span class="hljs-constructor">AsTextFile(<span class="hljs-params">out</span>)</span><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>
<p>7、本地测试<br>直接master(“local[2]”)，指定windows下的路径就可以了。如果不能运行一定是开发环境有问题，主要看看hadoop环境变量配置了吗<br>8、打成jar包<br>可参考：<a target="_blank" rel="noopener" href="https://jingyan.baidu.com/article/c275f6ba0bbb65e33d7567cb.html">https://jingyan.baidu.com/article/c275f6ba0bbb65e33d7567cb.html</a><br>9、上传至虚拟机中进行jar包方式提交到spark on yarn.<br>运行底层还是依赖于hdfs，前提要启动zk /hadoop /yarn.</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">bin<span class="hljs-regexp">/spark-submit --class  test  --master yarn --deploy-mode cluster /</span>opt<span class="hljs-regexp">/jars/</span>TestSpark.jar  hdfs:<span class="hljs-regexp">//</span>ns<span class="hljs-regexp">/input/</span>stu.txt  hdfs:<span class="hljs-regexp">//</span>ns/out<br></code></pre></td></tr></table></figure>
<p>运行结束去，可以在yarn的web:<a target="_blank" rel="noopener" href="http://bigdata-pro01.kfk.com:8088/cluster/">http://bigdata-pro01.kfk.com:8088/cluster/</a><br>看见调度success标志。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzol6uy6oaj30of09hjsa.jpg" srcset="/img/loading.gif" lazyload><br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzom68klrwj315f0l23zq.jpg" srcset="/img/loading.gif" lazyload></p>
<p>10、如果运行失败怎么办？看日志<br>有一个比较好的入口上图圈中的logs：<br>先配置yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log.server.url<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>http://bigdata-pro01.kfk.com:19888/jobhistory/logs<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br></code></pre></td></tr></table></figure>
<p>需要重启yarn，<br>并在你配置节点启动历史服务器./mr-jobhistory-daemon.sh start historyserver<br>点击：<a target="_blank" rel="noopener" href="http://bigdata-pro01.kfk.com:8088/cluster">http://bigdata-pro01.kfk.com:8088/cluster</a><br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzom51hwo5j30pa0npaci.jpg" srcset="/img/loading.gif" lazyload></p>
<h3 id="Spark-Streaming实时数据处理"><a href="#Spark-Streaming实时数据处理" class="headerlink" title="Spark Streaming实时数据处理"></a>Spark Streaming实时数据处理</h3><p><strong>Spark Streaming简介</strong></p>
<p>本质上就是利用批处理时间间隔来处理一小批的RDD集合。<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzosp71irxj30g108hq75.jpg" srcset="/img/loading.gif" lazyload></p>
<p><strong>idea中程序测试读取socket</strong></p>
<p>1、在节点1启动nc<br>nc -lk 9999<br>输入一些单词<br>2、在idea中运行程序</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">import org.apache.spark.SparkConf<br>import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;<br><span class="hljs-keyword">object</span> TestStreaming &#123;<br><br>  def main(args: Array<span class="hljs-literal">[S<span class="hljs-identifier">tring</span>]</span>): Unit = &#123;<br><br>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-constructor">SparkConf()</span>.set<span class="hljs-constructor">Master(<span class="hljs-string">&quot;local[2]&quot;</span>)</span>.set<span class="hljs-constructor">AppName(<span class="hljs-string">&quot;NetworkWordCount&quot;</span>)</span><br>    <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-constructor">StreamingContext(<span class="hljs-params">conf</span>, Seconds(5)</span>)<br><br>    <span class="hljs-keyword">val</span> lines = ssc.socket<span class="hljs-constructor">TextStream(<span class="hljs-string">&quot;bigdata-pro01.kfk.com&quot;</span>,9999)</span><br>    <span class="hljs-keyword">val</span> words = lines.flat<span class="hljs-constructor">Map(<span class="hljs-params">_</span>.<span class="hljs-params">split</span>(<span class="hljs-string">&quot; &quot;</span>)</span>)<br>    <span class="hljs-comment">//map reduce 计算</span><br>    <span class="hljs-keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="hljs-number">1</span>)).reduce<span class="hljs-constructor">ByKey(<span class="hljs-params">_</span> + <span class="hljs-params">_</span>)</span><br>    wordCounts.print<span class="hljs-literal">()</span><br>    ssc.start<span class="hljs-literal">()</span><br>    ssc.await<span class="hljs-constructor">Termination()</span><br><br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzot0y7ib8j30tl0a4mxy.jpg" srcset="/img/loading.gif" lazyload></p>
<p><strong>sparkstreaming和kafka进行集成</strong></p>
<p>版本问题：<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzr9u0l3szj31cl048js0.jpg" srcset="/img/loading.gif" lazyload><br>遇到了版本问题，之前用的是kafka0.9，现在和idea集成开发一般是kafka0.10了，还好官网里有支持kafka0.9程序案例，要不然就完犊子了，参考官网进行编写：<br><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/2.2.0/streaming-kafka-0-8-integration.html">http://spark.apache.org/docs/2.2.0/streaming-kafka-0-8-integration.html</a><br>代码案例：<a target="_blank" rel="noopener" href="https://github.com/apache/spark/blob/v2.2.0/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala">https://github.com/apache/spark/blob/v2.2.0/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala</a><br>基于kafka0.9的测试程序</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">import</span> kafka.serializer.<span class="hljs-type">StringDecoder</span><br><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span><br><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka.<span class="hljs-type">KafkaUtils</span><br><span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;<span class="hljs-type">Seconds</span>, <span class="hljs-type">StreamingContext</span>&#125;<br><br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">KfkStreaming</span> </span>&#123;<br>   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br><br>     <span class="hljs-keyword">val</span> spark  = <span class="hljs-type">SparkSession</span>.builder()<br>       .master(<span class="hljs-string">&quot;local[2]&quot;</span>)<br>       .appName(<span class="hljs-string">&quot;kfkstreaming&quot;</span>).getOrCreate()<br><br>     <span class="hljs-keyword">val</span> sc =spark.sparkContext<br>     <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sc, <span class="hljs-type">Seconds</span>(<span class="hljs-number">5</span>))<br><br>     <span class="hljs-keyword">val</span> topicsSet = <span class="hljs-type">Set</span>(<span class="hljs-string">&quot;weblogs&quot;</span>)<br>     <span class="hljs-keyword">val</span> kafkaParams = <span class="hljs-type">Map</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<span class="hljs-string">&quot;metadata.broker.list&quot;</span> -&gt; <span class="hljs-string">&quot;bigdata-pro01.kfk.com:9092&quot;</span>)<br>     <span class="hljs-keyword">val</span> messages = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>, <span class="hljs-type">StringDecoder</span>, <span class="hljs-type">StringDecoder</span>](<br>       ssc, kafkaParams, topicsSet)<br><br>     <span class="hljs-keyword">val</span> lines = messages.map(_._2)<br>     <span class="hljs-keyword">val</span> words = lines.flatMap(_.split(<span class="hljs-string">&quot; &quot;</span>))<br>     <span class="hljs-keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="hljs-number">1</span>L)).reduceByKey(_ + _)<br>     wordCounts.print()<br><br>     <span class="hljs-comment">// Start the computation</span><br>     ssc.start()<br>     ssc.awaitTermination()<br><br>   &#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure>
<p>在节点1上启动kafka程序</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">bin/kafka-server-start<span class="hljs-selector-class">.sh</span> config/server<span class="hljs-selector-class">.properties</span><br>bin/kafka-console-producer<span class="hljs-selector-class">.sh</span> <span class="hljs-attr">--broker-list</span> bigdata-pro01<span class="hljs-selector-class">.kfk</span><span class="hljs-selector-class">.com</span>:<span class="hljs-number">9092</span> <span class="hljs-attr">--topic</span> weblogs<br><br></code></pre></td></tr></table></figure>
<p>运行结果：<br><img src="http://ww1.sinaimg.cn/large/005BOtkIly1fzr9tkp4szj30om07paas.jpg" srcset="/img/loading.gif" lazyload></p>

              
            </div>
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A1%B9%E7%9B%AE/">#大数据项目</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>大数据项目</div>
      <div>http://example.com/2022/03/12/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A1%B9%E7%9B%AE/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Keven He</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年3月12日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/03/11/%E9%9A%8F%E7%AC%94/" title="随笔">
                        <span class="hidden-mobile">随笔</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  

  

  

  

  

  

  




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.0/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script>
  (function() {
    var enableLang = CONFIG.code_language.enable && CONFIG.code_language.default;
    var enableCopy = CONFIG.copy_btn;
    if (!enableLang && !enableCopy) {
      return;
    }

    function getBgClass(ele) {
      return Fluid.utils.getBackgroundLightness(ele) >= 0 ? 'code-widget-light' : 'code-widget-dark';
    }

    var copyTmpl = '';
    copyTmpl += '<div class="code-widget">';
    copyTmpl += 'LANG';
    copyTmpl += '</div>';
    jQuery('.markdown-body pre').each(function() {
      var $pre = jQuery(this);
      if ($pre.find('code.mermaid').length > 0) {
        return;
      }
      if ($pre.find('span.line').length > 0) {
        return;
      }

      var lang = '';

      if (enableLang) {
        lang = CONFIG.code_language.default;
        if ($pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2 && $pre.children().hasClass('hljs')) {
          lang = $pre[0].children[0].classList[1];
        } else if ($pre[0].getAttribute('data-language')) {
          lang = $pre[0].getAttribute('data-language');
        } else if ($pre.parent().hasClass('sourceCode') && $pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2) {
          lang = $pre[0].children[0].classList[1];
          $pre.parent().addClass('code-wrapper');
        } else if ($pre.parent().hasClass('markdown-body') && $pre[0].classList.length === 0) {
          $pre.wrap('<div class="code-wrapper"></div>');
        }
        lang = lang.toUpperCase().replace('NONE', CONFIG.code_language.default);
      }
      $pre.append(copyTmpl.replace('LANG', lang).replace('code-widget">',
        getBgClass($pre[0]) + (enableCopy ? ' code-widget copy-btn" data-clipboard-snippet><i class="iconfont icon-copy"></i>' : ' code-widget">')));

      if (enableCopy) {
        Fluid.utils.createScript('https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js', function() {
          var clipboard = new window.ClipboardJS('.copy-btn', {
            target: function(trigger) {
              var nodes = trigger.parentNode.childNodes;
              for (var i = 0; i < nodes.length; i++) {
                if (nodes[i].tagName === 'CODE') {
                  return nodes[i];
                }
              }
            }
          });
          clipboard.on('success', function(e) {
            e.clearSelection();
            e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-copy', 'icon-success');
            setTimeout(function() {
              e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-success', 'icon-copy');
            }, 2000);
          });
        });
      }
    });
  })();
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
